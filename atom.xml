<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-04-12T14:42:16.289Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>当你的浏览器中地址栏输入地址并回车的一瞬间到页面能够展示回来，经历了什么？</title>
    <link href="http://example.com/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E5%BD%93%E4%BD%A0%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E5%9C%B0%E5%9D%80%E6%A0%8F%E8%BE%93%E5%85%A5%E5%9C%B0%E5%9D%80%E5%B9%B6%E5%9B%9E%E8%BD%A6%E7%9A%84%E4%B8%80%E7%9E%AC%E9%97%B4%E5%88%B0%E9%A1%B5%E9%9D%A2%E8%83%BD%E5%A4%9F%E5%B1%95%E7%A4%BA%E5%9B%9E%E6%9D%A5%EF%BC%8C%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <id>http://example.com/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E5%BD%93%E4%BD%A0%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E5%9C%B0%E5%9D%80%E6%A0%8F%E8%BE%93%E5%85%A5%E5%9C%B0%E5%9D%80%E5%B9%B6%E5%9B%9E%E8%BD%A6%E7%9A%84%E4%B8%80%E7%9E%AC%E9%97%B4%E5%88%B0%E9%A1%B5%E9%9D%A2%E8%83%BD%E5%A4%9F%E5%B1%95%E7%A4%BA%E5%9B%9E%E6%9D%A5%EF%BC%8C%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/</id>
    <published>2022-04-12T02:42:02.000Z</published>
    <updated>2022-04-12T14:42:16.289Z</updated>
    
    <content type="html"><![CDATA[<p>（1）浏览器本身是一个客户端，当你输入URL的时候，首先浏览器会去请求DNS服务器，通过DNS获取相应的域名对应的IP<br>（2）然后通过IP地址找到IP对应的服务器后，要求建立TCP连接<br>（3）浏览器发送完HTTP Request（请求）包后，服务器接收到请求包之后才开始处理请求包<br>（4）在服务器收到请求之后，服务器调用自身服务，返回HTTP Response（响应）包<br>（5）客户端收到来自服务器的响应后开始渲染这个Response包里的主体（body），等收到全部的内容随后断开与该服务器之间的TCP连接。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;（1）浏览器本身是一个客户端，当你输入URL的时候，首先浏览器会去请求DNS服务器，通过DNS获取相应的域名对应的IP&lt;br&gt;（2）然后通过IP地址找到IP对应的服务器后，要求建立TCP连接&lt;br&gt;（3）浏览器发送完HTTP Request（请求）包后，服务器接收到请求包之</summary>
      
    
    
    
    
    <category term="面试题" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>请你谈谈网站是如何进行访问的</title>
    <link href="http://example.com/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E8%AF%B7%E4%BD%A0%E8%B0%88%E8%B0%88%E7%BD%91%E7%AB%99%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E7%9A%84/"/>
    <id>http://example.com/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E8%AF%B7%E4%BD%A0%E8%B0%88%E8%B0%88%E7%BD%91%E7%AB%99%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E7%9A%84/</id>
    <published>2022-04-12T02:38:04.000Z</published>
    <updated>2022-04-12T14:42:25.545Z</updated>
    
    <content type="html"><![CDATA[<ol><li>输入一个域名；回车 </li><li>检查本机的 C:\Windows\System32\drivers\etc\hosts配置文件下有没有这个域名映射；</li></ol><ul><li>有：直接返回对应的ip地址，这个地址中，有我们需要访问的web程序，可以直接访问</li><li>没有：去DNS服务器找，找到的话就返回，找不到就返回找不到；</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/12/YtJjzAHWIaFg1iL.png"                      alt="image-20220412104107515.png"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;输入一个域名；回车 &lt;/li&gt;
&lt;li&gt;检查本机的 C:\Windows\System32\drivers\etc\hosts配置文件下有没有这个域名映射；&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;有：直接返回对应的ip地址，这个地址中，有我们需要访问的web程</summary>
      
    
    
    
    
    <category term="面试题" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day13深度卷积网络：实例探究</title>
    <link href="http://example.com/2021/08/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day13%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6/"/>
    <id>http://example.com/2021/08/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day13%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6/</id>
    <published>2021-08-21T12:29:29.000Z</published>
    <updated>2022-04-13T13:02:34.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-为什么要进行实例探究？"><a href="#01-为什么要进行实例探究？" class="headerlink" title="01 为什么要进行实例探究？"></a>01 为什么要进行实例探究？</h1><p>如果有人训练出擅长识别猫狗人的神经网络或者神经网络框架，而你的计算机视觉识别任务是构建一个自动驾驶汽车，你完全可以借鉴别人的神经网络框架来解决自己问题。</p><hr><ul><li>非常有效的神经网络范例：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/bZtxT7Rs2wE6jGF.png"                      alt="image-20210821103041810"                ></p><h1 id="02-经典网络"><a href="#02-经典网络" class="headerlink" title="02 经典网络"></a>02 经典网络</h1><ul><li>LeNet - 5的网络结构：大约有6万个参数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/hZM7enoY1bFVl6c.png"                      alt="image-20210821113223201"                ></p><ul><li>AlexNet网络结构：大约有6000万个参数，AlexNet表现出色的原因有两个：一是当用于训练图像和数据集时，AlexNet能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据；二是它使用了ReLU激活函数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/PhfZkMzCrJGv4sE.png"                      alt="image-20210821113235155"                ></p><ul><li>VGG - 16的网络结构：没有那么多超参数，这是一种只需要专注于卷积层的简单网络。16这个数字代表这个网络包含16个卷积层和全连接层，是个很大的网络，总共包含约1.38亿个参数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/iVJzELp3lbRdrvy.png"                      alt="image-20210821113246142"                ></p><h1 id="03-残差网络"><a href="#03-残差网络" class="headerlink" title="03 残差网络"></a>03 残差网络</h1><p>非常深的网络是很难训练的，因为存在梯度消失和梯度爆炸问题。跳远连接，它可以从某一网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层，可以用跳远连接构建能够训练深度网络的ResNets。</p><hr><ul><li>ResNets是由残差块构建的。信息流从a[l]到a[l+2]需要经过线性激活-&gt;非线性ReLU激活-&gt;性激活-&gt;非线性ReLU激活这组神经网络的主路径。而在残差网络中有一点变化，我们将a[l]直接向后拷贝到神经网络深层（在线性激活之后，非线性ReLU激活前），这就意味这原来主路经的最后的等式去掉，用另一个ReLU非线性函数来代替。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/uCvAwSXlVozImGH.png"                      alt="image-20210821160810819"                ></p><ul><li>把下面普通网络变成ResNets的方法是加上所有的跳远连接，每两层增加一个捷径构成一个残差块。该图就是5个残差块。我们用一个优化算法来训练普通网络的话，训练误差实际上是先减小后增加，随着深度的增加用优化算法越难训练；但是有ResNets的话训练误差就是不断减小。对于x的激活或者这些中间的激活，能够达到网络的更深层，这种方法能有助于解决梯度消失和梯度爆炸问题。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/BzdwPr294LKUFbO.png"                      alt="image-20210821160844535"                ></p><h1 id="04-残差网络为什么有用？"><a href="#04-残差网络为什么有用？" class="headerlink" title="04 残差网络为什么有用？"></a>04 残差网络为什么有用？</h1><p>通常一个网络深度越深，它在训练集上训练网络的效率会有所减弱</p><hr><ul><li>如果使用L2正则化或权重衰减，那么它会压缩W[l+2]的值，同样对b应用权重衰退也一样。如果W[l+2]和b都为0，那么a[l+2] &#x3D; a[l]，所以给大型神经网络增加两层，无论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现。</li><li>残差网络起作用的主要原因是这些残差块学习恒等函数非常容易，能确定网络性能不会受到影响，甚至有时还会提高效率。</li><li>假设z[l+2]与a[l]具有相同维度，所以ResNets使用了许多相同卷积，使得a[l]的维度等于输出层的维度，因而实现了这个跳远连接，因为同一卷积保留了维度，所以很容易得出这个短连接，并输出这两个相同维度的向量；如果输入和输出有不同维度，假设输入是128维度，输出是256维度，就需要增加一个Ws（256×128），不需要对Ws做任何操作，它是网络通过学习得到的矩阵或参数，是一个固定矩阵，p&#x3D;0，用0填充a[l]，其维度为256。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Dp8bOi3roFGUjQC.png"                      alt="image-20210821221101843"                ></p><ul><li>图片识别的普通和ResNet神经网络：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/iYR7ZnT2J1u6vxm.png"                      alt="image-20210821222902428"                ></p><ul><li>ResNet类似于其他很多神经网络，也会有很多卷积层，其中偶尔会有池化层或者类似池化层的层，不论这些层是什么类型，都需要调整矩阵Ws的维度。</li></ul><h1 id="05-网络中的网络以及1-×-1卷积"><a href="#05-网络中的网络以及1-×-1卷积" class="headerlink" title="05 网络中的网络以及1 × 1卷积"></a>05 网络中的网络以及1 × 1卷积</h1><ul><li>从图中可以看出对6×6×1的图使用1×1过滤器进行卷积效果不是很好，就是乘一个数；但是对6×6×32的使用1×1的过滤器进行卷积效果更好，具体来说1×1的过滤器的作用是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的元素智能乘积，然后应用ReLU非线性函数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/WHA8X3hNvzCQPlp.png"                      alt="image-20210822210203888"                ></p><ul><li>1×1的卷积层实现一些重要功能：它给神经网络添加了一个非线性函数从而减小或保持输入层中的信道数量不变，当然也可以增加信道。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/PvNuAJIB984l6sz.png"                      alt="image-20210822210215346"                ></p><h1 id="06-谷歌-Inception-网络简介"><a href="#06-谷歌-Inception-网络简介" class="headerlink" title="06 谷歌 Inception 网络简介"></a>06 谷歌 Inception 网络简介</h1><p>Inception 网络的作用就是代替你来做决定：构建卷积层时，过滤器的大小或者要不要添加池化层</p><hr><ul><li>Inception 网络的核心内容：</li><li>5×5过滤器在该块中的计算成本：乘法运算的总次数为每个输出值所需的乘法运算次数乘以输出值的个数&#x3D;1.2亿。</li><li>1×1卷积的应用：为了降低计算成本，我们用计算成本除以因子10，将它从1.2亿减小到原来的十分之一</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-为什么要进行实例探究？&quot;&gt;&lt;a href=&quot;#01-为什么要进行实例探究？&quot; class=&quot;headerlink&quot; title=&quot;01 为什么要进行实例探究？&quot;&gt;&lt;/a&gt;01 为什么要进行实例探究？&lt;/h1&gt;&lt;p&gt;如果有人训练出擅长识别猫狗人的神经网络或者神</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day12卷积神经网络</title>
    <link href="http://example.com/2021/08/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day12%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2021/08/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day12%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-08-20T15:42:29.000Z</published>
    <updated>2022-04-13T12:53:23.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-计算机视觉"><a href="#01-计算机视觉" class="headerlink" title="01 计算机视觉"></a>01 计算机视觉</h1><p>即使你在计算机视觉方面没有做出成果，希望你可以将所学的知识应用到其他算法和结构</p><hr><ul><li>计算机视觉的一些例子：图片识别（给出一张图片让计算机去分辨出这是一只猫）、目标检测（首先算出有哪些物体，然后用一些技术识别出他们在图片的位置）、图片风格迁移（就是你有一张满意的图片和一张风格图，你可以用神经网络将他们融合在一起，描绘出一张新的图片，整体轮廓来自左边，风格却是右边的）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/c4AJBsUKf3TyjRw.png"                      alt="image-20210819152546875"                ></p><ul><li>在应用计算机视觉还有一个很大的挑战就是数据的输入可能会非常大，例如像素是1000×1000的，那么输入的数据就是三百万的数据量，你也许会有1000个隐藏单元，而所有的权值W，如果你使用的是标准的全连接网络，矩阵的大小也是（1000，三百万）也就是会有三十亿个参数。在参数量如此之大的情况下难以获取足够的数据来防止过拟合。如果想要也能计算大型图片就可以使用卷积运算。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/l5jndf4eCNRxEoT.png"                      alt="image-20210819152559336"                ></p><h1 id="02-边缘检测示例"><a href="#02-边缘检测示例" class="headerlink" title="02 边缘检测示例"></a>02 边缘检测示例</h1><p>卷积运算是卷积神经网络最基本的组成部分</p><hr><ul><li>给一张图片让电脑检测是什么物体，可能做的第一件事是检测图片中的垂直边缘，也可能想检测水平边缘。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/5AhQHvyPWOkowY1.png"                      alt="image-20210820092344753"                ></p><ul><li>那么如何在图像中检测这些边缘呢？下面是一个6×6的灰度图像，为了检测一个图像的垂直边缘可以构造一个3×3的矩阵（过滤器），卷积运算用*表示，该例中用3×3的过滤器对其进行卷积，输出4×4的矩阵（另一张图片）。4×4的矩阵的元素是将3×3的过滤器覆盖到6×6的灰度图像中，不断地平移得到。不同的语言使用不同的函数代表卷积。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/O78hn4qwLrCFRgB.png"                      alt="image-20210820092737334"                ></p><ul><li>在下面例子中输出图像中间区域的亮处，表示在图像中间有一个很明显的垂直边缘。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/armBpkl1hgHLofX.png"                      alt="image-20210820092445198"                ></p><h1 id="03-更多边缘检测内容"><a href="#03-更多边缘检测内容" class="headerlink" title="03 更多边缘检测内容"></a>03 更多边缘检测内容</h1><ul><li>通过不同的过滤器可以找出垂直或者水平的边缘。之前的图里面30是由10过渡到0得到的，所以就是由亮转暗；之前的 30 翻转成了-30，表明是由暗向亮过渡， 而不是由亮向暗过渡，因为看原图是从0到10，所以是从暗到亮的转化。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/PgaEqUfCLVsIhZv.png"                      alt="image-20210820101755312"                ></p><ul><li>水平边缘过滤器如下：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/i3ETomkwSYQXWe5.png"                      alt="image-20210820102053448"                ></p><ul><li>有多种过滤器：Sobel过滤器（增加了中间一行元素的权重）、Scharr过滤器、还有一种将九个数字都当成参数的思想（已经成为计算机视觉最为有效的思想之一）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/QwdaMvGLxoAi39r.png"                      alt="image-20210820102114071"                ></p><h1 id="04-Padding"><a href="#04-Padding" class="headerlink" title="04 Padding"></a>04 Padding</h1><p>为了构建深度神经网络，需要学会使用卷积的基本操作Padding</p><hr><ul><li>可以用公式计算为什么输出是4×4，公式为（n-f+1）×（n-f+1），其中n×n代表原图大小，f×f代表过滤器大小。这样的话会有两个缺点：一是每次做卷积操作，你的图像就会缩小，二是那些在角落或者边缘区域的像素点在输出中采用较少，意味着丢掉了图像边缘位置的许多信息。为了解决这些问题，可以在卷积操作之前填充这幅画（沿着图像边缘再填充一层像素），习惯上可以用0来填充，如果p是填充的数量（该例中p&#x3D;1，因为周围都填充了一个像素点），输出就变成了（n+2p-f+1）×（n+2p-f+1）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/bFtoevKHyMdOlJh.png"                      alt="image-20210820115241942"                ></p><ul><li>填充多少像素通常有两个选择：分别叫Valid卷积（不填充）和Same卷积（填充后你的输出大小和输入大小是一样的）。在计算机视觉中过滤器一般都是奇数维，因为如果f为偶数，那么你只能使用一些不对称填充，只有f为奇数时才会有自然填充；还有就是在奇数维中会有一个中心像素点，便于指出过滤器的位置。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/EzBZXkr31jLdFTJ.png"                      alt="image-20210820115253453"                ></p><h1 id="05-卷积步长"><a href="#05-卷积步长" class="headerlink" title="05 卷积步长"></a>05 卷积步长</h1><ul><li>步长为2，说明过滤器执行完一次后，平移两个格。输出格子公式：[(n+2p-f)&#x2F;s]+1 × [(n+2p-f)&#x2F;s]+1，如果不是一个整数的话就向下取整。按照惯例，过滤器必须完全处于图像中或者填充之后的图像区域内，才输出相应结果。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/nTuYINvzghmDj2A.png"                      alt="image-20210820152848282"                ></p><ul><li>总结下维度情况：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/INwQrlmeLn72Gi3.png"                      alt="image-20210820152914192"                ></p><ul><li>在有些定义中还有镜像翻转，对着对角线翻转，咱们这不用奥。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/u2wlxbph4PY1ejD.png"                      alt="image-20210820152957895"                ></p><h1 id="06-卷积为何有效"><a href="#06-卷积为何有效" class="headerlink" title="06 卷积为何有效"></a>06 卷积为何有效</h1><ul><li>假设我们想检测彩色图像的特征，那么对应的过滤器也应该有三层，对应红绿蓝三个通道。定义一下6×6×3这个形式：分别代表高、宽和通道数，并且图像的通道数必须和过滤器的通道数相等。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ib4k1hsEtd8RfSL.png"                      alt="image-20210820155626710"                ></p><ul><li>下面来看下计算立体图像的细节：首先我们的过滤器画成一个三维的立方体，然后把这个过滤器放到最左上角的位置，最后把27个对应数相乘相加就得到了输出的第一个数。依次平移得到剩下的数字。通过设置过滤器每层不同的参数，就可以检测不同颜色通道里的边界（右下角那两个例子）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/QiHenChouKUDxAc.png"                      alt="image-20210820155645064"                ></p><ul><li>我们想同时检测垂直和水平边缘还有45度倾斜的边缘还有70度倾斜的边缘该怎么做？（换句话说就是想同时用多个过滤器怎么办）就是用两个不同的过滤器，然后得到的结果依次进行叠加，得到一个立方体。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/xQcL36Dqa9fht4C.png"                      alt="image-20210820155718046"                ></p><h1 id="07-单层卷积网络"><a href="#07-单层卷积网络" class="headerlink" title="07 单层卷积网络"></a>07 单层卷积网络</h1><ul><li>通过不同的过滤器得到不同的输出后，通过python的广播机制给第一个矩阵这些元素都加上同一偏差，然后应用一个非线性激活函数ReLU；然后给第二个输出矩阵加上不同的偏差，同样也使用非线性激活函数ReLU，最后得到另一个4×4的矩阵，然后将两个矩阵叠加起来，就得到一个4×4×2的矩阵。</li><li>a[0]到a[1]的演变过程：首先执行线性函数，然后将所有元素相乘做卷积，再加上偏差，然后应用激活函数ReLU，最后通过神经网络的一层，将6×6×3的维度a[0]演化为一个4×4×2维度的a[1],这就是卷积神经网络的一层。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/6O1bW9fAUcXeJYp.png"                      alt="image-20210820195009232"                ></p><ul><li>举个例子，有十个过滤器，每个过滤器是3×3×3的，问这一层有多少参数？因为过滤器是3×3×3的，所以有27个参数，然后加一个偏差就是28个，然后是10个过滤器也就是一共280个参数。无论输入的图片有多大，参数始终是280个，这是卷积神经网络的一个特征叫做“避免过拟合”。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/bvrHws14CkLy5Te.png"                      alt="image-20210820195038745"                ></p><ul><li>总结下用于描述卷积神经网络中的一层的各种标记：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/1LapcKxJ53zPErq.png"                      alt="image-20210820200118875"                ></p><h1 id="08-简单卷积网络示例"><a href="#08-简单卷积网络示例" class="headerlink" title="08 简单卷积网络示例"></a>08 简单卷积网络示例</h1><ul><li>想做一个图像识别，输入x，然后判别图片中有没有猫（0&#x2F;1表示）。通过下面的不同卷积不断地进行，最终为图片提取出7×7×40个特征，然后对该卷积层进行处理处理，可以将其平滑或展开成1960个单元，平滑处理后输出一个向量，其填充内容是逻辑回归单元还是softmax回归单元完全取决于我们是想识别图片上有没有猫还是想识别K种不同对象中的一种用y^表示最终神经网络的预测输出。</li><li>随着神经网络计算深度不断加深，通常开始时的图像也要更大一些，高度和宽度会在一段时间内保持一致，然后随着深度的加深而逐渐减小，而信道数量在增加。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/aXEDsI9OtFM8wv5.png"                      alt="image-20210820204504429"                ></p><ul><li>一个典型的卷积神经网络通常有三层：卷积层（CONV）、池化层（POOL）、全连接层（FC）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/wHGmsxnS5pruJNI.png"                      alt="image-20210820204517662"                ></p><h1 id="09-池化层"><a href="#09-池化层" class="headerlink" title="09 池化层"></a>09 池化层</h1><p>使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性</p><hr><ul><li>在此用到的池化类型是最大池化，下面的例子相当于我们选用了一个规模为2、步幅为2的过滤器，然后选择其中最大值。最大化操作的功能就是只要在任何一个象限内提取到某个特征，他都会保留在最大池化的输出中，最大池化的实际作用就是在过滤器中提取某个特征，然后保留其最大值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/t3yT2xeXg56VGp1.png"                      alt="image-20210820211745699"                ></p><ul><li>再看一个有若干超级参数的例子，每个信道都独立执行最大池化计算。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Op5iKwShmoIeyGE.png"                      alt="image-20210820211821158"                ></p><ul><li>平均池化——就是取平均值</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/UjCruST7XD2ehvF.png"                      alt="image-20210820212010845"                ></p><ul><li>总结：池化的超级参数包括过滤器大小f和步幅s，大部分情况下p&#x3D;0</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/s5mfHBJPuqThVoL.png"                      alt="image-20210820211932204"                ></p><h1 id="10-卷积神经网络示例"><a href="#10-卷积神经网络示例" class="headerlink" title="10 卷积神经网络示例"></a>10 卷积神经网络示例</h1><ul><li>在卷积文献中，卷积有两种分类，一类卷积是一个卷积层和一个池化层一起作为一层（人们在计算神经网络有多少层时，通常只是统计具有权重和参数的层，因为池化层没有权重和参数，只有一些超参数，所以将两者当着一层Layer1）；另一类卷积是把卷积层作为一层，池化层单独作为一层。</li><li>我们针对识别数字例子，首先输入一个数字图片，通过两个神经网络的组合（卷积层+池化层），然后得到一个输出，将输出水平展开成向量，通过两个全连接层，最后用FC4的单元填充一个softmax单元，softmax会有十个输出。关于如何选定超级参数，常规做法是尽量不要自己设置，而是参考别人的文献采用了哪些超级参数，选择一个效果好的框架。在神经网络中另一种常见的模式是一个或多个卷积层后跟随一个池化层，然后一个或多个卷积层后再跟随一个池化层，然后是几个全连接层，最后是一个softmax。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/umNMPEZg5hIKvtC.png"                      alt="image-20210820230812377"                ></p><ul><li>神经网络激活值形状、激活值大小和参数数量。有几点注意：第一池化层和最大池化层没有参数；第二卷积层的参数相对较少，许多参数都存在于神经网络中的全连接层；第三随着神经网络的加深，激活值会逐渐减小（如果下降太快也会有所影响）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ARbxHeBP8Gmp1dw.png"                      alt="image-20210820230901334"                ></p><h1 id="11为什么使用卷积？"><a href="#11为什么使用卷积？" class="headerlink" title="11为什么使用卷积？"></a>11为什么使用卷积？</h1><ul><li>和只用全连接层相比卷积层的两个主要优势在于参数共享和稀疏连接。两个神经元相连会得到一个非常多的参数数量3072×4704，而使用卷积的话，一共才156个参数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/onDqR5eY4ZCktcK.png"                      alt="image-20210820233215874"                ></p><ul><li>卷积网络映射这么少参数有两个原因：一是参数共享，一个特征检测器如垂直边缘检测器，用于检测图片左上角区域的特征，这个特征很可能也适用于右下角区域，因此在计算图片的左上和右下角区域时，不需要添加其他的特征检测器，假如有一个左上角和右下角可能有不同分布，但也可能很相似的数据集，整张图片共享特征检测器，提取效果也很好；二是稀疏连接，右图的输出单元仅与36个输入特征中的9个相连接，而其他像素值都不会对输出产生任何影响。神经网络通过这两种机制减少参数，以便我们用更小的训练集训练它，从而防止过拟合。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/gQbqWChjAzYfonG.png"                      alt="image-20210820233229584"                ></p><ul><li>要训练神经网络要做的就是使用梯度下降法，来优化神经网络中的所有参数，以及减小代价函数J的值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/IH3dlqUafFPrDLt.png"                      alt="image-20210820233240036"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-计算机视觉&quot;&gt;&lt;a href=&quot;#01-计算机视觉&quot; class=&quot;headerlink&quot; title=&quot;01 计算机视觉&quot;&gt;&lt;/a&gt;01 计算机视觉&lt;/h1&gt;&lt;p&gt;即使你在计算机视觉方面没有做出成果，希望你可以将所学的知识应用到其他算法和结构&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day11 机器学习（ML）策略</title>
    <link href="http://example.com/2021/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day11%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5/"/>
    <id>http://example.com/2021/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day11%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5/</id>
    <published>2021-08-19T01:11:46.000Z</published>
    <updated>2022-04-13T12:04:25.279Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-进行误差分析"><a href="#01-进行误差分析" class="headerlink" title="01 进行误差分析"></a>01 进行误差分析</h1><p>人工检查你的算法错误，也许可以让你了解接下来应该做什么，这个过程就叫误差分析</p><hr><ul><li>同样是一个猫分类器，但是它的准确度只有90%，它会将一些狗的图片识别成猫，如果花费几个月的时间来开发专门研究狗的项目，那是大可不必，因为可能取不到效果。所以我们推荐使用误差分析：我们自己手动的观察100个错误标记的开发集例子，如果只有其中5个是狗的图片，那么就算把识别狗做的很好，误差也高达9.5%，如果其中有50个是狗的图片，这时我们在对识别狗进行完善就可以将误差减半到5%。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/yOIJjeVibLdG2nv.png"                      alt="image-20210817090520185"                ></p><ul><li><p>有时在做误差分析时，也可以同时并行评估几个想法，例如猫分类器：</p><ol><li>改善针对狗图的性能</li><li>解决针对于猫科动物看做成家猫的错误</li><li>提高画质</li></ol></li><li><p>针对于上面这三个想法我们可以将其记录成下面的表，统计出每个想法占的百分比，在以此判断是否应该提升哪个的性能，提升哪个性能上限比较高。</p></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/3VTZdgPmMnoNipc.png"                      alt="image-20210817090534858"                ></p><ul><li>总结：进行误差分析应该找一组错误标记的例子，可能在开发集或者测试集里，观察错误标记的例子，看看假阳性和假阴性，统计属于不同错误类型的错误数量，可以帮助你发现哪些问题需要优先处理。</li></ul><h1 id="02-清楚标注错误的数据"><a href="#02-清楚标注错误的数据" class="headerlink" title="02 清楚标注错误的数据"></a>02 清楚标注错误的数据</h1><p>  监督学习问题的数据是由输入x和输出标签y组成，通过发现，得知有些输出标签y是错误的，是否值得花时间去修正这些标签呢？</p><hr><ul><li><p>y为1是猫，为0不是猫，这时发现倒数第二个是错误标签，该怎么办？</p><ol><li>如果是训练集，深度学习算法对于训练集中的随机误差是很宽容的，只要这些错误例子离随机误差不太远，误差足够随机，那么放着这些误差不管可能也没问题。只要总数据集足够大，实际误差可能不会太高。</li><li>如果担心开发集和测试集上标记出错的例子带来的影响，一般建议在误差分析的时候，添加一个额外的列来统计标签y错误的例子数。是否值得修正这6%标记出错的例子呢？如果这些错误标签严重影响了你在开发集上评估算法的能力，那么就应该花费时间去修正错误标签，反之不用。</li><li>建议看三个数字来确定是否值得去人工修正标记出错的数据：整体的开发集误差10%；错误标记引起的错误的数量或者百分比0.6%（10%的6%）；其他原因导致错误的百分比9.4%（10%-0.6%）。对于左边的错误标签没有严重影响了你在开发集上评估算法的能力，而左边的错误标签严重影响了你在开发集上评估算法的能力。</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ecOt4uxyNC1w536.png"                      alt="image-20210817110813946"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/4COHUKSuF3h6r1z.png"                      alt="image-20210817110828006"                ></p><ul><li>修正的建议：首先无论使用什么修正手段，都要同时作用到开发集和测试集上（开发集和测试集必须来自相同的分布，因为开发集确定了你的目标，当你击中目标后将算法推广到测试集中，这样你的团队才能更高效）来确保他们继续来自同一分布；其次要同时检验算法判断正确和判断错误的例子，如果你只修正算法出错的例子，你对算法的偏差估计可能会变大，修正错误的例子之后，一些改变可能使原本正确的例子也变成错误的了。但是我们通常不会这么做，因为如果你的分类器很准确，那么判断错的次数比判断正确的次数要小得多；如果你进入到一个开发集和测试集去修正部分标签，你不大可能去修正训练集的，因为训练集数据比开发集和测试集多的多，需要花费的时间也就多。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/f2yVLtO5Yraeg9n.png"                      alt="image-20210817110840750"                ></p><ul><li>总结：在构造实际系统时，通常需要更多的人工误差分析和人类见解，我们要看下犯错误的例子，这样可以帮助找到需要优先处理的任务。</li></ul><h1 id="03-快速搭建你的第一个系统，并进行迭代"><a href="#03-快速搭建你的第一个系统，并进行迭代" class="headerlink" title="03 快速搭建你的第一个系统，并进行迭代"></a>03 快速搭建你的第一个系统，并进行迭代</h1><ul><li>一般来说，对于几乎所有的机器学习程序，可能会有50个不同的方向可以前进，并且都是相对合理的可以改善系统，但挑战在于你如何选择一个方向集中精力处理。</li><li>如果你像搭建一个全新的机器学习程序，建议：首先快速设立开发集和测试集还有指标，决定了你的目标所在，如果错了可以在改，但是一定要设立；然后建议马上搭好一个机器学习系统原型（初始系统的全部意义在于有一个训练过的系统可以让你确定偏差和方差的范围，就知道下一步应该优先做什么，能够进行误差分析），然后找训练集进行训练，看效果。开始理解你的算法表现如何，在开发集测试集你的评估指标上表现如何；当你建立第一个系统后，你就可以马上用到偏差方差分析和误差分析。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Z9lHt7LKDSn4aAx.png"                      alt="image-20210817120247822"                ></p><h1 id="04-在不同的划分上进行训练并测试"><a href="#04-在不同的划分上进行训练并测试" class="headerlink" title="04 在不同的划分上进行训练并测试"></a>04 在不同的划分上进行训练并测试</h1><ul><li>如果你有两个数据来源，一个较小，一个较大，那么该怎么分布？</li><li>第一条路：我们可以将来自互联网和用户的图片结合到一块，然后随机分配到训练、开发和测试集中，这样三个集的数据都来自同一分布，但是坏处在于，对于2500个开发集数据，其中很多图片都是来自网页下载，并不是真正关心的数据分布，真正要处理的是用户上传的，这样设立开发集导致团队针对不同于你实际关心的数据分布去优化。（不要用）</li><li>推荐第二条路：我们训练集是来自互联网的图片，如果需要的话，再加上5000张来自手机上传的图片，对于开发集和测试集都是手机图片，这样的好处在于你现在瞄准的目标就是你想要处理的目标。开发集全部数据都来自用户，可以告诉你的团队这是你真正关心的图片分布，试着搭建一个学习系统，让系统在处理用户图片分布时效果良好。缺点在于你的训练集和开发集、测试集不在一个分布</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NLS1lv3rjyqkICM.png"                      alt="image-20210817152708400"                ></p><ul><li>以语音激活后视镜为例，对于训练集你可以使用你所拥有的所有语音数据；对于开发集和测试集来说可能要小的多，比如实际上来自语音激活后视镜的数据。按照上面推荐第二条路进行分配。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/tJPjAav1SkXbCwM.png"                      alt="image-20210817152719753"                ></p><h1 id="05-不匹配数据划分的偏差和方差"><a href="#05-不匹配数据划分的偏差和方差" class="headerlink" title="05 不匹配数据划分的偏差和方差"></a>05 不匹配数据划分的偏差和方差</h1><p>当你的训练集和你的开发集、测试集来自不同的分布时，分析偏差和方差的方式可能不一样</p><hr><ul><li>训练集误差为1，开发集误差为10。如果训练集和开发集来自同一分布，这就存在很大的方差问题，算法不能很好的从训练集出发泛化；但如果你的训练集和开发集来自不同的分布，这时就不能轻易下结论，可能没有方差问题，只不过反映了开发集包括更难准确分类的图片。</li><li>分析的问题在于当你看训练误差再看开发误差有有两件事变了：<ol><li>算法只见过训练集数据，没见过开发集数据</li><li>开发集数据来自不同的分布</li></ol></li><li>我们设置一个新的数据子集叫训练-开发集，从训练集的分布里随机打散训练集，但是不会用来训练网络。为了进行误差分析，你应该做的是看看分类器在训练集上的误差1%、训练-开发集上的误差9%还有开发集上的误差10%。而训练集和训练-开发集上的差异在于你的神经网络能看到第一部分数据（红色部分）并在上面做训练，但没有在训练-开发集上直接训练，这就告诉算法存在方差。因为训练集上和训练-开发集来自同一分布的数据测得，所以尽管你的神经网络在训练集中表现良好，但无法泛化到来自相同分布的训练-开发集中；又如这三个误差分别是1%、1.5%、10%，这时方差问题就很小了。因为从训练集到训练-开发集误差只上升了一点点，但转到开发集时误差就大了很多，所以这是数据不匹配问题；又如这三个误差分别是10%、11%、12%，这时就存在偏差问题了；又如这三个误差分别是10%、11%、20%，这就有两个问题了：可避免偏差相当高还有数据不匹配问题很大。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jN4syq3G1InPhMd.png"                      alt="image-20210817231557467"                ></p><ul><li>一般的原则，我们要看的关键数据是人类水平误差、训练集误差、训练开发集误差、开发集误差。如果再加一个测试误差，那么测试误差和开发集误差的间距就是你对开发集过拟合的程度。当然也可能出现右边的情况，是因为开发测试集分布比你应用实际处理的数据要容易得多，那么误差可能真的就下降。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/lW2CvLJXFIAVxKT.png"                      alt="image-20210817231619845"                ></p><ul><li>我们以语音激活后视镜为例子，横轴是不同的数据集，竖轴是我们要标记处理数据不同的方式或算法，最后一行放不开了，所以放到了右边。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/fE1V3KePrOokyBw.png"                      alt="image-20210817231631551"                ></p><h1 id="06-定位数据不匹配"><a href="#06-定位数据不匹配" class="headerlink" title="06 定位数据不匹配"></a>06 定位数据不匹配</h1><ul><li>如果发现数据不匹配问题，通常会做误差分析。还是以语音激活后视镜为例，可能要听一下来自开发集的样本，尝试弄清楚开发集和训练集到底有什么不同（比如说开发集的噪音很多）。你可以尝试把训练数据变得更像开发集一点，或者也可以收集更多类似开发集和测试集的数据（比如说发现车辆背景噪音是主要的误差来源，那么就有意识的收集这些数据加到训练集中）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/xvfHONQLCmIZiyp.png"                      alt="image-20210818105420142"                ></p><ul><li>如果你的目的是把训练数据变得更像开发集一点，可以使用人工合成数据技术，通过人工合成你可以快速制造更多的训练集。但是人工合成有一个潜在的问题，比如说你在安静的环境下录制了1000小时音频，在移动的车上只录制了1一个小时，当你重复的将这1小时的音频叠加到1000小时音频时，可能你的学习算法对这一小时汽车噪声过拟合。人工数据合成的挑战在于人耳对于1000小时听起来和1小时噪音听起来没什么区别，所以最后可能制造出这个原始数据很少的，在一个小得多的空间子集合成的训练数据。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/aocQR4YELhJHCkp.png"                      alt="image-20210818105431867"                ></p><ul><li>总结：如果你认为存在数据不匹配问题，建议做误差分析或者看看训练集和开发集，这两个数据分布到底有什么不同，然后尝试收集看起来像开发集数据做训练。当使用人工合成数据一定要谨慎，你有可能从所有可能性的空间只选了很小一部分去模拟数据。</li></ul><h1 id="07-迁移学习"><a href="#07-迁移学习" class="headerlink" title="07 迁移学习"></a>07 迁移学习</h1><p>深度学习中最强大的理念之一就是有时候神经网络可以从一个任务习得知识，并将这些知识应用到另一个独立的任务中，这就是迁移学习。</p><hr><ul><li>假设你已经训练好一个图像识别神经网络，然后将这个神经网络拿来让他适应不同的任务（比如放射科诊断），可以做的就是把神经网络最后的输出层和进入到最后一层的权重删掉，然后加上一层或者几层新层，然后为最后一层或几层重新赋予随机权重，然后让它在放射诊断数据上训练。在进行图像识别的训练阶段，可以训练神经网络的所有常用参数、所有权重、所有层，然后就得到了一个能够做图像识别的神经网络，这时要实现迁移学习你要做的就是把数据集换成放射科的（x,y）训练集，而y就是你想要预测的诊断。然后我们随机初始化最后一层的权重w和b。已有语音识别系统神经网络，然后搭建一个触发词（可以唤醒智能设备）的神经网络，过程都同上。</li><li>要用新的数据集重新训练神经网络有几种做法：<ol><li>如果你有一个小的数据集，就只训练输出层前的最后一层或者最后两层。</li><li>如果你有很多数据集，那么也许你可以重新训练网络中的所有参数。在图像识别数据的初期训练阶段（预训练），你在用图像识别数据预初始化，然后以后更新所有权重，在放射科数据上训练，有时这个过程叫微调。</li></ol></li><li>迁移学习起作用的场合是：迁移来源问题你有很多数据，但迁移目标问题你没有那么多数据。（例如图像识别有一百万样本，可以学习低层次特征，可以在神经网络的前几层学到如何识别有用的特征，放射科数据有一万样本，在已经学习特征的基础上加以训练）数据量反着来的话（例如图像识别有50样本，放射科数据有300样本），就没有太大的意义了。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/1M5Jg9GcvWof3Sp.png"                      alt="image-20210818204432468"                ></p><ul><li>假设从任务A学习并迁移一些知识到任务B，那么什么时候迁移学习有意义？<ol><li>当任务A和任务B都有同样的输入x（例如A和B输入都为图像）</li><li>当任务A数据比任务B数据多得多的时候</li><li>任务A的低层次特征可以帮助任务B的学习</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/V3lMdz7LPbtcQaT.png"                      alt="image-20210818204444568"                ></p><h1 id="08-多任务学习"><a href="#08-多任务学习" class="headerlink" title="08 多任务学习"></a>08 多任务学习</h1><p>在迁移学习中，你的步骤是串行的（从A到B）；在多任务学习中，你可以同时开始学习，试图让单个神经网络同时做几件事，然后希望每个任务都能帮到其他所有任务</p><hr><ul><li>以无人驾驶为例，需要同时识别多个不同的物体，比如行人、车辆、停车标志还有交通灯，以这四个为例设计出Y。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/BmTu24altLnpVQx.png"                      alt="image-20210818215353005"                ></p><ul><li>现在就可以训练一个神经网络来预测这些y值，输入x，输出的是一个四维向量y。要训练这个神经网络，需要定义神经网络的损失函数，和之前分类猫的例子主要区别在于需要对j&#x3D;1到4求和；这与softmax回归的主要区别在于softmax将单个标签分配给单个样本，而这个例子一张图可以有很多不同标签（行人、车辆、停车标志还有交通灯），所以在该例中你不是只给图片一个标签，而是需要遍历不同类型，然后看看每个类型是否出现在图中。</li><li>如果你训练一个神经网络，并试图最小化这个成本函数，你做的就是多任务学习，由于神经网络早期的一些特征，在识别不同物体时都会用到，你发现训练一个神经网络做四件事会比训练四个完全独立的神经网络分别做四件事性能要更好。</li><li>多任务学习也可以处理图像只有部分物体被标记的情况（右下角Y的情况，不清楚标记？），你就只对带0和1标签的j值求和，如果有？你就在求和时忽略那个项。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/qQKAh6t3OLCHsJv.png"                      alt="image-20210818215407044"                ></p><ul><li>多任务学习什么时候有意义？当三件事为真时，他就是有意义的：<ol><li>如果你训练的一组任务可以共用低层次特征，比如无人驾驶例子，同时识别四个标签，因为这些都是道路上的特征。</li><li>不一定对哈，如果你专注于单项任务，想要从多任务学习得到很大性能提升，那么其他任务加起来必须要有比单个任务大得多的数据量或者每个任务的数据量很接近。</li><li>当你可以训练一个足够大的神经网络，同时做好所有的工作。多任务学习的替代方法是训练四个完全独立的神经网络，一个任务对应一个神经网络，如果神经网络足够大，就不用替代。</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/DIRroy8ZjH5Jswx.png"                      alt="image-20210818215421479"                ></p><ul><li>总结：迁移学习用的多</li></ul><h1 id="09-什么是端对端的深度学习"><a href="#09-什么是端对端的深度学习" class="headerlink" title="09 什么是端对端的深度学习"></a>09 什么是端对端的深度学习</h1><p>深度学习中最令人振奋的最新动态之一就是端对端深度学习的兴起，以前有一些数据处理系统或者学习系统，他们需要多个阶段的处理，那么端对端深度学习就是忽略所有这些不同的阶段，用单个神经网络替代它</p><hr><ul><li>以语音识别为例，需要很多阶段的处理，首先会提取一些特征（MFCC用来从音频中提取一组特定的人工设计的特征），提取完低层次的特征后就可以应用机器学习算法在音频片段中找到音位，然后将音位串在一起构成独立的词，然后将这些词串起来构成音频片段的听写文本；端对端深度学习就是训练一个巨大的神经网络，输入一段音频，输出直接是听写文本。</li><li>端对端深度学习的挑战之一是你可能需要大量数据才能让系统表现良好。小数据集用传统的比较好（3000h）；大的数据集用端对端才好（10000h）；数据量适中也可以用中间件方法。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/pGC4uPD7JZhYWbI.png"                      alt="image-20210819082822446"                ></p><ul><li>搭建人脸识别门禁系统：第一件事是看看相机拍到的照片，你可以直接学习图像x到人物y身份的函数映射（这不是最好的方法，因为人可以从很多不同的角度接近门禁）；迄今为止最好的方法是多步方法，首先运行一个软件来检查人脸，所以第一个检测器找的是人脸位置，找到后裁剪图像使人脸居中，再喂到神经网络里去学习，将现拍的图片跟已存的图片进行对比。为什么两步法更好呢？一是你解决的两个问题，每个问题实际上要简单得多，二是两个子任务的训练数据都很多</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jkFAYgCRI9S6wzQ.png"                      alt="image-20210819082835837"                ></p><ul><li>更多的例子：机器翻译，X扫描</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ulDXp2LUSEbqzfO.png"                      alt="image-20210819082920805"                ></p><h1 id="10-是否要使用端对端的深度学习"><a href="#10-是否要使用端对端的深度学习" class="headerlink" title="10 是否要使用端对端的深度学习"></a>10 是否要使用端对端的深度学习</h1><ul><li>端对端学习的好处：<ol><li>只是让数据说话，让你的学习算法学习它想学习的任意表示方法，而不是强迫它使用特定的某种(音位)作为表示方法，其整体表现可能会更好。</li><li>所需手工设计的组件更少，不需要花太多时间去手工设计功能。</li></ol></li><li>端对端学习的缺点：<ol><li>要直接学x到y的映射，它可能需要大量的数据。</li><li>排除了可能有用的手工设计组件，当没办法从很小的训练集数据中获得洞察力，这时手工设计组件就可以把人类知识直接注入到算法。</li></ol></li><li>学习算法有两个主要知识来源：如果数据少就需要用组件，如果数据多就可能不需要<ol><li>数据</li><li>手工设计的任何东西（也可能强迫学习算法使用特定的某种(音位)作为表示方法）</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/wBOdLi6jPtYhxeW.png"                      alt="image-20210819085149773"                ></p><ul><li>决定是否使用端对端的深度学习关键的问题是你有足够的数据能够直接学从x映射到y足够复杂的函数吗？</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/36ocdZPJhYsxNne.png"                      alt="image-20210819085536883"                ></p><ul><li>总之需要大量数据</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-进行误差分析&quot;&gt;&lt;a href=&quot;#01-进行误差分析&quot; class=&quot;headerlink&quot; title=&quot;01 进行误差分析&quot;&gt;&lt;/a&gt;01 进行误差分析&lt;/h1&gt;&lt;p&gt;人工检查你的算法错误，也许可以让你了解接下来应该做什么，这个过程就叫误差分析&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day10 机器学习（ML）策略</title>
    <link href="http://example.com/2021/08/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day10%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5/"/>
    <id>http://example.com/2021/08/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day10%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5/</id>
    <published>2021-08-15T22:54:24.000Z</published>
    <updated>2022-04-13T12:04:35.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-为什么是ML策略"><a href="#01-为什么是ML策略" class="headerlink" title="01 为什么是ML策略"></a>01 为什么是ML策略</h1><ul><li>当你尝试优化一个深度学习系统时，通常有很多想法可以去试。一些策略或者一些分析机器学习问题的方法可以指引你朝着最有希望的方向前进。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/V54kjvFlypodaI2.png"                      alt="image-20210814092245145"                ></p><h1 id="02-正交化"><a href="#02-正交化" class="headerlink" title="02 正交化"></a>02 正交化</h1><p>搭建机器学习系统的挑战之一是可以尝试和改变的东西太多太多（比如：有那么多超参数需要调整）</p><hr><ul><li>以左边电视为例，正交化指的是电视设计师设置不同的旋钮，使得每个旋钮只能调整一个性质，这样就可以将画面调整到合适的位置。又如右边的车，有方向盘、油门、刹车来分别控制性质，正交化就是这些控制互不影响，能够相互独立。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/zfOAKe6l3wEiIZD.png"                      alt="image-20210814105307415"                ></p><ul><li>要弄好一个监督学习系统，通常需要调节系统的旋钮来确保四件事：<ol><li>系统在训练集上得到的结果不错，训练集上 的表现必须通过某种评估达到能接受的程度。（如果你的算法在成本函数上不能很好的拟合训练集，你需要一个旋钮来确保可以调整你的算法，让他更好的拟合训练集，用来调试的旋钮可以训练更大的网络，或者切换到更好的优化算法。）</li><li>系统在开发集上有好的表现（如果发现算法对训练集很好，对开发集的拟合很差，那么就需要一个旋钮，在不影响训练集的同时，使算法更好的拟合开发集，增大训练集可以是另一个可用旋钮。）</li><li>系统在测试集上有好的表现（如果算法对开发集很好，对测试集很差，这就可能意味着对开发集过拟合了，需要往回退一步使用更大的开发集，增大开发集可以是另一个可用旋钮。）</li><li>系统在测试集上系统的成本函数在实际使用中表现令人满意（如果在测试集做的很好，但是无法给客户好的体验，这需要回去改变开发集或成本函数）</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/S9VKQgdXJD5nc78.png"                      alt="image-20210814105322388"                ></p><h1 id="03-单一数字评估标准"><a href="#03-单一数字评估标准" class="headerlink" title="03 单一数字评估标准"></a>03 单一数字评估标准</h1><p>无论是调整超参数或者是尝试不同的学习算法亦或者是在搭建机器学习系统时尝试不同的手段，如果有一个单实数评估标准，它可以快速的告诉你新尝试的手段比之前的手段是好还是坏。</p><hr><ul><li>比如说识别猫的例子：如果我们用查准率和查全率来评估分类器（查准率：就是这个图是猫的可能性，查全率：实际为猫的图片中有多少被系统识别出来），那么将很难选择出哪个好，哪个不好，因此需要找到一个结合了查准率和查全率新的评估指标F1调和平均数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/rao1ePuqELImjkT.png"                      alt="image-20210814114358167"                ></p><ul><li>在有个例子就是各个地方的上传误差，最终使用一个平均值来衡量多个分类器，误差越小越好。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/v7psUqLIaci4ldb.png"                      alt="image-20210814114410967"                ></p><h1 id="04-满足和优化指标"><a href="#04-满足和优化指标" class="headerlink" title="04 满足和优化指标"></a>04 满足和优化指标</h1><ul><li>在该例子中，即看重查准率也看重运行时间，这时就可以将查准率和运行时间组合成一个整体评估指标；你也可以选择一个分类器，能够最大限度提升准确度，但必须满足运行时间小于等于100ms，在这种情况下我们说准确度是一个优化指标（想让准确度最大化），运行时间是满足指标（只要达到要求就行，要求以内都一样），这时B就是很好的分类器；如果你要考虑N个指标，有时候选择其中一个指标作为优化指标是合理的，剩下的均为满足指标。右边就是个例子懂了就不用看。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Kxhseauq2gcO4MW.png"                      alt="image-20210814215048347"                ></p><h1 id="05-训练-x2F-开发-x2F-测试集划分"><a href="#05-训练-x2F-开发-x2F-测试集划分" class="headerlink" title="05 训练 &#x2F; 开发 &#x2F; 测试集划分"></a>05 训练 &#x2F; 开发 &#x2F; 测试集划分</h1><ul><li>dev集也叫开发集有时称为保留交叉验证集。机器学习的工作流程是：你尝试很多思路，用训练集训练不同的模型，然后使用开发集来评估不同的思路，然后选择一个去不断地迭代来改善开发集的性能，得到一个满意的成本，然后用测试集去评估。开发集和优化指标构成了靶心，训练的目的就是向靶心靠拢，而设立训练集是加速靠近靶心的速度。</li><li>以一个猫分类器为例，在下面这些区域内运营：<ol><li>不推荐：你选择其中4个区域（可随机选取）的数据构成开发集，其他四个区域的数据构成测试集，因为开发集和测试集不在一个分布</li><li>推荐：让二者来源一个分布，将所有数据随机洗牌将其放入开发集和测试集，这样开发集和测试集都有来自八个地区的数据。</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/2lQse4GKAd8vySI.png"                      alt="image-20210814221205577"                ></p><ul><li>在设置开发集和测试集，要选择能够反映你未来会得到的数据、认为很重要的数据、必须得到好结果的数据这样的。</li></ul><h1 id="06-开发集合测试集的大小"><a href="#06-开发集合测试集的大小" class="headerlink" title="06 开发集合测试集的大小"></a>06 开发集合测试集的大小</h1><ul><li>前两个是早期数据少的时候划分，最后一个是现在数据多的时候划分。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/yBSPL4FgAXQsifz.png"                      alt="image-20210815084825602"                ></p><ul><li>测试集的目的是完成系统开发之后，测试集可以帮你评估投产系统的性能。方针就是令你的测试集足够大以至于能够以高置信度评估系统整体性能。对于某些应用也许不需要对系统性能有置信度很高的评估，这时不单独分出一个测试集也是可以的，但是不建议，因为你可以使用这组不带偏差的数据来测量系统的性能。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/e9gZSE2tUr3V1kh.png"                      alt="image-20210815085143210"                ></p><h1 id="07-什么时候该改变开发-x2F-测试集和指标"><a href="#07-什么时候该改变开发-x2F-测试集和指标" class="headerlink" title="07 什么时候该改变开发 &#x2F; 测试集和指标"></a>07 什么时候该改变开发 &#x2F; 测试集和指标</h1><p>设置开发集和评估指标就像把目标定在某个位置，让你的团队瞄准</p><hr><ul><li>假设在构建一个猫分类器，使用的指标是分类误差，从图中看是A效果比较好，但是如果A会将色情图片看成猫的图片，用户是不接受的，而B不会将色情图片看成猫的图片，用户比较倾向于B，在这种情况下（原本的错误指标错误的预测算法A是更好的算法）就应该改变评估指标了或者改变开发集和测试集。可以将分类误差指标写成下面Error的形式，这个评估指标的问题是他对色情图片和非色情图片一视同仁，其中修改评估指标的方法是添加个权重w，图片x不是的话w为1，否则为10甚至100，如果希望归一化常数就是w(i)对所有i求和，这样误差仍然在0和1之间。在开发集和测试集中需要你自己将色情图片标记才能使用这个加权函数。评估指标的意义在于准确告诉你已知两个分类器哪一个更适合你的应用。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Do3i1aNF8PcHqU9.png"                      alt="image-20210815200723853"                ></p><ul><li>这实际上就是个正交化的例子，你想处理机器学习问题时，应该把它切分成独立的步骤：第一步是弄清楚如何定义一个指标来衡量你想要做的事情的表现（就是设置目标），第二步也许就是学习算法针对某个成本函数优化，加入权重还可能要修改归一化常数（如何精确瞄准，命中目标）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/sXZU7AH4vhxk3Mg.png"                      alt="image-20210815200734731"                ></p><ul><li>如果你当前的指标和当前用来评估数据和你真正关心必须做好的事情关系不大，那就应该改变指标或者你的开发测试集。（就好比下面这个图，你一直用网上下载下来的高质量图片训练，结果使用用户上传的质量层次不齐的图片，实际测试你就发现B比A效果好）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/rz4jftMx8FZ9dTE.png"                      alt="image-20210815200748278"                ></p><ul><li>总结：不建议在没有评估指标和开发集时跑太久</li></ul><h1 id="08-为什么是人的表现"><a href="#08-为什么是人的表现" class="headerlink" title="08 为什么是人的表现"></a>08 为什么是人的表现</h1><ul><li><p>当你开始往人类水平努力时，进展很快；但过了一段时间，这个算法表现比人类更好时，那么进展和精确度的提升就变得更慢了，也许它还会越来越好，但是斜率也就越来越平缓。</p></li><li><p>贝叶斯最优误差一般认为理论上可能达到的最优误差：随着时间的推移，当您继续训练算法时，可能模型越来越大、数据越来越多，但性能无法超过某个理论上限。就是说没有任何办法设计出一个x到y的函数，让他能够超过一定的准确度。</p></li><li><p>为什么当超越人类表现后，进展就缓慢了？</p><ol><li>当超越人类表现后没有太多的空间继续改善了。</li><li>没超越之前可以使用一些工具提升性能，超越之后就没那么好用了。</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Ug5t17YhfXcsrpu.png"                      alt="image-20210815220342252"                ></p><ul><li>对于人类擅长的任务，只要你的机器学习算法比人类差，你就可以：让人帮助你标记数据，这样就有更多的数据可以给学习算法；人工误差分析：让人类看算法处理的例子，知道错误出在哪，并尝试了解为什么人能作对，算法做错；更好分析偏差和误差。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/n8ZHlcpmzuLNGsh.png"                      alt="image-20210815220403038"                ></p><h1 id="09-可避免偏差"><a href="#09-可避免偏差" class="headerlink" title="09 可避免偏差"></a>09 可避免偏差</h1><p>可避免偏差：贝叶斯误差或者对贝叶斯误差的估计和训练误差之间的差值</p><hr><ul><li>你的算法在训练集上的表现和人类水平的表现有很大差距的话，说明你的算法对训练集的拟合并不好，所以从减小偏差和方差的工具来看，重点是减小偏差，你需要做的是比如训练更大的神经网络或者跑久一点梯度下降。你的算法在训练集上的表现和人类水平的表现相近的话，但是开发集和训练集相差较大，就将重心放到减小方差上。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/vlx21D7m8bd4foJ.png"                      alt="image-20210815222311401"                ></p><h1 id="10-理解人的表现"><a href="#10-理解人的表现" class="headerlink" title="10 理解人的表现"></a>10 理解人的表现</h1><p>人类水平误差用来估计贝叶斯误差也就是理论上最低的误差，任何函数不管是现在还是将来能够到达的最低值。</p><hr><ul><li>医学图像分类的例子：下面是四个不同人的观察的误差值，那么应该如何界定人类水平误差呢？就是那个到达最低的值0.5。</li><li>在定义人类水平误差时，要弄清楚你的目标所在，如果要表明你可以超越单个人类，那么就有理由在某些场合使用1%部署你的系统，如果你的目标是代替贝叶斯误差，那么就使用0.5的比较合适。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/N1fx8vicVIdW52T.png"                      alt="image-20210815225133174"                ></p><ul><li>误差分析的例子：第一个无论以哪个为目标，都要减少偏差，技术可以培训更大的网络；第二个无论以哪个为目标，都要减少方差，技术可以使用正则化或者去获得更大的训练集；第三个就是接近人类水平误差，改善空间就比较小了。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/KC4oGA7PY1953bI.png"                      alt="image-20210815225321181"                ></p><h1 id="11-超过人的表现"><a href="#11-超过人的表现" class="headerlink" title="11 超过人的表现"></a>11 超过人的表现</h1><ul><li>如果你的误差已经比一群充分讨论辩论后的人类专家更低，那么依靠人类直觉来判断你的算法还能往什么方向优化就很难了（比如右边的，如果都已经过拟合了，这咋判断）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/nf9wbJg7NkDWMSK.png"                      alt="image-20210815230714321"                ></p><ul><li>下面的结构化数据都是系统比人做的好的，都有一个数据库。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ehvGZXPH1moRTyK.png"                      alt="image-20210815230723952"                ></p><h1 id="12-改善模型表现"><a href="#12-改善模型表现" class="headerlink" title="12  改善模型表现"></a>12  改善模型表现</h1><ul><li>提高学习算法性能的指导方针：想让一个监督学习算法达到适用要做到：<ol><li>你的算法对训练集的拟合很好（可避免偏差很低），可以训练更大的网络或者训练更长时间、使用更好的优化算法、寻找更好的新神经网络架构或者更好的超参数、改变激活函数或者层数或者隐藏单位数。</li><li>在训练集做的好推广到开发集和测试集也要好（方差不是太大），可以正则化或者收集更多训练数据、寻找更好的新神经网络架构</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NUF4uWVOPzIR3ey.png"                      alt="image-20210815232000368"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/KEdz1Tla4DOJtrU.png"                      alt="image-20210815232011048"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-为什么是ML策略&quot;&gt;&lt;a href=&quot;#01-为什么是ML策略&quot; class=&quot;headerlink&quot; title=&quot;01 为什么是ML策略&quot;&gt;&lt;/a&gt;01 为什么是ML策略&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;当你尝试优化一个深度学习系统时，通常有很多想法可以去试。</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day09 超参数调试、Batch 正则化和程序框架</title>
    <link href="http://example.com/2021/08/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day09%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/"/>
    <id>http://example.com/2021/08/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day09%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/</id>
    <published>2021-08-13T15:53:50.000Z</published>
    <updated>2022-04-13T12:04:42.438Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-调试处理"><a href="#01-调试处理" class="headerlink" title="01 调试处理"></a>01 调试处理</h1><p>系统地组织超参调试过程的技巧</p><hr><ul><li>学习速率是需要调试的最重要的超参数（红色），其次是momentum、mini-batch的大小及隐藏单元（黄色），重要性拍第三位的就是层数、学习率衰减、Adam算法（其余都是）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/j32RWxhcJb9TyiU.png"                      alt="image-20210811090955591"                ></p><ul><li>如果要调整一些超参数，该如何选择调试值呢？<ol><li>在早一代的机器学习算法中（左图），如果有两个超参数，常见的作法是在网格中取样点，然后系统的研究这些数值，然后选择哪个参数效果最好。（参数的数量相对较少时）</li><li>在深度学习领域中（右图）推荐下面做法：随机选择点，然后用随机取的点试验超参数的效果，之所以这么做是因为你 很难提前知道哪个超参数最重要。</li></ol></li><li>就举两个极端的超参数，α和ε，无论ε取何值，结果基本都是一样的。对于左边的图来说25个样点，但是α只能尝试五个值，而对于右图，每个样点都是独立的也就是α可以取25个值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Fr7OSWuYZdbfUiV.png"                      alt="image-20210811091010777"                ></p><ul><li>当你给超参数取值时，另一个惯例就是采用由粗糙到精细的策略，也就是在整个的方格中进行粗略搜索后，发现某个点的效果最好，周围的点效果也不错，这时就可以聚集到这个点周围更小的方格中，在更小的方格中，更加密集的随机取点。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/oK5kFVN8J9aHnRe.png"                      alt="image-20210811091035843"                ></p><h1 id="02-为超参数选择合适的范围"><a href="#02-为超参数选择合适的范围" class="headerlink" title="02 为超参数选择合适的范围"></a>02 为超参数选择合适的范围</h1><p>随机取值并不是在有效范围内的随机均匀取值，而是选择合适的标尺用于探究这些超参数。</p><hr><ul><li>假设选取隐藏单元的数量n^ [l]，选择范围是从50到100中某点；如果要选取神经网络的层数L，选择范围是从2到4。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ca6IO7ZVk3vhCr5.png"                      alt="image-20210811174621431"                ></p><ul><li>假设你在搜索超参数学习率α，取值范围是0.0001到1，如果沿其随机均匀取值那么90%的数值会落在0.1到1，这样看起来不对，反而用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，而是分别依次取0.0001、0.001、0.01、0.1、1，在对数轴上随机均匀取点，这样每个数值之间都有更多的搜索资源可用。用python实现如下：将超参数设置为10^ r，r取值在a到b之间，其中a与b是通过对最小值和最大值进行对数运算得出。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/v489h5bowf7uGCi.png"                      alt="image-20210811174643052"                ></p><ul><li>另一个棘手的例子就是给β取值，用来计算指数的加权平均值，取0.9就相当于在10个值中取平均值，取0.999相当于在1000个值中取平均，如果想要在0.9到0.999之间进行搜索就不能用线性轴取值，我们要探究的应该是1-β，值在0.001到0.1区间，要做的就是在[-3,-1]里随即均匀的给r取值，就相当于给β取值。为什么不能用线性轴呢？因为β接近1时，任何细微的变化都会产生大影响（1-β），所以在β趋近于1的范围内需要密集的取值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/c3ouXzgUYPvQqLd.png"                      alt="image-20210811174702165"                ></p><ul><li>即使标尺没有选对也没关系，只要数据足够就影响不大。</li></ul><h1 id="03-超参数训练的实践：Pandas-VS-Caviar"><a href="#03-超参数训练的实践：Pandas-VS-Caviar" class="headerlink" title="03 超参数训练的实践：Pandas VS Caviar"></a>03 超参数训练的实践：Pandas VS Caviar</h1><p>组织超参数搜索过程的建议和技巧</p><hr><ul><li>由于某些情况你的原来设定的超参数不好用了，建议就是每隔几个月至少一次重新测试或评估你的超参数来确保你对数值依然很满意。</li><li>关于如何搜索超参数的问题：<ol><li>照看一个模型，对其细心照料（通常是有庞大的数据组，但是没有许多计算资源或者足够的CPU和GPU的情况下），即使它在试验时也可以进行改良。因为没有足够的计算能力，不能在同一时间试验大量的模型，所以要观察它的表现，不断调整学习率。</li><li>同时试验多种模型，一视同仁：你设置一些超参数，就让他自己运行（蓝线），也可以开始一个有着不同超参数设定的不同模型，第二个模型（紫线），第三个模型（红色）等等，最后选择工作效果最好的那个。</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/aXvHDeRQxm8Tl3j.png"                      alt="image-20210812091541070"                ></p><h1 id="04-正则化网络的激活函数"><a href="#04-正则化网络的激活函数" class="headerlink" title="04 正则化网络的激活函数"></a>04 正则化网络的激活函数</h1><p>Batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更稳定，超参数的范围会更庞大，工作效果也很好，也会很容易的训练深层网络。</p><hr><ul><li>对于逻辑回归和神经网络的归一化输入特征值来说，归一化输入特征是可以加速学习过程的。</li><li>对于深层网络来说：Batch归一化的作用是：对于任何一个隐藏层而言，能够归一化a值以更快速的训练w和b。严格的来说，我们真正归一化的不是a而是z。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/LceCvta7NpYzSx2.png"                      alt="image-20210812155916904"                ></p><ul><li>单一隐含层Batch归一化的使用方法：网络中已知有些中间值，如假设有一些隐藏单元值z^ (1)到z^ (m)，接下来要计算平均值，然后计算方差，然后取每个z^ (i)规范化，化为含平均值0和方差1（**(z^ (i)-均值)&#x2F;标准偏差**），为了确保数值稳定这里我们依旧加一个ε。因为隐藏单元有不同的分布可能会有意义（例如在sigmoid激活函数，我们不希望值都集中在线性那），因此我们不想隐藏单元总是含有平均值0和方差1，所以我们开始接下来计算ztilde，其中这里的γ和β是模型的学习参数，然后使用梯度下降算法等方法更新γ和β。如果γ&#x3D;z的分母、β&#x3D;u，那么他就可以精准的转化公式使得z&#x3D;ztilde。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/XaxQZP9BTtVmIFU.png"                      alt="image-20210812160006217"                ></p><ul><li>Batch归一化的作用是它适用的归一化过程不只是输入层，也适用于神经网络中的深度隐藏层。</li></ul><h1 id="05-将Batch-Norm拟合进神经网络"><a href="#05-将Batch-Norm拟合进神经网络" class="headerlink" title="05 将Batch Norm拟合进神经网络"></a>05 将Batch Norm拟合进神经网络</h1><ul><li>Batch归一化是发生在计算z和a之间的，与其应用没有规范过的z^ [i]，不如用经方差和均值归一后的ztilde^ [i]。而且这里的β与用在momentum、Adam、RMSprop里的β不同。可以用任何优化算法来更新参数β和γ，在TensorFlow框架中可以用右下角的函数来实现Batch归一化。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/FOZmSoJV3hD8rCT.png"                      alt="image-20210813110613549"                ></p><ul><li>在实践中，Batch归一化通常和训练集的mini-batch一起使用，应用Batch归一化的方式就是：用第一个mini-batch计算z^ [1]，然后在经Batch归一化得到ztilde^ [1]，再应用激活函数得到a^ [1]，然后再一直运行下去。再用第二个mini-batch，直到用完。值得注意的一点：使用Batch归一化可以消除参数b，因为mini-batch中增加任何常数，数值都不会变，加上的任何常数都会被均值减法所抵消。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ifCPauByoETeGbm.png"                      alt="image-20210813110626854"                ></p><ul><li>用Batch归一化来应用梯度下降法-&gt;假设你在使用mini-batch梯度下降法：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/idhWQlyICB3pojP.png"                      alt="image-20210813110640100"                ></p><h1 id="06-Batch-Norm为什么奏效？"><a href="#06-Batch-Norm为什么奏效？" class="headerlink" title="06 Batch Norm为什么奏效？"></a>06 Batch Norm为什么奏效？</h1><ul><li>batch归一化有效的第一个原因是它不仅仅针对输入值，还针对隐藏单元的值，将那一层所有的值通过归一化得到类似范围的值，可加速学习。第二个原因是它可以使权重比你的神经网络更滞后或者更深层（比如第十层相比于第一层的权重更能经受得住变化）。即使存在运行都很好的同一个函数，但你不会希望你的学习算法去发现绿色的决策边界，只看左边的数据的话，可能使得你产生数据改变分布的想法（covariate shift），正如x到y的映射一样，改变下，y也随之改变。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NcqRdVIZGsQ8J6g.png"                      alt="image-20210813160313036"                ></p><ul><li>batch归一化做的是它减小了这些隐藏值分布变化的数量，就以z^ [2] _1与z^ [2] _1为例，即使它的值改变了，至少他们的均值和方差也会是均值0和方差1，亦或者是由β和γ决定的其他均值与方差。直观来说batch归一化减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其他层，有利于加速整个网络的学习。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/q3zdSCKryb1JPYe.png"                      alt="image-20210813160357980"                ></p><ul><li>batch归一化另一个作用就是他有轻微的正则化效果，因为在mini-batch上计算的均值和方差，均值和方差都会有一些小噪音；缩放过程从z^ [l]到ztilde^ [l]也会有些噪音，因为它是用有噪音的均值和方差计算得出的，所以和dropout相似，他往每个隐藏层的激活值上增加了噪音，dropout含几重噪音是因为它以一定概率乘以0或1，batch归一化含几重噪音是因为标准差的缩放和减去均值带来的额外噪音。因为给隐藏单元添加了噪音，迫使后面单元不过分依赖任何一个隐藏单元，因为噪音很小，所以不是巨大的正则化效果，可以将batch归一化和dropout一块使用；dropout的一个特性是mini-batch越大，正则化效果越弱。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/51r9HwFJRZxgYut.png"                      alt="image-20210813160415039"                ></p><ul><li>注意：batch归一化一次只能处理一个mini-batch数据</li></ul><h1 id="07-测试时的-Batch-Norm"><a href="#07-测试时的-Batch-Norm" class="headerlink" title="07 测试时的 Batch Norm"></a>07 测试时的 Batch Norm</h1><p>Batch归一化将数据以mini-batch的形式逐一处理，但是在测试时，需要对每一个样本逐一处理</p><hr><ul><li>我们用m来表示一个mini-batch中的样本数量，在测试过程中不可能将一个mini-batch中所有样本同时处理，所以需要用其他方法得到u和σ^ 2。为了将神经网络应用于测试就需要单独估算u和σ^ 2。在这里我们用指数加权平均来估算（这个平均数涵盖了所有mini-batch），训练l层的每一个mini-batch都会得到一个u值。可以用指数加权平均来追踪在这一层所有mini-batch中所见的σ^ 2的值，也可以用来追踪均值向量的最新平均值，因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的u和σ^ 2的平均数的实时数值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/23w9QiHnZLKWoFl.png"                      alt="image-20210813164723790"                ></p><ul><li>总结：在训练时u和σ^ 2是在整个mini-batch上计算出来的，但在测试时需要逐一处理样本，方法是根据你的训练集通过运用指数加权平均估算出u和σ^ 2，然后用测试中的u和σ^ 2来进行你所需的隐藏单元z值得调整。</li></ul><h1 id="08-Softmax回归"><a href="#08-Softmax回归" class="headerlink" title="08 Softmax回归"></a>08 Softmax回归</h1><p>Softmax回归能让你在试图识别某一分类时做出预测（识别多种分类）</p><hr><ul><li>我们用C来表示有几个种类，我们想要输出层单元的数字告诉我们这四种类型中每一个的概率有多大，最后输出一个4*1矩阵。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/DiGeqdKOB3ocluV.png"                      alt="image-20210813222931774"                ></p><ul><li>让网络做到这一点的标准模型要用到Softmax层以及输出层来生成输出。算出z之后就需要应用Softmax激活函数（这个激活函数对于Softmax层而言有些不同）：首先计算一个适用于每个元素的临时变量t&#x3D;e的z^ [l]次方,然后经过计算得到a^ [l]（t经过归一化使和为1），举例右边：之前我们的激活函数都是接受单行数值输入（例如Sigmoid和ReLU激活函数输入一个实数输出一个实数），Softmax激活函数因为需要将所有可能的输出归一化，所以就需要输入一个向量，最后再输出一个向量。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/FLs8xhBNSy12TKd.png"                      alt="image-20210813222806913"                ></p><ul><li>Softmax分类器在没有隐藏层的情况下能后做到的事–线性决策边界，更深层的网络可以学习更复杂的非线性决策边界。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/d7smV56SUWEOupP.png"                      alt="image-20210813223008277"                ></p><h1 id="09-训练一个Softmax分类器"><a href="#09-训练一个Softmax分类器" class="headerlink" title="09 训练一个Softmax分类器"></a>09 训练一个Softmax分类器</h1><ul><li>hard max函数将概率最大的变为1，其他变为0。soft max函数相对比较柔和，该是多少概率就是多少。值得注意的是：如果C&#x3D;2，那么Softmax实际就变回了逻辑回归。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jIJhzRxtGpUDVY4.png"                      alt="image-20210813230021529"                ></p><ul><li>怎样训练带有Softmax输出层的神经网络？我们应首先定义损失函数（左边是单个训练样本的损失，右边是整个训练集的损失J），下方是使用向量化实现矩阵大写Y。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/takqvK1x2SNzc7s.png"                      alt="image-20210813230234480"                ></p><ul><li>在有Softmax输出层时实现梯度下降法：输出层会计算出z^ [l] ( C * 1维)，然后用Softmax激活函数得到a^ [l]，然后由此算出损失。初始化反向传播所需的关键步骤或者说关键方程是dz那个表达式</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/mdCWRnMUXJ83N7T.png"                      alt="image-20210813230338115"                ></p><h1 id="10-深度学习框架"><a href="#10-深度学习框架" class="headerlink" title="10 深度学习框架"></a>10 深度学习框架</h1><ul><li>深度学习的一些框架及选择框架的标准：一个重要的标准就是便于编程（神经网络的开发、迭代、为产品进行配置），第二个标准是运行速度（特别是训练大数据集），第三个标准是这个框架是否真的开放(长时间开源)。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/sDO47ZFPSxvqw2V.png"                      alt="image-20210813231615002"                ></p><h1 id="11-TensorFlow"><a href="#11-TensorFlow" class="headerlink" title="11 TensorFlow"></a>11 TensorFlow</h1><ul><li>假设有一个J需要最小化，来使用TensorFlow将其最小化：第一行和第二行是引入库，接下来是将w初始化为0，然后定义损失函数，然后定义train为学习算法（用梯度下降优化器使损失函数最小化），下面的两行是惯用的表达，开启了一个TensorFlow session，接下来是初始化全局变量，然后用TensorFlow评估一个变量，然后运行梯度下降法再输出w。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/iKahp71CP6eMcG2.png"                      alt="image-20210813232819815"                ></p><ul><li>现在我们运行梯度下降1000次迭代，然后输出w</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/8yfqhuVizGYnNQR.png"                      alt="image-20210813233207951"                ></p><ul><li>TensorFlow中的placeholder是之后会赋值的变量，这种方法便于将训练数据加入损失方程（feed_dict函数）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/w2MnhjgvREylsfI.png"                      alt="image-20210813233854025"                ></p><ul><li>TensorFlow已经内置了所有必要的反向函数，通过内置函数来计算前向函数，它就能自动用反向函数实现反向传播。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NuJjvPUFgW4YG26.png"                      alt="image-20210813234621008"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-调试处理&quot;&gt;&lt;a href=&quot;#01-调试处理&quot; class=&quot;headerlink&quot; title=&quot;01 调试处理&quot;&gt;&lt;/a&gt;01 调试处理&lt;/h1&gt;&lt;p&gt;系统地组织超参调试过程的技巧&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;学习速率是需要调试的最重要的超参数</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络：超参数调试、正则化以及优化第二周检测</title>
    <link href="http://example.com/2021/08/10/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%E7%AC%AC%E4%BA%8C%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/08/10/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%E7%AC%AC%E4%BA%8C%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-08-10T13:13:38.000Z</published>
    <updated>2022-04-13T12:01:48.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="答案见下方"><a href="#答案见下方" class="headerlink" title="答案见下方"></a>答案见下方</h1><hr><h2 id="1-当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？"><a href="#1-当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？" class="headerlink" title="1.当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？"></a>1.当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？</h2><p>A.a^ [3]{8}(7)</p><p>B.a^ [8]{7}(3)</p><p>C.a^ [8]{3}(7)</p><p>D.a^ [3]{7}(8)</p><h2 id="2-关于mini-batch的说法哪个是正确的？"><a href="#2-关于mini-batch的说法哪个是正确的？" class="headerlink" title="2. 关于mini-batch的说法哪个是正确的？"></a>2. 关于mini-batch的说法哪个是正确的？</h2><p>A.mini-batch迭代一次（计算1个mini-batch），要比批量梯度下降迭代一次快</p><p>B.用mini-batch训练完整个数据集一次，要比批量梯度下降训练完整个数据集一次快</p><p>C.在不同的mini-batch下，不需要显式地进行循环，就可以实现mini-batch梯度下降，从而使算法同时处理所有的数据（矢量化）</p><h2 id="3-为什么最好的mini-batch的大小通常不是1也不是m，而是介于两者之间？"><a href="#3-为什么最好的mini-batch的大小通常不是1也不是m，而是介于两者之间？" class="headerlink" title="3.为什么最好的mini-batch的大小通常不是1也不是m，而是介于两者之间？"></a>3.为什么最好的mini-batch的大小通常不是1也不是m，而是介于两者之间？</h2><p>A.如果mini-batch的大小是1，那么在你取得进展前，你需要遍历整个训练集</p><p>B.如果mini-batch的大小是m，就会变成批量梯度下降。在你取得进展前，你需要遍历整个训练集</p><p>C.如果mini-batch的大小是1，那么你将失去mini-batch将数据矢量化带来的的好处</p><p>D.如果mini-batch的大小是m，就会变成随机梯度下降，而这样做经常会比mini-batch慢</p><h2 id="4-如果你的模型的成本J随着迭代次数的增加，绘制出来的图如下，那么："><a href="#4-如果你的模型的成本J随着迭代次数的增加，绘制出来的图如下，那么：" class="headerlink" title="4.如果你的模型的成本J随着迭代次数的增加，绘制出来的图如下，那么："></a>4.如果你的模型的成本J随着迭代次数的增加，绘制出来的图如下，那么：</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.kesci.com/upload/image/q48rj172dh.png?imageView2/0/w/960/h/960"                      alt="Image Name"                ></p><p>A.如果你正在使用mini-batch梯度下降，那可能有问题；而如果你在使用批量梯度下降，那是合理的</p><p>B.如果你正在使用mini-batch梯度下降，那看上去是合理的；而如果你在使用批量梯度下降，那可能有问题</p><p>C.无论你在使用mini-batch还是批量梯度下降，看上去都是合理的</p><p>D.无论你在使用mini-batch还是批量梯度下降，都可能有问题</p><h2 id="5-假设一月的前三天卡萨布兰卡的气温是一样的：一月第一天-θ1-x3D-10"><a href="#5-假设一月的前三天卡萨布兰卡的气温是一样的：一月第一天-θ1-x3D-10" class="headerlink" title="5.假设一月的前三天卡萨布兰卡的气温是一样的：一月第一天: θ1&#x3D;10"></a>5.假设一月的前三天卡萨布兰卡的气温是一样的：一月第一天: θ1&#x3D;10</h2><p>一月第二天: θ2&#x3D;10,假设您使用β&#x3D;0.5的指数加权平均来跟踪温度：v0&#x3D;0,vt&#x3D;βv_t−1+(1−β)θ_t。如果v2是在没有偏差修正的情况下计算第2天后的值，并且v2corrected是您使用偏差修正计算的值。 这些下面的值是正确的是？</p><p>A.v2&#x3D;10,v2corrected&#x3D;10</p><p>B.v2&#x3D;10,v2corrected&#x3D;7.5</p><p>C.v2&#x3D;7.5,v2corrected&#x3D;7.5</p><p>D.v2&#x3D;7.5,v2corrected&#x3D;10</p><h2 id="6-下面哪一个不是比较好的学习率衰减方法？"><a href="#6-下面哪一个不是比较好的学习率衰减方法？" class="headerlink" title="6.下面哪一个不是比较好的学习率衰减方法？"></a>6.下面哪一个不是比较好的学习率衰减方法？</h2><p>A.α&#x3D;1&#x2F;(1+2∗t) α0</p><p>B.α&#x3D;1&#x2F;sqrt(t) α0</p><p>C.α&#x3D;0.95^ t α0</p><p>D.α&#x3D;e^ t α0</p><h2 id="7-您在伦敦温度数据集上使用指数加权平均，-使用以下公式来追踪温度：vt-x3D-βv-t−1-1−β-θt。下图中红线使用的是β-x3D-0-9来计算的。当你改变β时，你的红色曲线会怎样变化？（选出所有正确项）"><a href="#7-您在伦敦温度数据集上使用指数加权平均，-使用以下公式来追踪温度：vt-x3D-βv-t−1-1−β-θt。下图中红线使用的是β-x3D-0-9来计算的。当你改变β时，你的红色曲线会怎样变化？（选出所有正确项）" class="headerlink" title="7.您在伦敦温度数据集上使用指数加权平均， 使用以下公式来追踪温度：vt&#x3D;βv_t−1+(1−β)θt。下图中红线使用的是β&#x3D;0.9来计算的。当你改变β时，你的红色曲线会怎样变化？（选出所有正确项）"></a>7.您在伦敦温度数据集上使用指数加权平均， 使用以下公式来追踪温度：vt&#x3D;βv_t−1+(1−β)θt。下图中红线使用的是β&#x3D;0.9来计算的。当你改变β时，你的红色曲线会怎样变化？（选出所有正确项）</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.kesci.com/upload/image/q4c2nyuibb.png?imageView2/0/w/960/h/960"                      alt="Image Name"                ></p><p>A.减小β，红色线会略微右移<br>B.增加β，红色线会略微右移<br>C.减小β，红线会更加震荡<br>D.增加β，红线会更加震荡</p><h2 id="8-下图中的曲线是由：梯度下降，动量梯度下降（β-x3D-0-5）和动量梯度下降（β-x3D-0-9）。哪条曲线对应哪种算法？"><a href="#8-下图中的曲线是由：梯度下降，动量梯度下降（β-x3D-0-5）和动量梯度下降（β-x3D-0-9）。哪条曲线对应哪种算法？" class="headerlink" title="8.下图中的曲线是由：梯度下降，动量梯度下降（β&#x3D;0.5）和动量梯度下降（β&#x3D;0.9）。哪条曲线对应哪种算法？"></a>8.下图中的曲线是由：梯度下降，动量梯度下降（β&#x3D;0.5）和动量梯度下降（β&#x3D;0.9）。哪条曲线对应哪种算法？</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.kesci.com/upload/image/q4c3gp9w4.png?imageView2/0/w/960/h/960"                      alt="Image Name"                ></p><p>A.(1)是梯度下降；(2)是动量梯度下降（β&#x3D;0.9）；(3)是动量梯度下降（β&#x3D;0.5）</p><p>B.(1)是梯度下降；(2)是动量梯度下降（β&#x3D;0.5）；(3)是动量梯度下降（β&#x3D;0.9）</p><p>C.(1)是动量梯度下降（β&#x3D;0.5）；(2)是动量梯度下降（β&#x3D;0.9）；(3)是梯度下降</p><p>D.(1)是动量梯度下降（β&#x3D;0.5）；(2)是梯度下降；(3)是动量梯度下降（β&#x3D;0.9）</p><h2 id="9-假设在一个深度学习网络中，批量梯度下降花费了大量时间时来找到一组参数值，使成本函数J-W-1-b-1-…-W-L-b-L-小。以下哪些方法可以帮助找到J值较小的参数值？"><a href="#9-假设在一个深度学习网络中，批量梯度下降花费了大量时间时来找到一组参数值，使成本函数J-W-1-b-1-…-W-L-b-L-小。以下哪些方法可以帮助找到J值较小的参数值？" class="headerlink" title="9.假设在一个深度学习网络中，批量梯度下降花费了大量时间时来找到一组参数值，使成本函数J(W[1],b[1],…,W[L],b[L])小。以下哪些方法可以帮助找到J值较小的参数值？"></a>9.假设在一个深度学习网络中，批量梯度下降花费了大量时间时来找到一组参数值，使成本函数J(W[1],b[1],…,W[L],b[L])小。以下哪些方法可以帮助找到J值较小的参数值？</h2><p>A.令所有权重值初始化为0</p><p>B.尝试调整学习率</p><p>C.尝试mini-batch梯度下降</p><p>D.尝试对权重进行更好的随机初始化</p><p>E.尝试使用 Adam 算法</p><h2 id="10-关于Adam算法，下列哪一个陈述是错误的？"><a href="#10-关于Adam算法，下列哪一个陈述是错误的？" class="headerlink" title="10.关于Adam算法，下列哪一个陈述是错误的？"></a>10.关于Adam算法，下列哪一个陈述是错误的？</h2><p>A.Adam结合了Rmsprop和动量的优点</p><p>B.Adam中的学习率超参数α通常需要调整</p><p>C.我们经常使用超参数的“默认”值β1&#x3D;0,9,β2&#x3D;0.999,ϵ&#x3D;10−8</p><p>D.Adam应该用于批梯度计算，而不是用于mini-batch</p><hr><h2 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h2><ol><li>A</li><li>C</li><li>BC</li><li>B</li><li>D</li><li>D</li><li>BC</li><li>B</li><li>BCDE</li><li>D</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;答案见下方&quot;&gt;&lt;a href=&quot;#答案见下方&quot; class=&quot;headerlink&quot; title=&quot;答案见下方&quot;&gt;&lt;/a&gt;答案见下方&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;1-当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？&quot;&gt;</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络：超参数调试、正则化以及优化第一周检测</title>
    <link href="http://example.com/2021/08/10/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%E7%AC%AC%E4%B8%80%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/08/10/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%E7%AC%AC%E4%B8%80%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-08-10T09:37:24.000Z</published>
    <updated>2022-04-13T12:01:23.833Z</updated>
    
    <content type="html"><![CDATA[<h2 id="答案见下方"><a href="#答案见下方" class="headerlink" title="答案见下方"></a>答案见下方</h2><hr><h2 id="1-如果你有10-000-000个例子，你会如何划分训练-x2F-开发-x2F-测试集？"><a href="#1-如果你有10-000-000个例子，你会如何划分训练-x2F-开发-x2F-测试集？" class="headerlink" title="1.如果你有10,000,000个例子，你会如何划分训练&#x2F;开发&#x2F;测试集？"></a>1.如果你有10,000,000个例子，你会如何划分训练&#x2F;开发&#x2F;测试集？</h2><p>A.33%训练，33%开发，33%测试</p><p>B.60%训练，20%开发，20%测试</p><p>C.98%训练，1%开发，1%测试</p><h2 id="2-开发和测试集应该："><a href="#2-开发和测试集应该：" class="headerlink" title="2.开发和测试集应该："></a>2.开发和测试集应该：</h2><p>A.来自同一分布</p><p>B.来自不同分布</p><p>C.完全相同（一样的(x, y)对）</p><p>D.数据数量应该相同</p><h2 id="3-如果你的神经网络方差很高，下列哪个尝试是可能解决问题的？"><a href="#3-如果你的神经网络方差很高，下列哪个尝试是可能解决问题的？" class="headerlink" title="3.如果你的神经网络方差很高，下列哪个尝试是可能解决问题的？"></a>3.如果你的神经网络方差很高，下列哪个尝试是可能解决问题的？</h2><p>A.添加正则项</p><p>B.获取更多测试数据</p><p>C.增加每个隐藏层的神经元数量</p><p>D.用更深的神经网络</p><p>E.用更多的训练数据</p><h2 id="4-你正在为苹果，香蕉和橘子制作分类器。-假设您的分类器在训练集上有0-5％的错误，以及开发集上有7％的错误。-以下哪项尝试是有希望改善你的分类器的分类效果的？"><a href="#4-你正在为苹果，香蕉和橘子制作分类器。-假设您的分类器在训练集上有0-5％的错误，以及开发集上有7％的错误。-以下哪项尝试是有希望改善你的分类器的分类效果的？" class="headerlink" title="4.你正在为苹果，香蕉和橘子制作分类器。 假设您的分类器在训练集上有0.5％的错误，以及开发集上有7％的错误。 以下哪项尝试是有希望改善你的分类器的分类效果的？"></a>4.你正在为苹果，香蕉和橘子制作分类器。 假设您的分类器在训练集上有0.5％的错误，以及开发集上有7％的错误。 以下哪项尝试是有希望改善你的分类器的分类效果的？</h2><p>A.增大正则化参数λ</p><p>B.减小正则化参数λ</p><p>C.获取更多训练数据</p><p>D.用更大的神经网络</p><h2 id="5-什么是权重衰减？"><a href="#5-什么是权重衰减？" class="headerlink" title="5.什么是权重衰减？"></a>5.什么是权重衰减？</h2><p>A.正则化技术（例如L2正则化）导致梯度下降在每次迭代时权重收缩</p><p>B.在训练过程中逐渐降低学习率的过程</p><p>C.如果神经网络是在噪声数据下训练的，那么神经网络的权值会逐渐损坏</p><p>D.通过对权重值设置上限来避免梯度消失的技术</p><h2 id="6-当你增大正则化的超参数λ时会发生什么？"><a href="#6-当你增大正则化的超参数λ时会发生什么？" class="headerlink" title="6.当你增大正则化的超参数λ时会发生什么？"></a>6.当你增大正则化的超参数λ时会发生什么？</h2><p>A.权重变小（接近0）</p><p>B.权重变大（远离0）</p><p>C.2倍的λ导致2倍的权重</p><p>D.每次迭代，梯度下降采取更大的步距（与λ成正比）</p><h2 id="7-在测试时候使用dropout："><a href="#7-在测试时候使用dropout：" class="headerlink" title="7.在测试时候使用dropout："></a>7.在测试时候使用dropout：</h2><p>A.不随机关闭神经元，但保留1&#x2F;keep_brob因子</p><p>B.随机关闭神经元，保留1&#x2F;keep_brob因子</p><p>C.随机关闭神经元，但不保留1&#x2F;keep_brob因子</p><p>D.不随机关闭神经元，也不保留1&#x2F;keep_brob因子</p><h2 id="8-将参数keep-prob从（比如说）0-5增加到0-6可能会导致以下情况（选出所有正确项）："><a href="#8-将参数keep-prob从（比如说）0-5增加到0-6可能会导致以下情况（选出所有正确项）：" class="headerlink" title="8.将参数keep_prob从（比如说）0.5增加到0.6可能会导致以下情况（选出所有正确项）："></a>8.将参数keep_prob从（比如说）0.5增加到0.6可能会导致以下情况（选出所有正确项）：</h2><p>A.正则化效应被增强</p><p>B.正则化效应被减弱</p><p>C.训练集的误差会增加</p><p>D.训练集的误差会减小</p><h2 id="9-以下哪些技术可用于减少方差（减少过拟合）？（选出所有正确项）"><a href="#9-以下哪些技术可用于减少方差（减少过拟合）？（选出所有正确项）" class="headerlink" title="9.以下哪些技术可用于减少方差（减少过拟合）？（选出所有正确项）"></a>9.以下哪些技术可用于减少方差（减少过拟合）？（选出所有正确项）</h2><p>A.梯度消失</p><p>B.数据扩充</p><p>C.Dropout</p><p>D.梯度检查</p><p>E.Xavier初始化</p><p>F.L2正则化</p><p>G.梯度爆炸</p><h2 id="10-为什么要对输入x进行归一化？"><a href="#10-为什么要对输入x进行归一化？" class="headerlink" title="10.为什么要对输入x进行归一化？"></a>10.为什么要对输入x进行归一化？</h2><p>A.让参数初始化更快</p><p>B.让代价函数更快地优化</p><p>C.更容易做数据可视化</p><p>D.是另一种正则化——有助减少方差</p><hr><h2 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h2><ol><li>C</li><li>A</li><li>AE</li><li>AC</li><li>A</li><li>A</li><li>D(dropout只用在训练集上，目的是在每层添加噪声，降低对权重的依赖，从而防止过拟合。但是测试的时候不能用dropout，否则会影响评估.)</li><li>BD(在编写tensorflow程序的时候，会发现训练的时候dropout的参数keep_prob&#x3D;0.8（0.5,0.9等等），在测试的时候keep_prob&#x3D;1.0，即不进行dropout。)</li><li>BCF</li><li>B</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;答案见下方&quot;&gt;&lt;a href=&quot;#答案见下方&quot; class=&quot;headerlink&quot; title=&quot;答案见下方&quot;&gt;&lt;/a&gt;答案见下方&lt;/h2&gt;&lt;hr&gt;
&lt;h2 id=&quot;1-如果你有10-000-000个例子，你会如何划分训练-x2F-开发-x2F-测试集？&quot;&gt;&lt;a</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day07 08 优化算法</title>
    <link href="http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day07%2008%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day07%2008%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</id>
    <published>2021-08-09T13:34:16.000Z</published>
    <updated>2022-04-13T12:04:48.735Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-Mini-batch-梯度下降法"><a href="#01-Mini-batch-梯度下降法" class="headerlink" title="01 Mini-batch 梯度下降法"></a>01 Mini-batch 梯度下降法</h1><ul><li>X代表训练样本的集合，向量化能够相对较快地处理所有m个样本，但是如果m过大，处理速度仍然缓慢。在对整个训练集执行梯度下降法时，原本是需要处理完所有样本，才能进行一步梯度下降法，然后再重新处理所有样本，才能进行下一步梯度下降，但是如果在处理完所有样本之前，先让梯度下降法处理一部分，那么算法速度将会更快。因此我们可以将训练集分割为小一点地子集（子集就叫Mini-batch ），我们用X^ {t}代表第t个子集，当然Y也需要分割。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/4c3feUrkKyldG8t.png"                      alt="image-20210809115435646"                ></p><ul><li>在使用Mini-batch 梯度下降法时，同样要设有一个循环，这个循环是遍历所有的子训练集，同样的也要计算前向传播、代价函数J及执行后向传播来计算J^ {t}的梯度，只使用X^ {t}和Y^ {t}，然后更新权值。这只是遍历了一次训练集，如果还想多次遍历需要使用for或者while。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NXuBdKUqezHTah3.png"                      alt="image-20210809115452369"                ></p><h1 id="02-理解-mini-batch-梯度下降法"><a href="#02-理解-mini-batch-梯度下降法" class="headerlink" title="02 理解 mini-batch 梯度下降法"></a>02 理解 mini-batch 梯度下降法</h1><ul><li>Batch梯度下降法：成本函数J会随着每次迭代而减小；mini-batch 梯度下降法：由于每次迭代都在训练不同的样本集（X^ {t}和Y^ {t}），所以成本函数J会产生波动，但是总体趋势是下降的。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/GVwbdzjuntBERpl.png"                      alt="image-20210809161351943"                ></p><ul><li><p>决定mini-batch的大小有两种极端的情况：</p><ol><li>mini-batch &#x3D; m（整个训练集）–&gt;Batch梯度下降法（如果m很大，则单次迭代耗时太长）</li><li>mini-batch &#x3D; 1–&gt;随机梯度下降法（会失去所有向量化带来的加速，效率过于低下，并且结果永远都不会收敛，会一直在最小值附近徘徊）</li></ol></li><li><p>在这两种极端情况下，成本函数的优化情况：蓝色（Batch梯度下降法），紫色（随机梯度下降法）</p></li><li><p>所以要选择不大不小的mini-batch尺寸，实际上学习速率达到最快：一方面得到大量向量化，另一方面不需要等待整个训练集被处理完，就可以开始进行后续工作了。当然mini-batch 梯度下降法的结果也都不会收敛，会一直在最小值附近徘徊，这时可以慢慢减小学习率。</p></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/udjWegblTLxPzpk.png"                      alt="image-20210809161408930"                ></p><ul><li>那么该如何选择mini-batch尺寸呢：如果训练集较小，直接使用batch梯度下降（m&lt;2000）；如果训练集较大，mini-batch尺寸一般选择在64~512，经常将mini-batch尺寸设成2次方，这样代码会运行的比较快。最后要注意的是在mini-batch中X^ {t}和Y^ {t}要符合CPU&#x2F;GPU内存，如果不相符无论使用什么方法，算法都会表现得急转直下。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/hiESCG8g3uqxRJH.png"                      alt="image-20210809161425277"                ></p><h1 id="03-指数加权平均"><a href="#03-指数加权平均" class="headerlink" title="03 指数加权平均"></a>03 指数加权平均</h1><ul><li>计算下面温度散点图的趋势或者是温度的局部平均值或者说移动平均值：首先V_0&#x3D;0，然后运用公式：某天的V等于前一天的V的0.9倍加上当日温度的0.1倍，这样计算后用红线作图就得到了移动平均值&#x2F;每日温度的指数加权平均值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ZXuoW39PtdLc6y1.png"                      alt="image-20210809174303382"                ></p><ul><li>大体公式如下所示：在计算平均多少天温度时用1&#x2F;(1-b)来计算，如果b&#x3D;0.9将其带入该式子，可得10也就是十天的平均值，如果b为0.98就是绿色的线为50（平均过去50天的温度），如果b为0.5得到黄色线。b这个参数的大小决定了上一个值得权重。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/qdMmAVh8vtTr42j.png"                      alt="image-20210809174314626"                ></p><h1 id="04-理解指数加权平均"><a href="#04-理解指数加权平均" class="headerlink" title="04 理解指数加权平均"></a>04 理解指数加权平均</h1><ul><li><p>我们首先画一个每天温度得图（右上第一个），然后构建一个指数衰减函数（右上第二个）看sita的系数，sita99就是0.1×0.9、sita98就是0.1×(0.9)^ 2，以此类推画出图。计算V_100就是讲两个图对应的元素相乘然后求和。sita的系数加起来为1或者逼近1，我们称之为偏差修正，正是因为有偏差修正，这才是指数加权平均数。</p></li><li><p>那么到底需要平均多少天数呢：就是看几天后权重下降到不到当日权重的三分之一（设b等于一个数，他需要多少次方才能达到1&#x2F;e，多少次方用1&#x2F;(1-b)来计算）</p></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/x7XF8Ed4S2QLvkH.png"                      alt="image-20210809203021545"                ></p><ul><li>如何在实际中执行：看右图就是初始化v，然后直接讲这个v带入公式来更新新的v，不断地更新。注意：左图是&#x3D;，右图是：&#x3D;</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/AQXmcBVMoUF8npT.png"                      alt="image-20210809203344689"                ></p><h1 id="05-指数加权平均的偏差修正"><a href="#05-指数加权平均的偏差修正" class="headerlink" title="05 指数加权平均的偏差修正"></a>05 指数加权平均的偏差修正</h1><ul><li>如果你在设置b为0.98时不是绿色的线而是起点低的紫色的曲线，那么我们就不要用V_t，而是用V_t&#x2F;(1-b^t)。假如t&#x3D;2，经过计算就是1号和2号数据的加权平均数并除去偏差，随着t的增加，b的t次方接近0，所以当t很大时，偏差修正几乎没有作用，因此当t很大时紫色线基本和绿色线重合了。如果关心初始时期的偏差，那么在刚开始计算指数加权移动平均数的时候，偏差修正能帮助在早期获得更好的估测。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/IaR6ms1nZdKeuYT.png"                      alt="image-20210809213123541"                ></p><h1 id="06-动量梯度下降法"><a href="#06-动量梯度下降法" class="headerlink" title="06 动量梯度下降法"></a>06 动量梯度下降法</h1><p>还有一种算法叫Momentum梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本思想是：计算梯度的指数加权平均数，并用该梯度更新权重。</p><hr><ul><li>Momentum梯度下降法在第t次迭代过程中，用现有的Mini-batch计算dW、db 。然后通过指数加权平均计算得到dw及db的移动平均数，最后重新赋值权重，这样就可以减缓梯度下降的幅度（即纵轴方向的摆动小了），也可以增加横轴方向的运动速度（红色线）。如果要最小化碗装函数，其中dW、db相当于为你从山上往下滚的球提供了加速度，V_dW、V_db相当于速度。由于β稍小于1，相当于有摩擦力，所以球不会无限的加速下去。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/6xOB9wFaWsrc2V5.png"                      alt="image-20210810094409383"                ></p><ul><li>最后我们看看具体如何计算：就用左边的公式包含1-β，因为右边的公式，如果你最后要调整超参数β，那么就会影响V_dW、V_db，也许还要修改学习率α（α要根据1&#x2F;(1-β)相应变化）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jv6ueB3X7aSFxsC.png"                      alt="image-20210810094341929"                ></p><h1 id="07-RMSprop（均方根）"><a href="#07-RMSprop（均方根）" class="headerlink" title="07 RMSprop（均方根）"></a>07 RMSprop（均方根）</h1><p>RMSprop也可以加速梯度下降，因为将微分进行平方，最后还使用了平方根所以叫均方根</p><hr><ul><li>分析这个纵轴幅度大，横轴向前推进的例子，假设纵轴代表参数b，横轴代表参数w，想要减缓b方向的学习同时推进横轴方向的学习，RMSprop算法可以将其实现。同样的RMSprop也会在第t次迭代中计算当下mini-batch的微分dW、db，接着通过计算得到(dw)^ 2及(db)^ 2的加权平均数，在更新参数时也做了些许改动，公式如下：从斜率可以看出纵轴的斜率比横轴的大（dW&gt;db）结果就变为绿色线。同样还可以用一个较大的学习率α来加快学习，而无须在纵轴上垂直方向偏离。为了保证数值能稳定一些，需要确保分母不为0，就要在分母上加一个很小很小的ε。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Upy9rfWe7qEkcYD.png"                      alt="image-20210810110609785"                ></p><h1 id="08-Adam算法"><a href="#08-Adam算法" class="headerlink" title="08 Adam算法"></a>08 Adam算法</h1><p>Adam算法基本就是将Momentum和RMSprop结合在一起</p><hr><ul><li>使用Adam算法，首先要初始化，接着在第t次迭代中用当前的mini-batch计算dW、db，接下来计算momentum指数加权平均数，然后用RMSprop进行更新，相当于momentum更新了超参数β_1，RMSprop更新了β_2。一般使用Adam算法时要计算偏差修正，最后就可以更新权重了。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/kCNoc5eZ16HAnQT.png"                      alt="image-20210810115601945"                ></p><ul><li>其中有几个超参数：<ol><li>学习率α可以尝试一系列的值看哪个有效</li><li>β_1常用值为0.9，这是dW &#x2F; db的移动平均值也就是加权平均数</li><li>β_2常用值为0.999，这是(dW)^ 2 &#x2F; (db)^ 2的移动平均值也就是加权平均数</li><li>ε建议是10^ -8，但是没有必要设置</li></ol></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/thHVFCMTQx1paej.png"                      alt="image-20210810115615201"                ></p><h1 id="09-学习率衰退"><a href="#09-学习率衰退" class="headerlink" title="09 学习率衰退"></a>09 学习率衰退</h1><p>加快学习算法的一个办法就是随着时间慢慢减小学习率，我们称之为学习率衰退</p><hr><ul><li>蓝色的是使用同一学习率，而绿色的是随着时间慢慢减小学习率。慢慢减小学习率的本质在于：在学习初期你能承受较大的步伐，但当开始收敛时，小的学习率能让步伐小一些，不必在最小值范围内大幅度摆动。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/E865sAzXTqpdPtU.png"                      alt="image-20210810122936242"                ></p><ul><li>可以这样来进行学习率衰退：第一次遍历训练集叫做一代，第二次就是二代。可以将学习率设为（1&#x2F;(1+衰退率* 第几代）*α_0，α_0为初始学习率。如果想使用学习率衰退就需要不断地尝试参数α及超参数衰退率，找到合适的值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/t4hUfm8nvbVrGxF.png"                      alt="image-20210810122952569"                ></p><ul><li>除了学习率衰减的公式，人们还使用指数衰减等其他公式：其中t代表mini-batch的数字。还会用离散下降，学习率一会减一半一会减一半。当然还可以手动调试学习率（在训练集小的时候）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/pzARCWeoIQ2bfGu.png"                      alt="image-20210810123005014"                ></p><h1 id="10-局部最优的问题"><a href="#10-局部最优的问题" class="headerlink" title="10 局部最优的问题"></a>10 局部最优的问题</h1><ul><li>在高维度空间更有可能碰到右图的鞍点（导数为0的点），而不会碰到局部最优。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/rB4D2CGUapyLv9F.png"                      alt="image-20210810125524030"                ></p><ul><li>平稳段会减缓学习。慢慢下降到平稳段，然后在走出平稳段。</li><li>在训练较大的神经网络存在大量参数并且成本函数J被定义在较高的维度空间时，不大可能困在极差的局部最优中。</li><li>平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或者是RMSprop或者是Adam这样的算法加快学习算法的地方，让你尽早走出平稳段。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/qQWz8PhG9vbgUM1.png"                      alt="image-20210810125537786"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-Mini-batch-梯度下降法&quot;&gt;&lt;a href=&quot;#01-Mini-batch-梯度下降法&quot; class=&quot;headerlink&quot; title=&quot;01 Mini-batch 梯度下降法&quot;&gt;&lt;/a&gt;01 Mini-batch 梯度下降法&lt;/h1&gt;&lt;ul&gt;
</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day06 深度学习的实用层面</title>
    <link href="http://example.com/2021/08/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day06%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/"/>
    <id>http://example.com/2021/08/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day06%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/</id>
    <published>2021-08-08T14:07:11.000Z</published>
    <updated>2022-04-13T12:04:54.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-训练-x2F-开发-x2F-测试集"><a href="#01-训练-x2F-开发-x2F-测试集" class="headerlink" title="01 训练 &#x2F; 开发 &#x2F; 测试集"></a>01 训练 &#x2F; 开发 &#x2F; 测试集</h1><ul><li>应用型机器学习是一个高度迭代的过程（想法-&gt;代码-&gt;实现），循环该过程的效率是决定项目进展速度的一个关键因素，创建高质量的训练集、验证集、测试集也有助于提高循环效率。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/DSxRVLuKpOgJNyd.png"                      alt="image-20210806161754138"                ></p><ul><li>随着数据量的不断增加（从1000个样本到1000000个样本），那么验证集和测试集占数据总量的比例会趋向于变得更小。如果数据有一百万，那么就可以选择一万条作验证集，一万条作测试集。因为验证集的目的就是验证不同的算法，检验哪种算法最有效，同样的根据最终选择的分类器，测试集的主要目的是正确评估分类器的性能，选择这么多数据就足够了。（训练集98%，验证集1%，测试集1%）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/vtEC2qVdHk17o5Y.png"                      alt="image-20210806162107456"                ></p><ul><li>总结：现代深度学习的一个趋势：在训练和测试集分布不匹配的情况下进行训练（比如说训练集数据是从网上整下来的，验证集和测试集是用户上传的），针对于这种情况要确保验证集和测试集的数据来自同一分布。最后一点就是没有测试集也没关系（如果不需要无偏估计），如果只有验证集没有测试集，就应该在训练集上训练尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出合适的模型 。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/7fnzWmodxagwCYr.png"                      alt="image-20210806162137497"                ></p><h1 id="02-偏差-x2F-方差"><a href="#02-偏差-x2F-方差" class="headerlink" title="02 偏差 &#x2F; 方差"></a>02 偏差 &#x2F; 方差</h1><p>关于深度学习的误差问题就是要对偏差、方差的权衡。</p><hr><ul><li>分为欠拟合（高偏差） | 适度拟合 | 过度拟合（高方差）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/EgIKkZJf4nYHLhO.png"                      alt="image-20210807102539471"                ></p><ul><li>理解偏差和方差的两个关键数据是训练集误差和验证集误差。下面分别是基于人眼误差为0的情况下，高方差 | 高偏差 | 高偏差+高方差 | 低偏差+低方差。（以上分析的前提都是假设基本误差很小，训练集和验证集来自相同分布）如果最优误差（贝叶斯误差）为15%，那么第二组数据就是低偏差+低方差。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/fjcuJr2yKGWPdTM.png"                      alt="image-20210807102555855"                ></p><ul><li>下面用紫色线画出的分类器具有高偏差和高方差，高偏差是因为它几乎是一条线性分类器，并未拟合数据，高方差是因为采用曲线函数或二次函数，灵活性太高以致拟合了这两个错误样本。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/u2ZcwayY7RUOT4P.png"                      alt="image-20210807102611204"                ></p><ul><li>总结：通过分析训练集和验证集验证算法产生的误差来诊断算法是否存在高偏差或者高方差，以此来决定接下来你要做什么</li></ul><h1 id="03-机器学习基础"><a href="#03-机器学习基础" class="headerlink" title="03 机器学习基础"></a>03 机器学习基础</h1><ul><li>首先检查偏差，如果偏差过高，甚至无法拟合训练集，那么就需要选择一个新网络（含有更多的隐层或者隐藏单元）；或者花费更多的时间来训练网络（花费更多的时间训练算法或者尝试更先进的优化算法）；或者从多种神经网络架构中选择一种，通常采用规模更大的网络都会有所帮助。不断地重复这些步骤，直到偏差降低到可接受范围。</li><li>一旦偏差降低到可接受的范围，检查下方差有没有问题（评估方差要查看验证集性能），如果方差过高，需要采用更多的数据也许会有帮助；或者通过正则化来减少过拟合；找到更合适的网络架构。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ZOU1cMAfbrJiduz.png"                      alt="image-20210807154603673"                ></p><h1 id="04-正则化"><a href="#04-正则化" class="headerlink" title="04 正则化"></a>04 正则化</h1><ul><li>逻辑回归函数中的正则化：L2正则化是最常见的，还有L1正则化，如果使用L1正则化，w最终会是稀疏的（w向量中有很多0）。在python中我们使用lambd来代替lambda正则化参数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/iTkZuKNzQcO5GfL.png"                      alt="image-20210808091957852"                ></p><ul><li>神经网络实现L2正则化：字母L代表神经网络的层数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/8UyILZGqO6CvQwJ.png"                      alt="image-20210808092010146"                ></p><h1 id="05-为什么正则化可以减少过拟合？"><a href="#05-为什么正则化可以减少过拟合？" class="headerlink" title="05 为什么正则化可以减少过拟合？"></a>05 为什么正则化可以减少过拟合？</h1><ul><li>直观的理解就是lambda增加到足够大，w会接近于0，在这个过程中她会出现拟合正合适的情况，逐渐的会变成高偏差。直觉上我们会认为大量的隐藏单元被完全消除了，实际上依然存在，只是他们的影响变得更小了。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/zJQFSw4CqxYGt8B.png"                      alt="image-20210808095419078"                ></p><ul><li>假设我们用的是tanh()函数，z在很小的范围内，图像是呈线性的，如果z变得更大或者更小，图像将呈非线性。如果神经网络每层都是线性的，那么整个网络就是线性网络，最终我们只能计算线性函数，因此它不适合非常复杂的决策。总结：如果正则化参数变得很大，参数w很小，z也会相对变小，整个神经网络会计算离线性函数近的值，这个线性函数非常简单不会发生过拟合。</li><li>还有一点值得注意：为了调试梯度下降，一定要使用新定义的J函数，它包含了第二个正则化项，否则函数J可能不会在所有调幅范围内都单调递减。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/JKZaoeAkL5mnhfs.png"                      alt="image-20210808095442984"                ></p><h1 id="06-Dropout正则化"><a href="#06-Dropout正则化" class="headerlink" title="06 Dropout正则化"></a>06 Dropout正则化</h1><ul><li>Dropout（随机失活）工作流程：假设左图存在过拟合，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率（每一个节点都以抛硬币的方式设置概率即每个节点得以保留和消除的概率都是0.5），我们在消除一些节点的同时也会删除该节点进出的连线，最后得到一个节点更少、规模更小的网络。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Jrx6TVuDgXnk4S3.png"                      alt="image-20210808121838170"                ></p><ul><li>如何实施dropout：inverted dropout（反向随机失活）-&gt;首先定义一个向量d，d^ 3表示一个三层的dropout向量；然后看它是否小于某个数（keep-prob），keep-prob表示保留某个隐藏单元的概率，它的作用就是生成随机矩阵，这个d3就是个布尔类型的数组，值为1或者0；接下来是从第三层中获取激活函数（a3），a3等于上面的a3与d3元素相乘，它的作用就是过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%；最后向外扩展a3，通过除以keep-prob来确保a3的期望值不变，假设预期a^ [3]预期减少20%，为了不影响z^ [4]的期望值，就需要用w^ [4]*a^ [3]除以0.8，它将会修正我们所需的那20%。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/CXtxvLIMOARTkQb.png"                      alt="image-20210808121710187"                ></p><ul><li>我们在测试阶段不使用dropout，因为在测试阶段我们不期望输出的结果是随机的。测试阶段不同于训练阶段，即使在测试阶段不执行dropout来调整数值范围，激活函数的预期结果也不会发生变化。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/2Bnb1patl4kzhcM.png"                      alt="image-20210808121649464"                ></p><h1 id="07-理解Dropout"><a href="#07-理解Dropout" class="headerlink" title="07 理解Dropout"></a>07 理解Dropout</h1><ul><li>左图中用紫色圈起来的单元，它不能依靠任何特征，因为特征（该单元的输入）都有可能被随机清除。通过为单元的四个输入增加一点权重，Dropout将产生收缩权重的平方范数的效果。实施Dropout的结果是会压缩权重并完成一些预防过拟合的外层正则化。Dropout的功能类似于L2正则化，与L2正则化不同的是：被应用的方式不同，Dropout也会有所不同，甚至更适用于不同的输入范围。为了预防矩阵的过拟合，对权重最大的矩阵（例如右图的w^ [2]&#x3D;7×7）的那一层，keep-prob值应该相对较低，而且每一层的keep-prob值都有可能不同，如果某一层keep-prob为1，那么就不对这层使用Dropout。</li><li>在计算机视觉领域由于通常没有足够的数据，所以一直存在过拟合，所以Dropout在这是很热门的。Dropout的一个大缺点就是代价函数J不被明确定义，为了绘制学习曲线确保每次迭代都是呈下降趋势，通常会关闭Dropout函数，然后运行代码确保J函数单调递减，最后在打开Dropout。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jFPkWd2q4YLTwEH.png"                      alt="image-20210808153353102"                ></p><ul><li>总结：如果你担心某些层比其他层更加容易过拟合，那么就将某些层的keep-prob值设置相对较低，缺点是为了使用交叉验证，需要搜索更多的超级参数。另一种方案是一些层用Dropout，一些层不用，应用的层只含有一个超级参数就是keep-prob</li></ul><h1 id="08-其他正则化方法"><a href="#08-其他正则化方法" class="headerlink" title="08 其他正则化方法"></a>08 其他正则化方法</h1><ul><li>数据扩增可以作为正则化方法使用：对图片进行水平翻转，扩大裁剪，对数字进行扭曲等操作。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/5elauZS8y9sb7fT.png"                      alt="image-20210808162621924"                ></p><ul><li>early stopping代表提前停止训练神经网络。我们在绘制验证集误差时会发现。验证集误差通常会呈下降趋势，然后在某一节点开始上升，因此在迭代的过程中选择中间w的值（就是那个拐点），early stopping的优点是只运行一次坡度下降，就可以找到w的较小值、中间值和较大值，而无需尝试L2正则化超级参数lambda的很多值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/kBOTQGpcFNbvr6g.png"                      alt="image-20210808162639281"                ></p><h1 id="09-正则化输入"><a href="#09-正则化输入" class="headerlink" title="09 正则化输入"></a>09 正则化输入</h1><p>训练神经网络，其中一个加速训练的方法就是归一化输入：归一化的目的就是让特征值保持在相似的范围内</p><hr><ul><li>归一化输入有两个步骤：第一步是零均值化（左下公式）变为第二个图，第二步是归一化方差（右下公式）变为第三个图，第二个图的x_1的方差明显比x_2的方差要大-&gt;第三个图x_1的方差和x_2的方差一样大。我们希望不论是训练数据还是测试数据都是通过相同的u和sita^ 2定义的相同数据转换，其中u和sita^ 2是由训练集数据计算得出的。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/RLjO2othmpvKud7.png"                      alt="image-20210808181124568"                ></p><ul><li>如果使用非归一化特征（特征值不在一个相似的范围内），会得到一个非常细长狭窄的代价函数（左图）并且需要的学习率也要小，如果归一化特征后（特征值处于相似范围内），代价函数看起来更加的对称（右图）而且设置的学习率可以较大。如果特征的范围都很相似，那么将对优化算法很有利（下方x的取值），否则不利（上方x的取值）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ZJQnVwRkijFsEYy.png"                      alt="image-20210808181137915"                ></p><ul><li>总结：如果特征值不在一个相似的范围内，那么归一化将会显得格外重要，如果特征值处于相似范围内，那么归一化就不是很重要了。</li></ul><h1 id="10-梯度消失与梯度爆炸"><a href="#10-梯度消失与梯度爆炸" class="headerlink" title="10 梯度消失与梯度爆炸"></a>10 梯度消失与梯度爆炸</h1><p>当你训练神经网络时，导数或者坡度有时会变得非常大或者非常小，甚至以指数方式变小，这加大了训练难度。我们应更加明智的选择随机初始化权重，从而避免这个问题。</p><hr><ul><li>该例子中g（z）&#x3D;z，b^ [l]&#x3D;0。当权重w只比1略大一点或者说比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长，如果w比1略小一点，在神经网络中激活函数将以指数级递减。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/h6X38FCtTJ4VUzK.png"                      alt="image-20210808203127077"                ></p><h1 id="11-神经网络的权重初始化"><a href="#11-神经网络的权重初始化" class="headerlink" title="11 神经网络的权重初始化"></a>11 神经网络的权重初始化</h1><ul><li>神经单元权重初始化：为了防止z值过大或过小，当n越大你希望w_i越小，最合理的方法就是设置w_i&#x3D;1&#x2F;n，其中这里的n代表神经元的输入特征数量，实际上就是设置某层权重矩阵w。如果用的是Relu激活函数而不是1&#x2F;n，方差设置为2&#x2F;n更好。这里用n^ [l-1]是因为一般情况下l层的每个神经元都有n^ [l-1]个输入。</li><li>其他变体函数的公式：Tanh激活函数为右边第一个式子；Relu激活函数是左边画框的；有时也用右边第二个</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/AgKEYzaNMTPk3LB.png"                      alt="image-20210808205829700"                ></p><h1 id="12-梯度的数值逼近"><a href="#12-梯度的数值逼近" class="headerlink" title="12 梯度的数值逼近"></a>12 梯度的数值逼近</h1><p>梯度检验的作用是确保backprop正确实施</p><hr><ul><li>单边计算：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/alpyERwhz4Z18LG.png"                      alt="image-20210808210925721"                ></p><ul><li>使用双边误差的方法更逼近导数，双边计算出来是3.0001，而单边计算出来是3.0301，可以看出双边更加接近3。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/M2UcAmtYkE8ZOKR.png"                      alt="image-20210808210940411"                ></p><h1 id="13-梯度检验"><a href="#13-梯度检验" class="headerlink" title="13 梯度检验"></a>13 梯度检验</h1><ul><li>首先做些处理：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/QeZG3sbtj7Toyi5.png"                      alt="image-20210808213755158"                ></p><ul><li>为了实施梯度检验，要做的就是循环执行。先将J展开，然后进行循环，我们要验证的就是dsita_approx与dsita这两个向量是否真的接近，一般做下列运算：计算这两个向量的距离（dsita_approx-dsita的欧几里得范数，注意没有平方，它是误差平方之和，然后求平方根得到欧式距离），然后用向量长度做归一化，结果为||dsita_approx-dsita||&#x2F;||dsita_approx||+||dsita||。分母只是用来预防这些向量太小或太大，分母使这个方程式变成比率。如果计算方程式得到的值为10的-7次方甚至更小，这就很好；如果在10的-5次方就要小心了，检查这个向量所有的项确保没有一项误差过大；如果是10的-3那么就要担心是否存在bug了，看看是否有个具体的i值使得dsita_approx与dsita大不相同。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/nvMwEskfLJ9lxDm.png"                      alt="image-20210808213819586"                ></p><h1 id="14-关于梯度检验实现的注记"><a href="#14-关于梯度检验实现的注记" class="headerlink" title="14 关于梯度检验实现的注记"></a>14 关于梯度检验实现的注记</h1><p>如何在神经网络实现梯度检验的适用技巧和注意事项</p><hr><ul><li>不要在训练中适用梯度检验，它只用于调试</li><li>如果算法的梯度检验失败，要检查所有项，并试图找出bug：如果两个向量的值相差很大 ，我们要查找不同的i值，看看是哪个导致的。</li><li>在实施梯度检验时，如果使用正则化，请注意正则项</li><li>梯度检验不能与dropout同时使用：因为每次迭代过程中dropout会随机消除隐层单元的不同子集，难以计算dropout在梯度下降上的代价函数J</li><li>在随机初始化过程中，当w和b接近0时，梯度下降&#x2F;backprop的实施是正确的；但在运行梯度下降时w和b变得更大，也就越来越不准确。这时要做的就是在随机初始化过程中运行梯度检验，然后训练网络，w和b会有一段时间远离0，如果随机初始化值比较小，反复训练网络之后再重新运行梯度检验。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/OG8TsrDMlcpiyKz.png"                      alt="image-20210808214519694"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-训练-x2F-开发-x2F-测试集&quot;&gt;&lt;a href=&quot;#01-训练-x2F-开发-x2F-测试集&quot; class=&quot;headerlink&quot; title=&quot;01 训练 &amp;#x2F; 开发 &amp;#x2F; 测试集&quot;&gt;&lt;/a&gt;01 训练 &amp;#x2F; 开发 &amp;#x2</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>1编程作业：线性回归</title>
    <link href="http://example.com/2021/08/06/1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://example.com/2021/08/06/1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2021-08-06T03:59:57.000Z</published>
    <updated>2022-04-13T04:20:14.559Z</updated>
    
    <content type="html"><![CDATA[<p>原任务是在Octave&#x2F;MATLAB实现，本次编程作业全部以python完成。</p><hr><h1 id="01-简单的练习"><a href="#01-简单的练习" class="headerlink" title="01 简单的练习"></a>01 简单的练习</h1><p>总结下题目：输出一个5*5的单位矩阵</p><hr><ul><li>在此我们用np.eye(<em>N</em>,<em>M&#x3D;None</em>, <em>k&#x3D;0</em>, <em>dtype&#x3D;&lt;type ‘float’&gt;</em>)，首先N代表的是输出方阵的维度，第二个参数不用设置默认M&#x3D;N，主要看第三个参数，默认是对角线为1，其余全为0；如果k为正数，则对角线往上第k个全为1，其余全为0；如果k为负数，则对角线往下第k个全为1，其余全为0。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.eye(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[1. 0. 0. 0. 0.]</span></span><br><span class="line"><span class="string"> [0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="string"> [0. 0. 1. 0. 0.]</span></span><br><span class="line"><span class="string"> [0. 0. 0. 1. 0.]</span></span><br><span class="line"><span class="string"> [0. 0. 0. 0. 1.]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h1 id="02-单变量线性回归"><a href="#02-单变量线性回归" class="headerlink" title="02 单变量线性回归"></a>02 单变量线性回归</h1><p>根据这座城市的人口数量及该城市小吃店的利润，来预测开小吃店的利润。</p><hr><h2 id="2-1-绘制数据"><a href="#2-1-绘制数据" class="headerlink" title="2.1 绘制数据"></a>2.1 绘制数据</h2><ul><li>读入数据：在此我们引入pandas库，该库可以帮助我们从诸如 csv 类型的文件导入数据，并且可以用它快速的对数据进行转换和过滤的操作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">path = <span class="string">&quot;machine-learning-ex1\machine-learning-ex1\ex1\ex1data1.txt&quot;</span></span><br><span class="line">data = pd.read_csv(path,header=<span class="literal">None</span>,names=[<span class="string">&#x27;Population&#x27;</span>,<span class="string">&#x27;Profit&#x27;</span>])<span class="comment">#header决定要不要原始的表头，name给出自定义的表头。</span></span><br><span class="line"><span class="built_in">print</span>(data.head())<span class="comment">#从头查询数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   population   profit</span></span><br><span class="line"><span class="string">0      6.1101  17.5920</span></span><br><span class="line"><span class="string">1      5.5277   9.1302</span></span><br><span class="line"><span class="string">2      8.5186  13.6620</span></span><br><span class="line"><span class="string">3      7.0032  11.8540</span></span><br><span class="line"><span class="string">4      5.8598   6.8233</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>数据可视化：在此我们引入matplotlib.pyplot库，使用plot函数画图。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data.plot(kind=<span class="string">&#x27;scatter&#x27;</span>, x=<span class="string">&#x27;Population&#x27;</span>, y=<span class="string">&#x27;Profit&#x27;</span>, figsize=(<span class="number">12</span>,<span class="number">8</span>))<span class="comment">#生成图形，kind‘指定所画图的类型，figsize 指定图片大小。</span></span><br><span class="line">plt.show()<span class="comment">#显示图形</span></span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/TExH9s13tMWIioB.png"                      alt="image-20210805080023050"                ></p><h2 id="2-2-梯度下降"><a href="#2-2-梯度下降" class="headerlink" title="2.2 梯度下降"></a>2.2 梯度下降</h2><p>这部分需要使用梯度下降将线性回归参数 θ 拟合到数据集上。</p><h3 id="2-21-公式"><a href="#2-21-公式" class="headerlink" title="2.21 公式"></a>2.21 公式</h3><ul><li>代价函数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/3lprR9vik6M2dOG.png"                      alt="image-20210805112600247"                ></p><ul><li>假设函数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/xYra86AfkZ14HcR.png"                      alt="image-20210805112723564"                ></p><ul><li>参数更新</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/vSkcFgEYuK1do4I.png"                      alt="image-20210805115411195"                ></p><ul><li>随着梯度下降不断地更新参数，参数也就越接近使代价函数最小的最优值</li></ul><h3 id="2-22-实现"><a href="#2-22-实现" class="headerlink" title="2.22 实现"></a>2.22 实现</h3><ul><li>我们要为我们之前读取的数据添加一列x，用来更新θ_0。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>) <span class="comment">#相当于在第0列，添加一个表头名为Ones，并且该列均为1</span></span><br><span class="line"><span class="built_in">print</span>(data.head())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Ones  Population   Profit</span></span><br><span class="line"><span class="string">0     1      6.1101  17.5920</span></span><br><span class="line"><span class="string">1     1      5.5277   9.1302</span></span><br><span class="line"><span class="string">2     1      8.5186  13.6620</span></span><br><span class="line"><span class="string">3     1      7.0032  11.8540</span></span><br><span class="line"><span class="string">4     1      5.8598   6.8233</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>分割X和y。使用pandas的iloc来进行选择训练集X和目标y</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割X和y</span></span><br><span class="line">lists = data.shape[<span class="number">1</span>]<span class="comment">#输出列数</span></span><br><span class="line">X = data.iloc[:,:-<span class="number">1</span>]<span class="comment">#X是第一列到最后一列，但不包括最后一列，因为 python的范围/切片不包括终点</span></span><br><span class="line">y = data.iloc[:,lists-<span class="number">1</span>:lists]<span class="comment">#最后一列</span></span><br><span class="line"><span class="comment">#y = data.iloc[:,-1]#也是最后一列</span></span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Ones  Population</span></span><br><span class="line"><span class="string">0     1      6.1101</span></span><br><span class="line"><span class="string">1     1      5.5277</span></span><br><span class="line"><span class="string">2     1      8.5186</span></span><br><span class="line"><span class="string">3     1      7.0032</span></span><br><span class="line"><span class="string">4     1      5.8598</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(y.head())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Profit</span></span><br><span class="line"><span class="string">0  17.5920</span></span><br><span class="line"><span class="string">1   9.1302</span></span><br><span class="line"><span class="string">2  13.6620</span></span><br><span class="line"><span class="string">3  11.8540</span></span><br><span class="line"><span class="string">4   6.8233</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>我们还要将θ初始化为0，并将θ、X、y全部转化为矩阵</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(X.shape)<span class="comment">#(97, 2)</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)<span class="comment">#(97, 1)</span></span><br><span class="line"><span class="built_in">print</span>(theta.shape)<span class="comment">#(1, 2)</span></span><br></pre></td></tr></table></figure><h3 id="2-23-计算J-θ"><a href="#2-23-计算J-θ" class="headerlink" title="2.23 计算J(θ)"></a>2.23 计算J(θ)</h3><ul><li>计算代价函数来检测代价函数的收敛性。根据上面的公式我们写出代价函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">computeCost</span>(<span class="params">X, y, theta</span>):</span><br><span class="line">    inner = np.power((X * theta.T)-y,<span class="number">2</span>)<span class="comment">#数组元素求n次方</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(inner) / (<span class="number">2</span> * <span class="built_in">len</span>(X))</span><br><span class="line"><span class="built_in">print</span>(computeCost(X, y, theta)) <span class="comment">#32.072733877455676</span></span><br></pre></td></tr></table></figure><h3 id="2-24-梯度下降"><a href="#2-24-梯度下降" class="headerlink" title="2.24 梯度下降"></a>2.24 梯度下降</h3><p>代价函数J(θ)的参数是由向量θ表示，假设你已经实现了梯度下降，如果计算正确，J(θ)的值不应该增加，而应该减小然后在算法结束时收敛到一个稳定值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X, y, theta, alpha, iters</span>):</span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))<span class="comment">#创建0矩阵[[0. 0.]]</span></span><br><span class="line">    parameters = <span class="built_in">int</span>(theta.ravel().shape[<span class="number">1</span>]) <span class="comment">#ravel()将多维数组转换为一维数组,.shape[1]是看列数为多少-2</span></span><br><span class="line">    cost = np.zeros(iters)<span class="comment">#初始化代价函数数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        error = (X * theta.T) - y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / <span class="built_in">len</span>(X)) * np.<span class="built_in">sum</span>(term))<span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1500</span></span><br><span class="line">g, cost = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"><span class="built_in">print</span>(g)<span class="comment">#[[-3.63029144  1.16636235]]</span></span><br><span class="line">predict1 = [<span class="number">1</span>,<span class="number">3.5</span>]*g.T</span><br><span class="line"><span class="built_in">print</span>(predict1)<span class="comment">#[[0.45197679]]</span></span><br><span class="line">predict2 = [<span class="number">1</span>,<span class="number">7</span>]*g.T</span><br><span class="line"><span class="built_in">print</span>(predict2)<span class="comment">#[[4.53424501]]</span></span><br></pre></td></tr></table></figure><h3 id="2-3-调试"><a href="#2-3-调试" class="headerlink" title="2.3 调试"></a>2.3 调试</h3><ul><li>python可视化：原始数据以及拟合的直线</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在指定的间隔内返回均匀间隔的数字：从data.Population的最小值到最大的范围内，等间距的返回100个样本</span></span><br><span class="line">x = np.linspace(data.Population.<span class="built_in">min</span>(), data.Population.<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)<span class="comment">#参数为最优值的直线</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))<span class="comment">#创建一个12*8的图即多维窗口</span></span><br><span class="line">ax.plot(x, f, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Prediction&#x27;</span>) <span class="comment">#定义x, y, 颜色，图例上显示的东西</span></span><br><span class="line">ax.scatter(data.Population, data.Profit, label=<span class="string">&#x27;Traning Data&#x27;</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)<span class="comment">#指定图例的位置</span></span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Population&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Profit&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Predicted Profit vs. Population Size&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/v5wimh6HzINd8lE.png"                      alt="image-20210805233002972"                ></p><h1 id="03-多变量线性回归"><a href="#03-多变量线性回归" class="headerlink" title="03 多变量线性回归"></a>03 多变量线性回归</h1><p>根据ex1data2.txt里的数据建立模型，预测房屋的价格，其中第一列是房屋大小，第二列是卧室数量，第三列是房屋售价</p><hr><ul><li>第一步依旧是读入数据：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;machine-learning-ex1\machine-learning-ex1\ex1\ex1data2.txt&#x27;</span></span><br><span class="line">data2 = pd.read_csv(path,header = <span class="literal">None</span>,names=[<span class="string">&#x27;Size&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(data2.head())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> Size  Bedrooms   Price</span></span><br><span class="line"><span class="string">0  2104         3  399900</span></span><br><span class="line"><span class="string">1  1600         3  329900</span></span><br><span class="line"><span class="string">2  2400         3  369000</span></span><br><span class="line"><span class="string">3  1416         2  232000</span></span><br><span class="line"><span class="string">4  3000         4  539900</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="3-1-特征归一化"><a href="#3-1-特征归一化" class="headerlink" title="3.1 特征归一化"></a>3.1 特征归一化</h2><p>特征缩放的目的只是为了运行更快。使特征值比较接近，使图像变得比较圆。以至于梯度下降的速度更快，收敛所需要的迭代次数更少，收敛更快。</p><hr><ul><li>mean()函数功能：求取均值，std()函数是用来求标准差的（std &#x3D; sqrt(mean(abs(x - x.mean())**2))）。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line"><span class="built_in">print</span>(data2.head())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">      Size  Bedrooms     Price</span></span><br><span class="line"><span class="string">0  0.130010 -0.223675  0.475747</span></span><br><span class="line"><span class="string">1 -0.504190 -0.223675 -0.084074</span></span><br><span class="line"><span class="string">2  0.502476 -0.223675  0.228626</span></span><br><span class="line"><span class="string">3 -0.735723 -1.537767 -0.867025</span></span><br><span class="line"><span class="string">4  1.257476  1.090417  1.595389</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="3-2-梯度下降"><a href="#3-2-梯度下降" class="headerlink" title="3.2 梯度下降"></a>3.2 梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data2.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cols = data2.shape[<span class="number">1</span>]</span><br><span class="line">X2 = data2.iloc[:,<span class="number">0</span>:cols-<span class="number">1</span>]</span><br><span class="line">y2 = data2.iloc[:,cols-<span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line">X2 = np.matrix(X2.values)</span><br><span class="line">y2 = np.matrix(y2.values)</span><br><span class="line">theta2 = np.matrix(np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)</span><br><span class="line"><span class="built_in">print</span>(g2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[-1.10898288e-16  8.84042349e-01 -5.24551809e-02]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="3-3-正规方程"><a href="#3-3-正规方程" class="headerlink" title="3.3 正规方程"></a>3.3 正规方程</h2><p>训练集特征矩阵为 X（包含了x_0&#x3D;1）训练集结果为向量 y，则利用正规方程解出向量:其中np.linalg.inv()：矩阵求逆。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/cAfMsjPdLED5gJB.png"                      alt="image-20210806114131781"                ></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normalEqn</span>(<span class="params">X, y</span>):</span><br><span class="line">    theta = ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)</span><br><span class="line">    <span class="comment"># theta = np.linalg.inv(X.T@X)@X.T@y</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line">theta2=normalEqn(X2, y2)</span><br><span class="line"><span class="built_in">print</span>(theta2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[-7.11223170e-17]</span></span><br><span class="line"><span class="string"> [ 8.84765988e-01]</span></span><br><span class="line"><span class="string"> [-5.31788197e-02]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="04-代码总结"><a href="#04-代码总结" class="headerlink" title="04 代码总结"></a>04 代码总结</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">path = <span class="string">&quot;machine-learning-ex1\machine-learning-ex1\ex1\ex1data1.txt&quot;</span></span><br><span class="line">data = pd.read_csv(path,header=<span class="literal">None</span>,names=[<span class="string">&#x27;Population&#x27;</span>,<span class="string">&#x27;Profit&#x27;</span>])<span class="comment">#header决定要不要原始的表头，name给出自定义的表头。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#data.plot(kind=&#x27;scatter&#x27;, x=&#x27;Population&#x27;, y=&#x27;Profit&#x27;, figsize=(12,8))#生成图形，kind‘指定所画图的类型，figsize 指定图片大小。</span></span><br><span class="line"><span class="comment"># plt.show()#显示图形</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>) <span class="comment">#相当于在第0列，添加一个表头名为Ones，并且该列均为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割X和y</span></span><br><span class="line">lists = data.shape[<span class="number">1</span>]<span class="comment">#输出列数</span></span><br><span class="line">X = data.iloc[:,:-<span class="number">1</span>]<span class="comment">#X是第一列到最后一列，但不包括最后一列，因为 python的范围/切片不包括终点</span></span><br><span class="line">y = data.iloc[:,lists-<span class="number">1</span>:lists]<span class="comment">#最后一列</span></span><br><span class="line"><span class="comment">#y = data.iloc[:,-1]#也是最后一列</span></span><br><span class="line"></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeCost</span>(<span class="params">X, y, theta</span>):</span><br><span class="line">    inner = np.power((X * theta.T)-y,<span class="number">2</span>)<span class="comment">#数组元素求n次方</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(inner) / (<span class="number">2</span> * <span class="built_in">len</span>(X))</span><br><span class="line"><span class="comment"># print(computeCost(X, y, theta)) #32.072733877455676</span></span><br><span class="line"><span class="comment">#梯度下降算法如下：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X, y, theta, alpha, iters</span>):</span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))<span class="comment">#创建0矩阵[[0. 0.]]</span></span><br><span class="line">    parameters = <span class="built_in">int</span>(theta.ravel().shape[<span class="number">1</span>]) <span class="comment">#ravel()将多维数组转换为一维数组,.shape[1]是看列数为多少</span></span><br><span class="line">    cost = np.zeros(iters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        error = (X * theta.T) - y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / <span class="built_in">len</span>(X)) * np.<span class="built_in">sum</span>(term))</span><br><span class="line"></span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1500</span></span><br><span class="line">g, cost = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"><span class="comment">#print(g)#[[-3.63029144  1.16636235]]</span></span><br><span class="line">predict1 = [<span class="number">1</span>,<span class="number">3.5</span>]*g.T</span><br><span class="line"><span class="comment">#print(predict1)#[[0.45197679]]</span></span><br><span class="line">predict2 = [<span class="number">1</span>,<span class="number">7</span>]*g.T</span><br><span class="line"><span class="comment">#print(predict2)#[[4.53424501]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在指定的间隔内返回均匀间隔的数字：从data.Population的最小值到最大的范围内，等间距的返回100个样本</span></span><br><span class="line">x = np.linspace(data.Population.<span class="built_in">min</span>(), data.Population.<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)<span class="comment">#参数为最优值的直线</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))<span class="comment">#创建一个12*8的图即多维窗口</span></span><br><span class="line">ax.plot(x, f, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Prediction&#x27;</span>) <span class="comment">#定义x, y, 颜色，图例上显示的东西</span></span><br><span class="line">ax.scatter(data.Population, data.Profit, label=<span class="string">&#x27;Traning Data&#x27;</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)<span class="comment">#指定图例的位置</span></span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Population&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Profit&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Predicted Profit vs. Population Size&#x27;</span>)</span><br><span class="line"><span class="comment">#plt.show()</span></span><br><span class="line"><span class="comment">#===========================================================================</span></span><br><span class="line">path = <span class="string">&#x27;machine-learning-ex1\machine-learning-ex1\ex1\ex1data2.txt&#x27;</span></span><br><span class="line">data2 = pd.read_csv(path,header = <span class="literal">None</span>,names=[<span class="string">&#x27;Size&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line"></span><br><span class="line">data2.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">cols = data2.shape[<span class="number">1</span>]</span><br><span class="line">X2 = data2.iloc[:,<span class="number">0</span>:cols-<span class="number">1</span>]</span><br><span class="line">y2 = data2.iloc[:,cols-<span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line">X2 = np.matrix(X2.values)</span><br><span class="line">y2 = np.matrix(y2.values)</span><br><span class="line">theta2 = np.matrix(np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)</span><br><span class="line"><span class="built_in">print</span>(g2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalEqn</span>(<span class="params">X, y</span>):</span><br><span class="line">    theta = ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)</span><br><span class="line">    <span class="comment"># theta = np.linalg.inv(X.T@X)@X.T@y</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line">theta2=normalEqn(X2, y2)</span><br><span class="line"><span class="built_in">print</span>(theta2)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原任务是在Octave&amp;#x2F;MATLAB实现，本次编程作业全部以python完成。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&quot;01-简单的练习&quot;&gt;&lt;a href=&quot;#01-简单的练习&quot; class=&quot;headerlink&quot; title=&quot;01 简单的练习&quot;&gt;&lt;/a&gt;01 简单</summary>
      
    
    
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习第四周检测</title>
    <link href="http://example.com/2021/08/03/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/08/03/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-08-03T14:45:19.000Z</published>
    <updated>2022-04-13T11:58:37.777Z</updated>
    
    <content type="html"><![CDATA[<h1 id="答案见下方"><a href="#答案见下方" class="headerlink" title="答案见下方"></a>答案见下方</h1><hr><h2 id="1-在实现前向传播和反向传播中使用的“cache”是什么？"><a href="#1-在实现前向传播和反向传播中使用的“cache”是什么？" class="headerlink" title="1.在实现前向传播和反向传播中使用的“cache”是什么？"></a>1.在实现前向传播和反向传播中使用的“cache”是什么？</h2><p>A. 用于在训练期间缓存成本函数的中间值。<br>B. 我们用它传递前向传播中计算的变量到相应的反向传播步骤，它包含用于计算导数的反向传播的有用值。<br>C. 它用于跟踪我们正在搜索的超参数，以加速计算。<br>D. 我们使用它将向后传播计算的变量传递给相应的正向传播步骤，它包含用于计算计算激活的正向传播的有用值。</p><h2 id="2-以下哪些是“超参数”？"><a href="#2-以下哪些是“超参数”？" class="headerlink" title="2.以下哪些是“超参数”？"></a>2.以下哪些是“超参数”？</h2><p>A. 隐藏层的大小n^ [l]<br>B. 学习率α<br>C. 迭代次数<br>D. 神经网络中的层数L</p><h2 id="3-下列哪个说法是正确的？"><a href="#3-下列哪个说法是正确的？" class="headerlink" title="3.下列哪个说法是正确的？"></a>3.下列哪个说法是正确的？</h2><p>A. 神经网络的更深层通常比前面的层计算更复杂的输入特征。<br>B. 神经网络的前面的层通常比更深层计算输入的更复杂的特性。</p><h2 id="4-向量化允许您在L层神经网络中计算前向传播，而不需要在层-l-x3D-1-2，…，L-上显式的使用for-loop（或任何其他显式迭代循环），正确吗？"><a href="#4-向量化允许您在L层神经网络中计算前向传播，而不需要在层-l-x3D-1-2，…，L-上显式的使用for-loop（或任何其他显式迭代循环），正确吗？" class="headerlink" title="4.向量化允许您在L层神经网络中计算前向传播，而不需要在层(l &#x3D; 1,2，…，L)上显式的使用for-loop（或任何其他显式迭代循环），正确吗？"></a>4.向量化允许您在L层神经网络中计算前向传播，而不需要在层(l &#x3D; 1,2，…，L)上显式的使用for-loop（或任何其他显式迭代循环），正确吗？</h2><p>A. 正确<br>B.  错误</p><h2 id="5-假设我们将n-l-的值存储在名为layers的数组中，如下所示：layer-dims-x3D-n-x-4-3-2-1-。-因此，第1层有四个隐藏单元，第2层有三个隐藏单元，依此类推。-您可以使用哪个for循环初始化模型参数？"><a href="#5-假设我们将n-l-的值存储在名为layers的数组中，如下所示：layer-dims-x3D-n-x-4-3-2-1-。-因此，第1层有四个隐藏单元，第2层有三个隐藏单元，依此类推。-您可以使用哪个for循环初始化模型参数？" class="headerlink" title="5. 假设我们将n^ [l]的值存储在名为layers的数组中，如下所示：layer_dims &#x3D; [n_x,4,3,2,1]。 因此，第1层有四个隐藏单元，第2层有三个隐藏单元，依此类推。 您可以使用哪个for循环初始化模型参数？"></a>5. 假设我们将n^ [l]的值存储在名为layers的数组中，如下所示：layer_dims &#x3D; [n_x,4,3,2,1]。 因此，第1层有四个隐藏单元，第2层有三个隐藏单元，依此类推。 您可以使用哪个for循环初始化模型参数？</h2><p>答：</p><h2 id="6-下面关于神经网络的说法正确的是："><a href="#6-下面关于神经网络的说法正确的是：" class="headerlink" title="6.下面关于神经网络的说法正确的是："></a>6.下面关于神经网络的说法正确的是：</h2><p>A. 层数L为4，隐藏层数为3</p><h2 id="7-在前向传播期间，在层l的前向传播函数中，您需要知道层l中的激活函数（Sigmoid，tanh，ReLU等）是什么，-在反向传播期间，相应的反向传播函数也需要知道第l层的激活函数是什么，因为梯度是根据它来计算的，正确吗？"><a href="#7-在前向传播期间，在层l的前向传播函数中，您需要知道层l中的激活函数（Sigmoid，tanh，ReLU等）是什么，-在反向传播期间，相应的反向传播函数也需要知道第l层的激活函数是什么，因为梯度是根据它来计算的，正确吗？" class="headerlink" title="7.在前向传播期间，在层l的前向传播函数中，您需要知道层l中的激活函数（Sigmoid，tanh，ReLU等）是什么， 在反向传播期间，相应的反向传播函数也需要知道第l层的激活函数是什么，因为梯度是根据它来计算的，正确吗？"></a>7.在前向传播期间，在层l的前向传播函数中，您需要知道层l中的激活函数（Sigmoid，tanh，ReLU等）是什么， 在反向传播期间，相应的反向传播函数也需要知道第l层的激活函数是什么，因为梯度是根据它来计算的，正确吗？</h2><p>A. 正确<br>B. 错误</p><h2 id="8-有一些功能具有以下属性：-i-使用浅网络电路计算函数时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是（ii）使用深网络电路来计算它，只需要一个指数较小的网络。真-x2F-假？"><a href="#8-有一些功能具有以下属性：-i-使用浅网络电路计算函数时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是（ii）使用深网络电路来计算它，只需要一个指数较小的网络。真-x2F-假？" class="headerlink" title="8.有一些功能具有以下属性：(i) 使用浅网络电路计算函数时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是（ii）使用深网络电路来计算它，只需要一个指数较小的网络。真&#x2F;假？"></a>8.有一些功能具有以下属性：(i) 使用浅网络电路计算函数时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是（ii）使用深网络电路来计算它，只需要一个指数较小的网络。真&#x2F;假？</h2><p>A. 正确<br>B. 错误</p><h2 id="9-前面的问题使用了一个特定的网络，与层l有关的权重矩阵在一般情况下，W-l-的维数是多少"><a href="#9-前面的问题使用了一个特定的网络，与层l有关的权重矩阵在一般情况下，W-l-的维数是多少" class="headerlink" title="9.前面的问题使用了一个特定的网络，与层l有关的权重矩阵在一般情况下，W^ [l]的维数是多少?"></a>9.前面的问题使用了一个特定的网络，与层l有关的权重矩阵在一般情况下，W^ [l]的维数是多少?</h2><p>答：</p><hr><h2 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h2><ol><li><p>B</p></li><li><p>ABCD</p></li><li><p>A</p></li><li><p>B</p></li><li><pre><code class="PYTHON">for(i in range(1, len(layer_dims))):    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i - 1])) * 0.01    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01</code></pre></li><li><p>A</p></li><li><p>A(在反向传播期间，您需要知道正向传播中使用哪种激活函数才能计算正确的导数。)</p></li><li><p>A</p></li><li><p>w^ [l]的维度是（n^ [l]，n^ [l-1]）</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;答案见下方&quot;&gt;&lt;a href=&quot;#答案见下方&quot; class=&quot;headerlink&quot; title=&quot;答案见下方&quot;&gt;&lt;/a&gt;答案见下方&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;1-在实现前向传播和反向传播中使用的“cache”是什么？&quot;&gt;&lt;a href=&quot;#1-在实现前向</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day05 深层神经网络</title>
    <link href="http://example.com/2021/08/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day05%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2021/08/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day05%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-08-03T07:24:30.000Z</published>
    <updated>2022-04-13T12:05:00.096Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-深度神经网络"><a href="#01-深度神经网络" class="headerlink" title="01 深度神经网络"></a>01 深度神经网络</h1><ul><li>L代表的深度神经网络的层数，n^ [l]代表的是l层有多少个节点，a^ [l]代表l层的激活函数，w^ [l]表示在a^ [l]中计算z^ [l]值的权重。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Jq4QKjGPAIxbX61.png"                      alt="image-20210803094546100"                ></p><h1 id="02-深层网络中的前向传播"><a href="#02-深层网络中的前向传播" class="headerlink" title="02 深层网络中的前向传播"></a>02 深层网络中的前向传播</h1><ul><li>在深层网络中应用前向传播，图中左侧是针对单个样本的，左侧最后一行a的上标应该是3；右上角就是前向传播的总结公式，右下角是用向量化的方法训练整个训练集。在我们实现前向传播时用for循环，没有比它更好的方法，可以计算1到L层的激活函数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/HdDz5on8YL2Uf1u.png"                      alt="image-20210803095954094"                ></p><h1 id="03-核对矩阵的维度"><a href="#03-核对矩阵的维度" class="headerlink" title="03 核对矩阵的维度"></a>03 核对矩阵的维度</h1><ul><li>图的右上角的公式可以帮助检查w和b的维度，反向传播时dw和w具有相同的维度，db和b的维度一样。因为a^ [l] &#x3D; g^ [l] (z^ [l])，所以z和a的维度应该相等。（只针对于没有向量化的）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/MY7rJ8ESXzC3ZNc.png"                      alt="image-20210803102640239"                ></p><ul><li>向量化后的dw和w，db和b的维度同向量化前的一样，但是Z、A、X的维度会发生变化，右侧是发生的变化。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/jtRWuc8MSoBbZds.png"                      alt="image-20210803102824301"                ></p><h1 id="04-为什么使用深层表示"><a href="#04-为什么使用深层表示" class="headerlink" title="04 为什么使用深层表示"></a>04 为什么使用深层表示</h1><ul><li>例如人脸识别，将第一层当成一个特征探测器（边缘探测器），然后将探测到的边缘组合成面部的不同部分（第二层），然后将这些部分组合到一起就可以识别或者探测不同的人脸了。边缘探测器相对来说都是针对照片中非常小块的面积，面部探测器就会针对于大一些的区域，就是从简单到复杂。这种从简单到复杂的金字塔状网络还可以应用于语音识别等领域。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/6zIwjYLVcquCTSm.png"                      alt="image-20210803105142741"                ></p><ul><li>电路理论：在非正式情况下，能够用电路元件计算的函数可以用相对较小但很深的神经网络来计算（小指的是隐藏单元的数量相对比较少），但是如果用浅一些的神经网络计算同样的函数，会需要呈指数增长的单元数量才能到到同样的计算结果。如下图所示右侧就是单层神经网络，而左侧就是相对较小但很深的神经网络。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/9E82gquXkafKQSO.png"                      alt="image-20210803140312617"                ></p><h1 id="05-搭建深层网络块"><a href="#05-搭建深层网络块" class="headerlink" title="05 搭建深层网络块"></a>05 搭建深层网络块</h1><ul><li>用作前向传播的正向函数，输入是a^ [l-1]，输出是a^ [l]以及输出到缓存的z^ [l]，用作反向传播的反向函数，输入da^ [l]及之前缓存的z^ [l]，输出da^ [l-1]和dw^ [l]和db^ [l]。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/bHQ4wY7FniAJTPU.png"                      alt="image-20210803142223476"                ></p><ul><li>下面是前向传播和后向传播的流程图，在编程过程中缓存z、w、b可以将这些参数复制到计算反向传播所需要的地方。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/YpMXJlHDrQvVKPk.png"                      alt="image-20210803142305035"                ></p><h1 id="06-前向和反向传播"><a href="#06-前向和反向传播" class="headerlink" title="06 前向和反向传播"></a>06 前向和反向传播</h1><ul><li>前向传播的步骤：非向量化 | 向量化</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/L4zNkGhQ7YVXpST.png"                      alt="image-20210803143933332"                ></p><ul><li>反向传播的步骤：非向量化 | 向量化，右侧第二个不是l-1是l</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/XhUf58mwCvkG2WR.png"                      alt="image-20210803144059333"                ></p><ul><li>总结：初始一个向量化反向传播的方法（右下角那个）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/BspLI6kxEh8wCDV.png"                      alt="image-20210803144232250"                ></p><h1 id="07-参数-VS-超参数"><a href="#07-参数-VS-超参数" class="headerlink" title="07 参数 VS 超参数"></a>07 参数 VS 超参数</h1><ul><li>超参数是实际上控制了最后的参数w和b的值，比如说：学习率a、梯度下降法循环的数量、隐层数L、隐藏单元数n^ [l]、以及各种激活函数。这些超参数某种程度上决定了最终得到的w和b。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/sviWglbrD6xZnoj.png"                      alt="image-20210803150054727"                ></p><ul><li>不断地尝试超参数的设置，这样才能找到个好的值来完成学习。随着时间的推移，一些电脑硬件或者其他的改变，之前设置好的超参数的值到现在不一定适用，因此一定要勤快点，多试试。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/5GhR8YJxlIgr4uc.png"                      alt="image-20210803150116289"                ></p><h1 id="08-这和大脑有什么关系"><a href="#08-这和大脑有什么关系" class="headerlink" title="08 这和大脑有什么关系"></a>08 这和大脑有什么关系</h1><ul><li>没啥关系</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/8LtQhk5K74WUAVT.png"                      alt="image-20210803150801959"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-深度神经网络&quot;&gt;&lt;a href=&quot;#01-深度神经网络&quot; class=&quot;headerlink&quot; title=&quot;01 深度神经网络&quot;&gt;&lt;/a&gt;01 深度神经网络&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;L代表的深度神经网络的层数，n^ [l]代表的是l层有多少个节点，a^ </summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习第三周检测</title>
    <link href="http://example.com/2021/08/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/08/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-08-02T14:22:29.000Z</published>
    <updated>2022-04-13T11:57:15.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="答案见下方"><a href="#答案见下方" class="headerlink" title="答案见下方"></a>答案见下方</h1><hr><h2 id="1-以下哪一项是正确的？"><a href="#1-以下哪一项是正确的？" class="headerlink" title="1.以下哪一项是正确的？"></a>1.以下哪一项是正确的？</h2><p>A. X是一个矩阵，其中每个列都是一个训练示例。<br>B. a^[2]_4 是第二层第四层神经元的激活的输出。<br>C. a^[2] (12)表示第二层的第12个样本的激活向量。<br>D. a^[2] 表示第二层的激活向量。</p><h2 id="2-tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？"><a href="#2-tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？" class="headerlink" title="2.tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？"></a>2.tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？</h2><p>A. True<br>B. False</p><h2 id="3-其中哪一个是第l层向前传播的正确向量化实现，其中1≤-l-≤L"><a href="#3-其中哪一个是第l层向前传播的正确向量化实现，其中1≤-l-≤L" class="headerlink" title="3.其中哪一个是第l层向前传播的正确向量化实现，其中1≤ l ≤L"></a>3.其中哪一个是第l层向前传播的正确向量化实现，其中1≤ l ≤L</h2><p>A. Z^ [l]&#x3D;W^ [l]A^ [l−1]+b[l]<br>B. A^ [l]&#x3D;g^ [l] (Z^ [l])</p><h2 id="4-您正在构建一个识别黄瓜（y-x3D-1）与西瓜（y-x3D-0）的二元分类器。-你会推荐哪一种激活函数用于输出层？"><a href="#4-您正在构建一个识别黄瓜（y-x3D-1）与西瓜（y-x3D-0）的二元分类器。-你会推荐哪一种激活函数用于输出层？" class="headerlink" title="4.您正在构建一个识别黄瓜（y &#x3D; 1）与西瓜（y &#x3D; 0）的二元分类器。 你会推荐哪一种激活函数用于输出层？"></a>4.您正在构建一个识别黄瓜（y &#x3D; 1）与西瓜（y &#x3D; 0）的二元分类器。 你会推荐哪一种激活函数用于输出层？</h2><p>A. ReLU<br>B. Leaky ReLU<br>C. sigmoid<br>D. tanh</p><h2 id="5-看一下下面的代码：请问B-shape的值是多少"><a href="#5-看一下下面的代码：请问B-shape的值是多少" class="headerlink" title="5.看一下下面的代码：请问B.shape的值是多少?"></a>5.看一下下面的代码：请问B.shape的值是多少?</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = np.random.randn(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">B = np.<span class="built_in">sum</span>(A, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="6-假设你已经建立了一个神经网络。-您决定将权重和偏差初始化为零。-以下哪项陈述是正确的？"><a href="#6-假设你已经建立了一个神经网络。-您决定将权重和偏差初始化为零。-以下哪项陈述是正确的？" class="headerlink" title="6.假设你已经建立了一个神经网络。 您决定将权重和偏差初始化为零。 以下哪项陈述是正确的？"></a>6.假设你已经建立了一个神经网络。 您决定将权重和偏差初始化为零。 以下哪项陈述是正确的？</h2><p>A. 第一个隐藏层中的每个神经元节点将执行相同的计算。 所以即使经过多次梯度下降迭代后，层中的每个神经元节点都会计算出与其他神经元节点相同的东西。<br>B. 第一个隐藏层中的每个神经元将在第一次迭代中执行相同的计算。 但经过一次梯度下降迭代后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。<br>C. 第一个隐藏层中的每一个神经元都会计算出相同的东西，但是不同层的神经元会计算不同的东西，因此我们已经完成了“对称破坏”。<br>D. 即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算， 他们的参数将以自己的方式不断发展。</p><h2 id="7-Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？"><a href="#7-Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？" class="headerlink" title="7.Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？"></a>7.Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？</h2><p>A. True<br>B. False</p><h2 id="8-您已经为所有隐藏单元使用tanh激活建立了一个网络。-使用np-random-randn（-，-）-1000将权重初始化为相对较大的值。-会发生什么？"><a href="#8-您已经为所有隐藏单元使用tanh激活建立了一个网络。-使用np-random-randn（-，-）-1000将权重初始化为相对较大的值。-会发生什么？" class="headerlink" title="8.您已经为所有隐藏单元使用tanh激活建立了一个网络。 使用np.random.randn（..，..）* 1000将权重初始化为相对较大的值。 会发生什么？"></a>8.您已经为所有隐藏单元使用tanh激活建立了一个网络。 使用np.random.randn（..，..）* 1000将权重初始化为相对较大的值。 会发生什么？</h2><p>A. 这没关系。只要随机初始化权重，梯度下降不受权重大小的影响。<br>B. 这将导致tanh的输入也非常大，因此导致梯度也变大。因此，您必须将α设置得非常小以防止发散; 这会减慢学习速度。<br>C. 这会导致tanh的输入也非常大，导致单位被“高度激活”，从而加快了学习速度，而权重必须从小数值开始。<br>D. 这将导致tanh的输入也很大，因此导致梯度接近于零， 优化算法将因此变得缓慢。</p><h2 id="9-看一下下面的单隐层神经网络："><a href="#9-看一下下面的单隐层神经网络：" class="headerlink" title="9.看一下下面的单隐层神经网络："></a>9.看一下下面的单隐层神经网络：</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20210802211946697.png"                      alt="image-20210802211946697"                ></p><p>A. b^[1] 的维度是(4, 1)<br>B. W^[1] 的维度是 (4, 2)<br>C. W^[2] 的维度是 (1, 4)<br>D. b^[2] 的维度是 (1, 1)</p><h2 id="10-I在和上一个相同的网络中，Z-1-和-A-1-的维度是多少？"><a href="#10-I在和上一个相同的网络中，Z-1-和-A-1-的维度是多少？" class="headerlink" title="10.I在和上一个相同的网络中，Z^ [1] 和 A^ [1]的维度是多少？"></a>10.I在和上一个相同的网络中，Z^ [1] 和 A^ [1]的维度是多少？</h2><p>答：</p><hr><h2 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h2><ol><li><p>ABCD</p></li><li><p>A</p></li><li><p>AD</p></li><li><p>C</p></li><li><pre><code class="PYTHON">B.shape = (4, 1) #keepdims = True）来确保A.shape是（4,1）</code></pre></li><li><p>A</p></li><li><p>A</p></li><li><p>D</p></li><li><p>ACD（B应该是（4，3））</p></li><li><p>Z^ [1] 和 A^ [1]的维度都是 (4,m)(注意z和Z的区别：z表示单个样本，Z表示所有样本)</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;答案见下方&quot;&gt;&lt;a href=&quot;#答案见下方&quot; class=&quot;headerlink&quot; title=&quot;答案见下方&quot;&gt;&lt;/a&gt;答案见下方&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;1-以下哪一项是正确的？&quot;&gt;&lt;a href=&quot;#1-以下哪一项是正确的？&quot; class=&quot;hea</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day04 浅层神经网络</title>
    <link href="http://example.com/2021/08/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day04%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2021/08/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day04%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-08-02T13:00:55.000Z</published>
    <updated>2022-04-13T12:05:06.491Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-神经网络概览"><a href="#01-神经网络概览" class="headerlink" title="01 神经网络概览"></a>01 神经网络概览</h1><ul><li>在这里用[ l ]来表示神经网络的第l层，用来跟( i )表示的第几个训练样本做区分。神经网络需要反复的计算z和a。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/hQZnDcgTCRfNvKd.png"                      alt="image-20210802094344557"                ></p><h1 id="02-神经网络表示"><a href="#02-神经网络表示" class="headerlink" title="02 神经网络表示"></a>02 神经网络表示</h1><ul><li>只有一个隐藏层的神经网络：分为输入层、隐藏层、输出层。其中输入层和输出层的值都是在训练集中能看到的，隐藏层的值不能看到。在计算神经网络层数时是不算输入层的，同时我们使用a^[ l ]表示符号，a也代表激活的意思，它意味着网络中不同层的值会传递给后面的层，即每一层都会产生激活值，我们将这些激活值用a^[ l ]_i表示（l表示第几层，下标i表示层中的第几个节点）。在本例中w参数是（4，3）维的，其中4代表四个隐藏单元，3代表有三个输入特征</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/XnvdfhPCUKxHTzA.png"                      alt="image-20210802100711582"                ></p><h1 id="03-计算神经网络的输出"><a href="#03-计算神经网络的输出" class="headerlink" title="03 计算神经网络的输出"></a>03 计算神经网络的输出</h1><p>神经网络的输出究竟是如何算出来的</p><hr><ul><li>这里的圆圈代表逻辑回归计算的两个步骤，神经网络只不过是计算这些步骤很多次（隐藏层的每一个节点都计算一次）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/EZqPbNCKwu7ntQp.png"                      alt="image-20210802104402117"                ></p><ul><li>下面把这四个等式向量化：向量化时的一条经验法则就是当我们在一层中有不同的节点，那就纵向的堆叠起来（例如a^[1]就是a^[1] _1~a^[4] _ 4这些激活值的堆叠）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/SqoHhxEXn1zYvDI.png"                      alt="image-20210802104426554"                ></p><ul><li>计算出四个隐藏层中的逻辑回归单元使用的是前两个等式，计算出输出层的逻辑回归用的是后两个等式</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Zlon2AQaY7X3yu4.png"                      alt="image-20210802104449194"                ></p><h1 id="04-多个例子中的向量化"><a href="#04-多个例子中的向量化" class="headerlink" title="04 多个例子中的向量化"></a>04 多个例子中的向量化</h1><ul><li>对于新的符号a^[2] (i)：这个i表示训练样本i。下面是没有向量化的实现并且想要计算所有训练样本的预测：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/mxta4LjFek8sqnd.png"                      alt="image-20210802113830274"                ></p><ul><li>下面将for循环变成向量化实现：将这些向量横向堆叠起来。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Z8sMWJKAz76xorQ.png"                      alt="image-20210802113849678"                ></p><ul><li>总结一下就是横向堆叠对应的是不同的训练样本，竖向堆叠的是不同的输入特征（也就是一层中不同的节点）。</li></ul><h1 id="05-向量化实现的解释"><a href="#05-向量化实现的解释" class="headerlink" title="05 向量化实现的解释"></a>05 向量化实现的解释</h1><ul><li>为什么z^[1]&#x3D;w^[1]x+b^[1]?</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/dMQTzwDY35x2Z1j.png"                      alt="image-20210802145315344"                ></p><ul><li>如果将输入成列向量堆叠，那么在方程运算之后，也能得到成列对堆叠的输出。右上图是在单个训练样本中实现正向传播算法就是从1循环到m，右下图第一行代码可以对所有m个例子同时向量化，类似的右下图这四行代码都是上面四行代码正确的向量化形式。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/l2zUymPt8Kkr7Bq.png"                      alt="image-20210802145354073"                ></p><h1 id="06-激活函数"><a href="#06-激活函数" class="headerlink" title="06 激活函数"></a>06 激活函数</h1><p>搭建神经网络，你可以选择在隐藏层用哪个激活函数，在输出层用哪个激活函数。</p><hr><ul><li>一些其他的激活函数：【1】tanh函数（双曲正切函数）范围在-1到1之间。如果让函数g(z)&#x3D;tanh(z)，这几乎总比sigma函数效果好，因为现在函数输出介于-1和1之间，激活函数的平均值就更接近0。使用tanh也有类似数据中心化的效果，使得数据的平均值接近0而不是0.5，这使得下一层的学习更方便。几乎tanh函数在所有场合都适用，但是在输出层例外，因为如果输出层y是0或1，那么肯定要介于0和1之间，于此同时在二元分类就可以使用sigma函数作为输出层了。tanh函数和sigma函数都有一个缺点：当z特别大或者特别小时，函数的斜率可能就很小，这样会拖慢梯度下降算法。【2】ReLU函数（修正线性单元），ReLU的好处在于对很多z空间激活函数的斜率和0差很远。在实践中使用ReLU函数，你的神经网络的学习速度通常会比使用tanh或者sigma激活函数快很多，主要是ReLU没有这种斜率接近0时减慢学习速度的效应。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/H1WgSmjUQ2ED6ir.png"                      alt="image-20210802162429956"                ></p><ul><li>选择激活函数的经验：如果在做二元分类，输出值是0和1，那么选择sigma函数作为输出层的激活函数，然后其他所有单元都用ReLU。一般不使用sigma函数，因为tanh函数比他更适用，ReLU是默认的激活函数，不知道选谁就选它。如果实在不知道选择哪个激活函数，就在验证集或者开发集上跑跑，看看哪个效果好就选择哪个。</li><li>下面是四种激活函数（最后一个是ReLU的特殊形式叫做带泄露的ReLU）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/RnmfUCDVv4cBOt9.png"                      alt="image-20210802162459839"                ></p><h1 id="07-为什么需要非线性激活函数？"><a href="#07-为什么需要非线性激活函数？" class="headerlink" title="07 为什么需要非线性激活函数？"></a>07 为什么需要非线性激活函数？</h1><p>要让你的神经网络能够计算出有趣的函数就必须使用非线性激活函数。</p><hr><ul><li>如果使用线性激活函数或者叫恒等激活函数，那么神经网络只是把输入线性组合再输出。线性隐层一点用都没有，只有一个地方可以使用线性激活函数g(z)&#x3D;z，就是你的机器学习是回归问题的输出层。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/a4PyuzomcjgBM5Y.png"                      alt="image-20210802170616075"                ></p><h1 id="08-激活函数的导数"><a href="#08-激活函数的导数" class="headerlink" title="08 激活函数的导数"></a>08 激活函数的导数</h1><p>当对神经网络使用反向传播的时候，你需要计算激活函数的斜率或者说导数</p><hr><ul><li>sigma激活函数的导数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/2F476AJwPz5srgt.png"                      alt="image-20210802174103141"                ></p><ul><li>tanh激活函数的导数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/GKcErp4BiIDNz9H.png"                      alt="image-20210802174116844"                ></p><ul><li>ReLU和带泄露的ReLU激活函数的导数</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/pMTrBVIafq7j1LW.png"                      alt="image-20210802174132642"                ></p><h1 id="09-神经网络的梯度下降法"><a href="#09-神经网络的梯度下降法" class="headerlink" title="09 神经网络的梯度下降法"></a>09 神经网络的梯度下降法</h1><ul><li>输入层有n^[0]个，隐层有n^[1]个，输出层有n^[2]个，还有一个神经网络的成本函数，在二元分类的情况下，成本函数就是1&#x2F;m对损失函数求平均。要训练参数，算法就需要做梯度下降，在训练神经网络时随机初始化参数很重要，而不是全部初始化为0。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/bLSsoME1CfI57Dh.png"                      alt="image-20210802180705723"                ></p><ul><li>针对于所有样本的前向传播和后向传播：keepdims就是防止python直接输出秩为1的数组（(n,)），确保python输出的是矩阵（(n,1)）。*代表逐个元素乘积。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/gCEud5RJw18vAMN.png"                      alt="image-20210802180735707"                ></p><h1 id="10-直观理解反向传播"><a href="#10-直观理解反向传播" class="headerlink" title="10 直观理解反向传播"></a>10 直观理解反向传播</h1><ul><li>单层神经网络：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/YTsozX43KhIPARO.png"                      alt="image-20210802201831405"                ></p><ul><li>双层神经网络：实现后向传播算法有个技巧，你必须确保矩阵的维度互相匹配。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/3WX6l8OQRhuVHFt.png"                      alt="image-20210802201851527"                ></p><ul><li>反向传播公式小总结：单个样本 | 总样本</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/T5Cw69YgQVIqso4.png"                      alt="image-20210802201913358"                ></p><h1 id="11-随机初始化"><a href="#11-随机初始化" class="headerlink" title="11 随机初始化"></a>11 随机初始化</h1><p>对于逻辑回归可以将权重初始化为0，但是如果将神经网络的各参数数组全部初始化为0，再使用梯度下降算法将会完全无效</p><hr><ul><li>如果将w所有值初始化为0，那么因为两个隐藏单元最开始就在做同样的计算，对输出单元的影响也一样大。那么一次迭代之后，同样的对称性依然存在，两个隐藏单元依然是对称的。无论你神经网络训练多久，两个隐藏单元依然在计算完全一样的函数，所以这种情况多个隐藏单元是没有意义的。当然对多个隐藏单元也适用。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/2UPb7tczsa6ITWG.png"                      alt="image-20210802204543096"                ></p><ul><li>因此解决这个问题就要随机初始化，通常喜欢将权重初始化成很小的数，因此乘一个0.01（深层就要乘一个0.01以外的数）。因为当使用tanh和sigma激活函数时，如果权重过大就会落到斜率平缓处，导致学习缓慢。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Ha2bpDcGBsAqV9o.png"                      alt="image-20210802205246538"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-神经网络概览&quot;&gt;&lt;a href=&quot;#01-神经网络概览&quot; class=&quot;headerlink&quot; title=&quot;01 神经网络概览&quot;&gt;&lt;/a&gt;01 神经网络概览&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;在这里用[ l ]来表示神经网络的第l层，用来跟( i )表示的第几个训</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习第二周检测</title>
    <link href="http://example.com/2021/08/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/08/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-08-01T02:13:09.000Z</published>
    <updated>2022-04-13T11:55:59.813Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-神经元节点计算什么？"><a href="#1-神经元节点计算什么？" class="headerlink" title="1.神经元节点计算什么？"></a>1.神经元节点计算什么？</h2><p>A. 神经元节点先计算激活函数，再计算线性函数(z &#x3D; Wx + b)</p><p>B. 神经元节点先计算线性函数（z &#x3D; Wx + b），再计算激活。</p><p>C. 神经元节点计算函数g，函数g计算(Wx + b)。</p><p>D. 在 将输出应用于激活函数之前，神经元节点计算所有特征的平均值</p><h2 id="2-Logistic损失函数表达式的形式？"><a href="#2-Logistic损失函数表达式的形式？" class="headerlink" title="2.Logistic损失函数表达式的形式？"></a>2.Logistic损失函数表达式的形式？</h2><p>答：</p><h2 id="3-假设img是一个（32-32-3）数组，具有3个颜色通道：红色、绿色和蓝色的32x32像素的图像。-如何将其重新转换为列向量？"><a href="#3-假设img是一个（32-32-3）数组，具有3个颜色通道：红色、绿色和蓝色的32x32像素的图像。-如何将其重新转换为列向量？" class="headerlink" title="3.假设img是一个（32,32,3）数组，具有3个颜色通道：红色、绿色和蓝色的32x32像素的图像。 如何将其重新转换为列向量？"></a>3.假设img是一个（32,32,3）数组，具有3个颜色通道：红色、绿色和蓝色的32x32像素的图像。 如何将其重新转换为列向量？</h2><p>代码答：</p><h2 id="4-看一下下面的这两个随机数组“a”和“b”：请问数组c的维度是多少？"><a href="#4-看一下下面的这两个随机数组“a”和“b”：请问数组c的维度是多少？" class="headerlink" title="4.看一下下面的这两个随机数组“a”和“b”：请问数组c的维度是多少？"></a>4.看一下下面的这两个随机数组“a”和“b”：请问数组c的维度是多少？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># a.shape = (2, 3)</span></span><br><span class="line">b = np.random.randn(<span class="number">2</span>, <span class="number">1</span>) <span class="comment"># b.shape = (2, 1)</span></span><br><span class="line">c = a + b</span><br></pre></td></tr></table></figure><p>答： </p><h2 id="5-看一下下面的这两个随机数组“a”和“b”：请问数组“c”的维度是多少？"><a href="#5-看一下下面的这两个随机数组“a”和“b”：请问数组“c”的维度是多少？" class="headerlink" title="5.看一下下面的这两个随机数组“a”和“b”：请问数组“c”的维度是多少？"></a>5.看一下下面的这两个随机数组“a”和“b”：请问数组“c”的维度是多少？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># a.shape = (4, 3)</span></span><br><span class="line">b = np.random.randn(<span class="number">3</span>, <span class="number">2</span>) <span class="comment"># b.shape = (3, 2)</span></span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure><p>答： </p><h2 id="6-假设你的每一个实例有n-x个输入特征，想一下在X-x3D-x-1-x-2-…x-m-中，X的维度是多少？"><a href="#6-假设你的每一个实例有n-x个输入特征，想一下在X-x3D-x-1-x-2-…x-m-中，X的维度是多少？" class="headerlink" title="6.假设你的每一个实例有n_x个输入特征，想一下在X&#x3D;[x^(1), x^(2)…x^(m)]中，X的维度是多少？"></a>6.假设你的每一个实例有n_x个输入特征，想一下在X&#x3D;[x^(1), x^(2)…x^(m)]中，X的维度是多少？</h2><p>答：</p><h2 id="7-看一下下面的这两个随机数组“a”和“b”：请问c的维度是多少？"><a href="#7-看一下下面的这两个随机数组“a”和“b”：请问c的维度是多少？" class="headerlink" title="7.看一下下面的这两个随机数组“a”和“b”：请问c的维度是多少？"></a>7.看一下下面的这两个随机数组“a”和“b”：请问c的维度是多少？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">12288</span>, <span class="number">150</span>) <span class="comment"># a.shape = (12288, 150)</span></span><br><span class="line">b = np.random.randn(<span class="number">150</span>, <span class="number">45</span>) <span class="comment"># b.shape = (150, 45)</span></span><br><span class="line">c = np.dot(a, b)</span><br></pre></td></tr></table></figure><p>答： </p><h2 id="8-看一下下面的这个代码片段：请问要怎么把它们向量化？"><a href="#8-看一下下面的这个代码片段：请问要怎么把它们向量化？" class="headerlink" title="8.看一下下面的这个代码片段：请问要怎么把它们向量化？"></a>8.看一下下面的这个代码片段：请问要怎么把它们向量化？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a.shape = (3,4)</span></span><br><span class="line"><span class="comment"># b.shape = (4,1)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    c[i][j] = a[i][j] + b[j]</span><br></pre></td></tr></table></figure><p>答：</p><h2 id="9-看一下下面的代码：请问c的维度会是多少？"><a href="#9-看一下下面的代码：请问c的维度会是多少？" class="headerlink" title="9.看一下下面的代码：请问c的维度会是多少？"></a>9.看一下下面的代码：请问c的维度会是多少？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">b = np.random.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure><p>答：</p><hr><h2 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h2><ol><li>B（神经元输出的是g(Wx + b)，根据前向传播应该是先计算(Wx + b)，再将其带入到激活函数g中）</li><li>针对于单个训练集的：（误差函数）</li></ol><p>![image-20210730152607594](C:\Users\1\Desktop\深度学习\深度学习 day02神经网络基础\image-20210730152607594.png)</p><p>针对于整个训练集的：（成本函数）</p><p>![image-20210730152655792](C:\Users\1\Desktop\深度学习\深度学习 day02神经网络基础\image-20210730152655792.png)<br>3. &#96;&#96;&#96;python<br>   x &#x3D; img.reshape((32<em>32</em>3,1))#reshape的作用是重塑数组<br>   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">4. ```python</span><br><span class="line">   c.shape = (2, 3)#根据python的广播原理（B的列向量复制三次与A相加）</span><br></pre></td></tr></table></figure></p><ol start="5"><li><p>直接报错，想广播都广播不了，行和列向量都没有办法复制</p></li><li><pre><code class="python">X.shape = (n_x,m)#这个形式的X=[[x_1^(1), x_1^(2)…x_1^(m)]   [x_2^(1), x_2^(2)…x_2^(m)]   ........................   [x_n^(1), x_n^(2)…x_n^(m)]]<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">7. ```python</span><br><span class="line">   c.shape = (12288, 45)#就是简单的矩阵乘法，dot(n×m , m×v) = n×v </span><br></pre></td></tr></table></figure></code></pre></li><li><p>&#96;&#96;&#96;python<br>c&#x3D; a+b.T#这个就相当于是a的所有元素加上b转置的广播</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">9. ```python</span><br><span class="line">   c.shape = (3, 3)#这题跟第五题一样，但是这题中b的行数跟a的行数一样，就可以将b的列广播</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-神经元节点计算什么？&quot;&gt;&lt;a href=&quot;#1-神经元节点计算什么？&quot; class=&quot;headerlink&quot; title=&quot;1.神经元节点计算什么？&quot;&gt;&lt;/a&gt;1.神经元节点计算什么？&lt;/h2&gt;&lt;p&gt;A. 神经元节点先计算激活函数，再计算线性函数(z &amp;#x3</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习第一周检测</title>
    <link href="http://example.com/2021/07/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%91%A8%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/07/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%91%A8%E6%A3%80%E6%B5%8B/</id>
    <published>2021-07-31T15:27:34.000Z</published>
    <updated>2022-04-13T11:55:05.500Z</updated>
    
    <content type="html"><![CDATA[<p>第一周测验 - 深度学习简介</p><h2 id="1-和“AI是新电力”相类似的说法是什么？"><a href="#1-和“AI是新电力”相类似的说法是什么？" class="headerlink" title="1.和“AI是新电力”相类似的说法是什么？"></a>1.和“AI是新电力”相类似的说法是什么？</h2><p>A: AI为我们的家庭和办公室的个人设备供电，类似于电力。<br>B: 通过“智能电网”，AI提供新的电能。<br>C: AI在计算机上运行，并由电力驱动，但是它正在让以前的计算机不能做的事情变为可能。<br>D: 就像100年前产生电能一样，AI正在改变很多的行业。</p><h2 id="2-哪些是深度学习快速发展的原因？-两个选项"><a href="#2-哪些是深度学习快速发展的原因？-两个选项" class="headerlink" title="2.哪些是深度学习快速发展的原因？ (两个选项)"></a>2.哪些是深度学习快速发展的原因？ (两个选项)</h2><p>A:  现在我们有了更好更快的计算能力。<br>B: 神经网络是一个全新的领域。<br>C: 我们现在可以获得更多的数据。<br>D: 深度学习已经取得了重大的进展，比如在在线广告、语音识别和图像识别方面有了很多的应用。</p><h2 id="3-回想一下关于不同的机器学习思想的迭代图。下面哪些陈述是正确的？"><a href="#3-回想一下关于不同的机器学习思想的迭代图。下面哪些陈述是正确的？" class="headerlink" title="3.回想一下关于不同的机器学习思想的迭代图。下面哪些陈述是正确的？"></a>3.回想一下关于不同的机器学习思想的迭代图。下面哪些陈述是正确的？</h2><p>A: 能够让深度学习工程师快速地实现自己的想法。<br>B: 在更好更快的计算机上能够帮助一个团队减少迭代(训练)的时间。<br>C:在数据量很多的数据集上训练上的时间要快于小数据集。<br>D:使用更新的深度学习算法可以使我们能够更快地训练好模型（即使更换CPU &#x2F; GPU硬件）。</p><h2 id="4-当一个经验丰富的深度学习工程师在处理一个新的问题的时候，他们通常可以利用先前的经验来在第一次尝试中训练一个表现很好的模型，而不需要通过不同的模型迭代多次从而选择一个较好的模型，这个说法是正确的吗？"><a href="#4-当一个经验丰富的深度学习工程师在处理一个新的问题的时候，他们通常可以利用先前的经验来在第一次尝试中训练一个表现很好的模型，而不需要通过不同的模型迭代多次从而选择一个较好的模型，这个说法是正确的吗？" class="headerlink" title="4.当一个经验丰富的深度学习工程师在处理一个新的问题的时候，他们通常可以利用先前的经验来在第一次尝试中训练一个表现很好的模型，而不需要通过不同的模型迭代多次从而选择一个较好的模型，这个说法是正确的吗？"></a>4.当一个经验丰富的深度学习工程师在处理一个新的问题的时候，他们通常可以利用先前的经验来在第一次尝试中训练一个表现很好的模型，而不需要通过不同的模型迭代多次从而选择一个较好的模型，这个说法是正确的吗？</h2><p>A. 正确<br>B. 错误</p><h2 id="5-用于识别猫的图像是“结构化”数据的一个例子，因为它在计算机中被表示为结构化矩阵，是真的吗？"><a href="#5-用于识别猫的图像是“结构化”数据的一个例子，因为它在计算机中被表示为结构化矩阵，是真的吗？" class="headerlink" title="5.用于识别猫的图像是“结构化”数据的一个例子，因为它在计算机中被表示为结构化矩阵，是真的吗？"></a>5.用于识别猫的图像是“结构化”数据的一个例子，因为它在计算机中被表示为结构化矩阵，是真的吗？</h2><p>A. 正确<br>B. 错误</p><h2 id="6-统计不同城市人口、人均GDP、经济增长的人口统计数据集是“非结构化”数据的一个例子，因为它包含来自不同来源的数据，是真的吗？"><a href="#6-统计不同城市人口、人均GDP、经济增长的人口统计数据集是“非结构化”数据的一个例子，因为它包含来自不同来源的数据，是真的吗？" class="headerlink" title="6.统计不同城市人口、人均GDP、经济增长的人口统计数据集是“非结构化”数据的一个例子，因为它包含来自不同来源的数据，是真的吗？"></a>6.统计不同城市人口、人均GDP、经济增长的人口统计数据集是“非结构化”数据的一个例子，因为它包含来自不同来源的数据，是真的吗？</h2><p>A. 正确<br>B. 错误</p><h2 id="7-为什么在RNN（循环神经网络）可以应用机器翻译将英语翻译成法语？"><a href="#7-为什么在RNN（循环神经网络）可以应用机器翻译将英语翻译成法语？" class="headerlink" title="7.为什么在RNN（循环神经网络）可以应用机器翻译将英语翻译成法语？"></a>7.为什么在RNN（循环神经网络）可以应用机器翻译将英语翻译成法语？</h2><p>A. 因为它可以被用做监督学习。<br>B. 严格意义上它比卷积神经网络（CNN）效果更好。<br>C. 它比较适合用于当输入&#x2F;输出是一个序列的时候（例如：一个单词序列）<br>D. RNN代表递归过程：想法-&gt;编码-&gt;实验-&gt;想法-&gt;…</p><h2 id="8-在我们手绘的这张图中，横轴（x轴）和纵轴（y轴）代表什么"><a href="#8-在我们手绘的这张图中，横轴（x轴）和纵轴（y轴）代表什么" class="headerlink" title="8.在我们手绘的这张图中，横轴（x轴）和纵轴（y轴）代表什么?"></a>8.在我们手绘的这张图中，横轴（x轴）和纵轴（y轴）代表什么?</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Uh38VtNWTcJkY9w.png"                      alt="image-20210729152017843"                ></p><p>x轴是:                       y轴是:</p><h2 id="9-假设上一个问题图中描述的是准确的（并且希望您的轴标签正确），以下哪一项是正确的"><a href="#9-假设上一个问题图中描述的是准确的（并且希望您的轴标签正确），以下哪一项是正确的" class="headerlink" title="9.假设上一个问题图中描述的是准确的（并且希望您的轴标签正确），以下哪一项是正确的?"></a>9.假设上一个问题图中描述的是准确的（并且希望您的轴标签正确），以下哪一项是正确的?</h2><p>A. 增加训练集的大小通常不会影响算法的性能，这可能会有很大的帮助。<br>B. 增加神经网络的大小通常不会影响算法的性能，这可能会有很大的帮助。<br>C. 减小训练集的大小通常不会影响算法的性能，这可能会有很大的帮助。<br>D. 减小神经网络的大小通常不会影响算法的性能，这可能会有很大的帮助。</p><p>答案 1. D 2. AC 3. ACD 4. B(不可能每次都很准确呀) 5. B（看过都知道图片、语音、自然语言都是非结构化的数据） 6. B（注意关键词数据集，可以看出是结构化数据） 7.AC 8. 数据量 算法的性能 9.AB(A是针对一个算法来说，B是针对所有算法并且在同一训练集上而言)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第一周测验 - 深度学习简介&lt;/p&gt;
&lt;h2 id=&quot;1-和“AI是新电力”相类似的说法是什么？&quot;&gt;&lt;a href=&quot;#1-和“AI是新电力”相类似的说法是什么？&quot; class=&quot;headerlink&quot; title=&quot;1.和“AI是新电力”相类似的说法是什么？&quot;&gt;&lt;/a&gt;1</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 day02 03 神经网络基础</title>
    <link href="http://example.com/2021/07/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day02%2003%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2021/07/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20day02%2003%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</id>
    <published>2021-07-30T15:42:19.000Z</published>
    <updated>2022-04-13T12:05:13.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-二分分类"><a href="#01-二分分类" class="headerlink" title="01 二分分类"></a>01 二分分类</h1><ul><li>逻辑回归是一个用于二分分类的算法</li><li>计算机保存一张图片就需要保存三个独立矩阵，分别对应图片中的红、绿、蓝三个颜色通道，如果照片是64×64像素的就有三个64×64的矩阵。然后需要将这三个矩阵所有元素都存放在特征x向量中（x向量就是64×64×3&#x3D;12288维）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/RIh6KoGnSNlgj5f.png"                      alt="image-20210730144139789"                ></p><ul><li>输入X用矩阵表示：python中用来输出矩阵的维度代码是X.shape&#x3D;(n,m)，表示X是一个n×m的矩阵。</li></ul><h1 id="02-逻辑回归"><a href="#02-逻辑回归" class="headerlink" title="02 逻辑回归"></a>02 逻辑回归</h1><ul><li>当你实现逻辑回归时，你要做的就是学习参数w和b。因为要求输出y帽是在0到1之间，所以我们将输入x的线性函数w^T+b带入到sigma函数中，得到sigma函数。当z很大时输出接近于1，当z很小时输出接近于0。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/nzvKTeEDwRJxi51.png"                      alt="image-20210730145910350"                ></p><h1 id="03-逻辑回归损失函数"><a href="#03-逻辑回归损失函数" class="headerlink" title="03 逻辑回归损失函数"></a>03 逻辑回归损失函数</h1><ul><li>损失函数又叫误差函数可以用来衡量算法在单个训练样本运行情况，在这里我们不用第一个式子，我们用第二个损失函数，因为在逻辑回归中如果使用第一个式子，最终会得到很多个局部最优解，梯度下降法可能找不到局部最优值（非凸），而使用第二个就会给我们一个凸的优化问题。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ltQL4PrIKf57YNm.png"                      alt="image-20210730152607594"                ></p><ul><li>成本函数基于参数的总成本，衡量的是w和b在全体训练样本上的表现</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/NS2TlBz86WCjPaO.png"                      alt="image-20210730152655792"                ></p><h1 id="04-梯度下降法"><a href="#04-梯度下降法" class="headerlink" title="04 梯度下降法"></a>04 梯度下降法</h1><ul><li>我们想要找到参数w和b来使成本函数最小，我们可以随机初始化参数，梯度下降法就是从初始点开始沿着最陡的下坡方向走一步，不断地进行迭代最终达到全局最小点。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/2VBLtvmQnIoR61S.png"                      alt="image-20210730154521464"                ></p><ul><li>下面是参数更新的过程，无论参数w在哪一边，他都会向成本函数最小值方向前进，并且可以看出随着w的的变化，函数的斜率也是不断地减小，随着斜率的减小，w变化幅度也就随之减小（也就是朝着成本函数最小值方向移动的越慢）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ifCEsUQ1VXgzqWh.png"                      alt="image-20210730154846622"                ></p><h1 id="05-计算图"><a href="#05-计算图" class="headerlink" title="05 计算图"></a>05 计算图</h1><ul><li>一个神经网络的计算都是按照前向或者反向传播过程来实现的，首先计算出神将网络的输出，紧接着进行一个反向传输操作（我们用来计算出对应的梯度或者导数）。</li><li>从左到右的过程，可以计算出J的值。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/rQLByTl5AhcdOZx.png"                      alt="image-20210730162812955"                ></p><h1 id="06-计算图的导数计算"><a href="#06-计算图的导数计算" class="headerlink" title="06 计算图的导数计算"></a>06 计算图的导数计算</h1><ul><li>当计算所有这些导数时，最有效率的办法就是从右到左计算，跟着红色的箭头走，我们第一次计算对v的导数在之后计算对a的导数就可以用到，同样的对u的导数的计算在之后计算b的导数时就可以用到。他这个计算导数是，计算哪个导数就对那个值进行一些增加，看看他对J是如何影响的，使用微积分的链式法则就可以算出其导数。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/ZeNuDdfh7mIPtyY.png"                      alt="image-20210730164026926"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Efo5z3a9lCIvhWm.png"                      alt="image-20210730164043308"                ></p><h1 id="07-逻辑回归中的梯度下降法"><a href="#07-逻辑回归中的梯度下降法" class="headerlink" title="07 逻辑回归中的梯度下降法"></a>07 逻辑回归中的梯度下降法</h1><ul><li>该样本的偏导数流程图：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/yqgiBnlvAMaKhLY.png"                      alt="image-20210730170954124"                ></p><ul><li>单个样本实例的一次梯度更新步骤：想要计算损失函数L的导数，首先我们要向前一步先计算损失函数关于变量a的导数（da），再向后一步计算出损失函数关于z的导数（dz），最后就是计算出dw、db了，就可以进行更新参数更新了。（更新w&#x2F;b为b减去学习率乘以dw&#x2F;db）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/9w76TWi8CgOPEeI.png"                      alt="image-20210730171009830"                ></p><h1 id="08-m个样本的梯度下降"><a href="#08-m个样本的梯度下降" class="headerlink" title="08 m个样本的梯度下降"></a>08 m个样本的梯度下降</h1><ul><li>全局成本函数是从1到m项损失函数和的平均&#x3D;&#x3D;&gt;根据这个我们可以推导出全局成本函数对w1的导数也同样是各项损失函数对w1导数的平均。所以真正需要做的就是计算这些导数并且求平均，这样会得到全局梯度值，能够直接将其应用到梯度下降算法中。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/fD7kBzyEwsmcOjA.png"                      alt="image-20210730224937045"                ></p><ul><li>首先让我们初始化，接着我们要使用for循环来遍历训练集，并计算相应的每个训练样本的导数，然后将他们加起来。（这里有两个循环：第一个是遍历训练集，第二个是遍历所有特征）</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/xDzw4cGJajqrVZy.png"                      alt="image-20210730224959369"                ></p><h1 id="09-向量化"><a href="#09-向量化" class="headerlink" title="09 向量化"></a>09 向量化</h1><ul><li>向量化技术可以使代码摆脱这些显式的for循环，会帮助处理更大的数据集。</li><li>可扩展深度学习实现是在GPU（图像处理单元）上做的，但是课程是在Jupyter Notebook做的，仅用CPU。CPU和GPU都有并行化的指令有时候也叫做SIMD指令（单指令流多数据流，这个意思是如果你使用了能去掉显式for循环的函数，这样python的numpy能充分利用并行化去更快的计算）</li><li>下图是一个非向量化与向量化实现的对比：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/gEScQBuemlY2zi1.png"                      alt="image-20210730233618090"                ></p><ul><li>下面是通过python来实际进行操作，对比非向量化与向量化实现：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Fmiodq4P7NfG1K8.png"                      alt="image-20210730233745017"                ></p><h1 id="10-向量化更多的例子"><a href="#10-向量化更多的例子" class="headerlink" title="10 向量化更多的例子"></a>10 向量化更多的例子</h1><ul><li>依旧是非向量化与向量化前后对比：往往python中一个内置函数就搞定</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/gIXEMZzY2pwUAh6.png"                      alt="image-20210731091632337"                ></p><ul><li>式子进行去掉一个for循环的写法：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/aZowHypSLhOVieT.png"                      alt="image-20210731091559616"                ></p><h1 id="11-向量化逻辑回归"><a href="#11-向量化逻辑回归" class="headerlink" title="11 向量化逻辑回归"></a>11 向量化逻辑回归</h1><p>向量化是如何实现在逻辑回归的上面的。这样可以同时处理整个训练集来实现梯度下降法的一步迭代，不需要任何显示的for循环。</p><hr><ul><li>不需要显式的for循环就可以从m个训练样本中一次性计算出z和a，这就是正向传播一步迭代的向量化实现（同时处理所有M个训练样本）。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/QesbqnpjiwHIUFl.png"                      alt="image-20210731094933836"                ></p><h1 id="12-向量化逻辑回归的梯度输出"><a href="#12-向量化逻辑回归的梯度输出" class="headerlink" title="12 向量化逻辑回归的梯度输出"></a>12 向量化逻辑回归的梯度输出</h1><p>用向量化同时计算m个训练数据的梯度</p><hr><ul><li>不使用for循环来计算参数的更新</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/atyND7XQeH2mxZh.png"                      alt="image-20210731100950695"                ></p><ul><li>用高度向量化实现一个逻辑回归</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/CAimwjNUaqG2B4P.png"                      alt="image-20210731101025592"                ></p><h1 id="13-python中的广播"><a href="#13-python中的广播" class="headerlink" title="13 python中的广播"></a>13 python中的广播</h1><p>广播技术是一种能使python和Numpy部分代码更高效的技术</p><hr><ul><li>用两行代码求出每个元素所占列的百分比：第一行代码求出列的总和，第二行代码求出百分比。其实reshape有些多余，因为已经知道了cal是一行四列的向量了，但是为了确保正确还是用了。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/oQbKdYTpvCmyHxS.png"                      alt="image-20210731105929036"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/PQTfSqO4nBaM6oE.png"                      alt="image-20210731104518977"                ></p><ul><li>在实现神经网络算法时主要用到的广播形式</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/Rfj3trNKSlIYLbw.png"                      alt="image-20210731110057481"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/BnGO7Tl5p6uWgLm.png"                      alt="image-20210731110128433"                ></p><h1 id="14-关于python-x2F-numpy向量的说明"><a href="#14-关于python-x2F-numpy向量的说明" class="headerlink" title="14 关于python&#x2F;numpy向量的说明"></a>14 关于python&#x2F;numpy向量的说明</h1><ul><li>当你实现神经网络的逻辑回归时就不要用这些秩为1的数组</li><li>每次创建数组时，要将其定义为列向量或者行向量</li><li>如果不确定一个向量的具体维度是多少，就用assert()进行声明，确保这是一个向量</li><li>如果由于某种原因得到了一个秩为1的数组就用reshape转换成一个列向量和行向量行为的数组</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/sYTWvPUd6AagEN9.png"                      alt="image-20210731113700939"                ></p><h1 id="15-逻辑回归损失函数的解释"><a href="#15-逻辑回归损失函数的解释" class="headerlink" title="15 逻辑回归损失函数的解释"></a>15 逻辑回归损失函数的解释</h1><ul><li>损失函数的表达式：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/PNbSc7GlKhn2ptV.png"                      alt="image-20210731120639413"                ></p><ul><li>总体成本函数表达式：</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/04/13/MJcOvAqUtk5LZTX.png"                      alt="image-20210731121019860"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-二分分类&quot;&gt;&lt;a href=&quot;#01-二分分类&quot; class=&quot;headerlink&quot; title=&quot;01 二分分类&quot;&gt;&lt;/a&gt;01 二分分类&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;逻辑回归是一个用于二分分类的算法&lt;/li&gt;
&lt;li&gt;计算机保存一张图片就需要保存三个独立</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>java day15 异常</title>
    <link href="http://example.com/2021/07/30/java%20day15%E5%BC%82%E5%B8%B8/"/>
    <id>http://example.com/2021/07/30/java%20day15%E5%BC%82%E5%B8%B8/</id>
    <published>2021-07-30T02:52:56.000Z</published>
    <updated>2022-04-13T07:09:39.941Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-Error和Exception"><a href="#01-Error和Exception" class="headerlink" title="01 Error和Exception"></a>01 Error和Exception</h1><ul><li>异常是指程序运行时出现的不期而至的各种状况，如：文件找不到、网络连接失败、非法参数。</li><li>三种类型的异常：<ol><li>检查性异常：是用户错误或问题引起的异常，这时程序员无法遇见的。比如打开一个不存在的文件。</li><li>运行时异常：是可能被程序员避免的异常。</li><li>错误：错误不是异常，而是脱离程序员控制的问题。错误在代码中通常被忽略。例如堆栈溢出，在编译时也检测不到。</li></ol></li><li>Java把异常当作对象来处理，并定义一个基类java.lang.Throwable作为所有异常的超类。</li><li>这些异常通常分为两大类：错误Error和异常Exception</li><li>在Exception分支中有一个重要的子类RuntimeException（运行时异常）：<ol><li>ArrayIndexOutOfBoundsException（数组下标越界）</li><li>NullPointerException（空指针异常）</li><li>ArithmeticException（算数异常）</li><li>MissingResourceException（丢失资源）</li><li>ClassNotFoundException（找不到类）</li></ol></li></ul><h1 id="02-捕获和抛出异常"><a href="#02-捕获和抛出异常" class="headerlink" title="02 捕获和抛出异常"></a>02 捕获和抛出异常</h1><ul><li>异常处理五个关键字：try、catch、finally、throw、throws</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//方法一</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">b</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//假设要捕获多个异常：从小到大</span></span><br><span class="line">        <span class="keyword">try</span>&#123;<span class="comment">//try监控区域</span></span><br><span class="line">            System.out.println(a/b);</span><br><span class="line">        &#125;<span class="keyword">catch</span>(Error e)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Error&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Exception&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Throwable t)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Throwable&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;<span class="comment">//处理善后工作</span></span><br><span class="line">            System.out.println(<span class="string">&quot;finally&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="comment">//打印出Exception和finally</span></span><br><span class="line"><span class="comment">//方法二</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">b</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//通过Ctrl+Alt+t快捷键来完成</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(a/b);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();<span class="comment">//打印错误的栈信息</span></span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="comment">//打印出ava.lang.ArithmeticException: / by zero</span></span><br><span class="line"><span class="comment">//方法三</span></span><br><span class="line"><span class="keyword">public</span>  <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Application</span>().test(<span class="number">1</span>,<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(b==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ArithmeticException</span>();<span class="comment">//主动抛出异常</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="comment">//打印出Exception in thread &quot;main&quot; java.lang.ArithmeticException</span></span><br><span class="line"><span class="comment">//方法四</span></span><br><span class="line"><span class="keyword">public</span>  <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Application</span>().test(<span class="number">1</span>,<span class="number">0</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ArithmeticException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//假设方法处理不了这个异常。那么就将其向上抛出，在方法上抛出。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span><span class="keyword">throws</span> ArithmeticException&#123;</span><br><span class="line">        <span class="keyword">if</span>(b==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ArithmeticException</span>();<span class="comment">//主动抛出异常</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="comment">//输出java.lang.ArithmeticException</span></span><br></pre></td></tr></table></figure><h1 id="03-自定义异常"><a href="#03-自定义异常" class="headerlink" title="03 自定义异常"></a>03 自定义异常</h1><ul><li>自定义异常类，大体可以分为以下几个步骤：<ol><li>创建自定义异常类</li><li>在方法中通过throw关键字抛出异常对象</li><li>如果在当前抛出异常的方法中处理异常，可以使用try-catch语句捕获并处理；否则在方法的声明处通过throws关键字指明要抛出给方法调用者的异常，继续进行下一步操作</li><li>在出现异常方法的调用者中捕获并处理异常</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyException</span> <span class="keyword">extends</span> <span class="title class_">Exception</span>&#123;</span><br><span class="line">    <span class="comment">//传递数字&gt;10</span></span><br><span class="line">    <span class="keyword">private</span>  <span class="type">int</span> detail;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">MyException</span><span class="params">(<span class="type">int</span> a)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.detail = a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//toString:异常的打印信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;MyException&#123;&quot;</span> + <span class="string">&quot;detail=&quot;</span> + detail + <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span>  <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Application</span>().test(<span class="number">11</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (MyException e) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;MyException=&gt;&quot;</span>+e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//假设方法处理不了这个异常。那么就将其向上抛出，在方法上抛出。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">(<span class="type">int</span> a)</span><span class="keyword">throws</span> MyException&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;传递的参数为：&quot;</span>+a);</span><br><span class="line">        <span class="keyword">if</span>(a&gt;<span class="number">10</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">MyException</span>(a);<span class="comment">//主动抛出异常</span></span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;OK&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在实际应用中的经验：<ol><li>处理运行时异常时，采用逻辑去合理规避同时辅助try-catch处理</li><li>在多重catch块后面，可以加一个catch(Exception)来处理可能会被遗漏的异常</li><li>对于不确定的代码，也可以加上try-catch，处理潜在的异常（当在IDEA中出现红色波浪线可以Alt+Enter）</li><li>尽量去处理异常，不要只是简单的调用printStackTrace()去打印输出</li><li>尽量添加finally语句块去释放占用资源</li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-Error和Exception&quot;&gt;&lt;a href=&quot;#01-Error和Exception&quot; class=&quot;headerlink&quot; title=&quot;01 Error和Exception&quot;&gt;&lt;/a&gt;01 Error和Exception&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    
    <category term="java基础" scheme="http://example.com/tags/java%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
</feed>
