<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-08-25T01:53:40.087Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s之CRI</title>
    <link href="http://example.com/2022/08/25/k8s%E4%B9%8BCRI/"/>
    <id>http://example.com/2022/08/25/k8s%E4%B9%8BCRI/</id>
    <published>2022-08-25T01:49:19.000Z</published>
    <updated>2022-08-25T01:53:40.087Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>Kubernetes</strong> 节点的底层由一个叫做<strong>容器运行时</strong>的软件进行支撑，它主要负责启停容器。</p><p><strong>Docker</strong> 是目前最广为人知的容器运行时软件，但是它并非唯一。在这几年中，容器运行时这个领域发展的迅速。为了使得 <strong>Kubernetes</strong> 的扩展变得更加容易，一直在打磨支持容器运行时的 <strong>K8S</strong>插件 <strong>API</strong>，也就是 容器运行时接口 ( Container Runtime Interface, CRI) 。</p><h2 id="k8s架构"><a href="#k8s架构" class="headerlink" title="k8s架构"></a>k8s架构</h2><p>这里通过分析 <strong>k8s</strong> 目前默认的一种容器运行时架构，来帮助我们更好的理解 <strong>k8s</strong> 运行时的背后逻辑，同时引出 <strong>CRI</strong> 和 <strong>OCI</strong> 提出的背景。</p><p>我们在创建 <strong>k8s</strong> 集群的时候，首先需要搭建 <strong>master</strong> 节点，其次需要创建 <strong>node</strong> 节点，并将 <strong>node</strong> 节点加入到 <strong>k8s</strong> 集群中。当我们构建好 <strong>k8s</strong> 集群后，可以通过下面命令来创建应用对应的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f nginx.yml</span><br></pre></td></tr></table></figure><p>执行完成后，该命令首先会提交给 <strong>API Server</strong> ，然后解析 <strong>yml</strong> 文件，并对其以 <strong>API</strong> 对象的形式存到 <strong>etcd</strong> 里。</p><p>这时候，<strong>master</strong> 组件中的 <strong>Controller Manager</strong> 会通过控制循环的方式来做编排工作，创建应用所需的Pod。同时 <strong>Scheduler</strong> 会 <strong>watch etcd</strong> 中新 <strong>pod</strong> 的变化，如果他发现有一个新的 <strong>pod</strong> 的变化。</p><p>如果 <strong>Scheduler</strong> 发现有一个新的 <strong>pod</strong> 出现，它会运行调度算法，然后选择出最佳的 <strong>Node</strong> 节点，并将这个节点的名字写到 <strong>pod</strong> 对象的 <strong>NodeName</strong> 字段上，这一步就是所谓的 <strong>Bind Pod to Node</strong>，然后把 <strong>bind</strong> 的结果写到 <strong>etcd</strong>。</p><p>其次，当我们在构建 <strong>k8s</strong> 集群的时候，默认每个节点都会初始化创建一个 <strong>kubectl</strong> 进程，<strong>kubectl</strong> 进程会  <strong>watch etcd</strong> 中 <strong>pod</strong> 的变化，当 <strong>kubectl</strong> 进程监听到 <strong>pod</strong> 的 <strong>bind</strong> 的更新操作，并且 <strong>bind</strong> 的节点是本节点时，它会接管接下来的所有事情，如镜像下载，创建容器等。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/595328-20220924154453278-2062939099.jpg"                      alt="595328-20220924154453278-2062939099"                ></p><h2 id="k8s默认容器运行时架构"><a href="#k8s默认容器运行时架构" class="headerlink" title="k8s默认容器运行时架构"></a>k8s默认容器运行时架构</h2><p>接下来将通过 <strong>k8s</strong> 默认集成的容器运行时架构，来看 <strong>kubernetes</strong> 如何创建一个容器 （如下图）</p><ul><li><strong>kubernetes</strong> 通过 <strong>CRI</strong> (Container Runtime Interface) 接口调用 <strong>dockershim</strong>，请求创建一个容器。这一步中，<strong>Kubectl</strong> 可以视作一个简单的 <strong>CRI Client</strong>，而 <strong>dockershim</strong> 就是接收的 <strong>Server</strong>。</li><li><strong>dockershim</strong> 收到请求后，通过适配的方式，适配成 <strong>Docker Daemon</strong> 的请求格式，发到 <strong>Docker Daemon</strong> 上请求创建一个容器。在 docker 1.12 后的版本，docker daemon 被拆分成了 <strong>dockerd</strong> 和 <strong>containerd</strong>，其中，<strong>containerd</strong> 负责操作容器。</li><li><strong>dockerd</strong> 收到请求后，会调用 <strong>containerd</strong> 进程去创建一个容器</li><li><strong>containerd</strong> 收到请求后，并不会自己直接去操作容器，而是创建一个叫做 <strong>containerd-shim</strong> 的进程，让 <strong>containerd-shim</strong> 去操作容器，创建 <strong>containerd-shim</strong> 的目的主要有以下几个<ul><li>让 <strong>containerd-shim</strong> 做诸如收集状态，维持 stdin 等 fd 打开等工作。</li><li>允许容器运行时( <strong>runC</strong> ) 启动容器后退出，不必为每个容器一直运行一个容器运行时的 <strong>runC</strong></li><li>即使在 <strong>containerd</strong> 和 <strong>dockerd</strong> 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也是可以用的</li><li>向 <strong>containerd</strong> 报告容器的退出状态</li><li>在不中断容器运行时的情况下，升级或重启 <strong>dockerd</strong></li></ul></li><li>而 <strong>containerd-shim</strong> 在这一步需要调用 <strong>runC</strong> 这个命令行工具，来启动容器，<strong>runC</strong> 是 <strong>OCI</strong> (Open Container Initiative， 开放标准协议) 的一个参考实现。主要用来设置 <strong>namespaces</strong> 和 <strong>cgroups</strong>，挂载 root filesystem等操作。</li><li><strong>runC</strong> 启动完容器后，本身会直接退出。<strong>containerd-shim</strong> 则会成为容器进程的父进程，负责收集容器进程的状态，上报给 <strong>containerd</strong>，并在容器中的 <strong>pid</strong> 为 <strong>1</strong> 的进程退出后接管容器中的子进程进行清理，确保不会出现僵尸进程 (关闭进程描述符)。</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/595328-20220924162334102-1814702716.png"                      alt="595328-20220924162334102-1814702716"                ></p><h2 id="容器与容器编排背景"><a href="#容器与容器编排背景" class="headerlink" title="容器与容器编排背景"></a>容器与容器编排背景</h2><p>从 <strong>k8s</strong> 的容器运行时可以看出，<strong>kubectl</strong> 启动容器的过程经过了很长的一段调用链路。这个是由于在容器及编排领域各大厂商与 <strong>docker</strong> 之间的竞争以及 <strong>docker</strong> 公司为了抢占 <strong>Pass</strong> ( Platform-as-a-service，平台服务)  领域市场，对架构做出的一系列调整。</p><p>其实 <strong>k8s</strong> 最开始的运行时架构链路调用没有这么复杂：kubelet想要创建容器直接通过 <strong>docker api</strong> 调用 <strong>Docker Daemon</strong> ， 然后Docker Daemon 调用 libcontainer 这个库来启动容器。</p><p>后面为了防止 <strong>docker</strong> 垄断以及受控 docker 运行时，各大厂商于是就联合起来，制订出开放容器标准<strong>OCI</strong> ( Open Containers Initiative ) 。大家可以基于这个标准开发自己的容器运行时。Docker公司则把 <strong>libcontainer</strong> 做了一层封装，变成 <strong>runC</strong> 捐献给 <strong>CNCF</strong> 作为 <strong>OCI</strong> 的参考实现。</p><p>接下来就是 <strong>Docker</strong> 要搞 <strong>Swarm</strong> 进军 <strong>PaaS</strong> 市场，于是做了个架构切分，把容器操作都移动到一个单独的 <strong>Daemon</strong> 进程的 <strong>containerd</strong> 中去，让 Docker Daemon专门负责上层封装编排，但是最终 <strong>Swarm</strong> 败给了 <strong>K8S</strong> ，于是Docker公司就把 <strong>Containerd</strong> 捐给了 CNCF，专注于搞 <strong>Docker</strong> 企业版了。</p><p>与此同时，容器领域 <strong>core os</strong> 公司推出了 <strong>rkt</strong> 容器运行时，希望 <strong>k8s</strong> 原生支持 <strong>rkt</strong> 作为运行时，由于 <strong>core os</strong> 与 <strong>Google</strong> 的关系，最终 <strong>rkt</strong> 运行时的支持在 <strong>2016</strong> 年也被合并进 <strong>kubelet</strong> 主干代码里，这样做反而给 <strong>k8s</strong> 中负责维护 <strong>kubelet</strong> 的小组 <strong>SIG-Node</strong> 带来了更大的负担，每一次 <strong>kubectl</strong> 的更新都要维护 <strong>docker</strong> 和 <strong>rkt</strong> 作为两部分代码。与此同时，随着虚拟化技术强隔离容器技术 <strong>runV</strong> (Kata Containers 前身, 后与 intel clear container 合并)的逐渐成熟。<strong>K8S</strong> 上游对虚拟化容器的支持很快被提上日程。为了从集成每一种运行时都要维护的一份代码中解救出来，<strong>K8S SIG-Node</strong> 工作组决定对容器的操作统一地抽象成一个接口，这样 <strong>kubelet</strong> 只需要跟这个接口打交道，而具体地容器运行时，他们只需要实现该接口，并对kubelet暴露 <strong>gRPC</strong> 服务即可。这个统一地抽象的接口就是 <strong>k8s</strong> 中俗称的 <strong>CRI</strong>。</p><h2 id="CRI接口"><a href="#CRI接口" class="headerlink" title="CRI接口"></a>CRI接口</h2><p><strong>CRI</strong> (容器运行时接口)基于 <strong>gRPC</strong> 定义了 <strong>RuntimeService</strong> 和 <strong>ImageService</strong> 等两个 <strong>gRPC</strong> 服务，分别用于容器运行时和镜像的管理。如下所示</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Runtime service defines the public APIs for remote container runtimes</span></span><br><span class="line">service RuntimeService &#123;</span><br><span class="line">    <span class="comment">// Version returns the runtime name, runtime version, and runtime API version.</span></span><br><span class="line">    rpc Version(VersionRequest) returns (VersionResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure</span></span><br><span class="line">    <span class="comment">// the sandbox is in the ready state on success.</span></span><br><span class="line">    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// StopPodSandbox stops any running process that is part of the sandbox and</span></span><br><span class="line">    <span class="comment">// reclaims network resources (e.g., IP addresses) allocated to the sandbox.</span></span><br><span class="line">    <span class="comment">// If there are any running containers in the sandbox, they must be forcibly</span></span><br><span class="line">    <span class="comment">// terminated.</span></span><br><span class="line">    <span class="comment">// This call is idempotent, and must not return an error if all relevant</span></span><br><span class="line">    <span class="comment">// resources have already been reclaimed. kubelet will call StopPodSandbox</span></span><br><span class="line">    <span class="comment">// at least once before calling RemovePodSandbox. It will also attempt to</span></span><br><span class="line">    <span class="comment">// reclaim resources eagerly, as soon as a sandbox is not needed. Hence,</span></span><br><span class="line">    <span class="comment">// multiple StopPodSandbox calls are expected.</span></span><br><span class="line">    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// RemovePodSandbox removes the sandbox. If there are any running containers</span></span><br><span class="line">    <span class="comment">// in the sandbox, they must be forcibly terminated and removed.</span></span><br><span class="line">    <span class="comment">// This call is idempotent, and must not return an error if the sandbox has</span></span><br><span class="line">    <span class="comment">// already been removed.</span></span><br><span class="line">    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not</span></span><br><span class="line">    <span class="comment">// present, returns an error.</span></span><br><span class="line">    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ListPodSandbox returns a list of PodSandboxes.</span></span><br><span class="line">    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// CreateContainer creates a new container in specified PodSandbox</span></span><br><span class="line">    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// StartContainer starts the container.</span></span><br><span class="line">    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// StopContainer stops a running container with a grace period (i.e., timeout).</span></span><br><span class="line">    <span class="comment">// This call is idempotent, and must not return an error if the container has</span></span><br><span class="line">    <span class="comment">// already been stopped.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> what must the runtime do after the grace period is reached?</span></span><br><span class="line">    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// RemoveContainer removes the container. If the container is running, the</span></span><br><span class="line">    <span class="comment">// container must be forcibly removed.</span></span><br><span class="line">    <span class="comment">// This call is idempotent, and must not return an error if the container has</span></span><br><span class="line">    <span class="comment">// already been removed.</span></span><br><span class="line">    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ListContainers lists all containers by filters.</span></span><br><span class="line">    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ContainerStatus returns status of the container. If the container is not</span></span><br><span class="line">    <span class="comment">// present, returns an error.</span></span><br><span class="line">    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// UpdateContainerResources updates ContainerConfig of the container.</span></span><br><span class="line">    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ReopenContainerLog asks runtime to reopen the stdout/stderr log file</span></span><br><span class="line">    <span class="comment">// for the container. This is often called after the log file has been</span></span><br><span class="line">    <span class="comment">// rotated. If the container is not running, container runtime can choose</span></span><br><span class="line">    <span class="comment">// to either create a new log file and return nil, or return an error.</span></span><br><span class="line">    <span class="comment">// Once it returns error, new container log file MUST NOT be created.</span></span><br><span class="line">    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// ExecSync runs a command in a container synchronously.</span></span><br><span class="line">    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// Exec prepares a streaming endpoint to execute a command in the container.</span></span><br><span class="line">    rpc Exec(ExecRequest) returns (ExecResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// Attach prepares a streaming endpoint to attach to a running container.</span></span><br><span class="line">    rpc Attach(AttachRequest) returns (AttachResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// PortForward prepares a streaming endpoint to forward ports from a PodSandbox.</span></span><br><span class="line">    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// ContainerStats returns stats of the container. If the container does not</span></span><br><span class="line">    <span class="comment">// exist, the call returns an error.</span></span><br><span class="line">    rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ListContainerStats returns stats of all running containers.</span></span><br><span class="line">    rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// UpdateRuntimeConfig updates the runtime configuration based on the given request.</span></span><br><span class="line">    rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Status returns the status of the runtime.</span></span><br><span class="line">    rpc Status(StatusRequest) returns (StatusResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// ImageService defines the public APIs for managing images.</span></span><br><span class="line">service ImageService &#123;</span><br><span class="line">    <span class="comment">// ListImages lists existing images.</span></span><br><span class="line">    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ImageStatus returns the status of the image. If the image is not</span></span><br><span class="line">    <span class="comment">// present, returns a response with ImageStatusResponse.Image set to</span></span><br><span class="line">    <span class="comment">// nil.</span></span><br><span class="line">    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// PullImage pulls an image with authentication config.</span></span><br><span class="line">    rpc PullImage(PullImageRequest) returns (PullImageResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// RemoveImage removes the image.</span></span><br><span class="line">    <span class="comment">// This call is idempotent, and must not return an error if the image has</span></span><br><span class="line">    <span class="comment">// already been removed.</span></span><br><span class="line">    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) &#123;&#125;</span><br><span class="line">    <span class="comment">// ImageFSInfo returns information of the filesystem that is used to store images.</span></span><br><span class="line">    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体容器运行时则需要实现 <strong>CRI</strong> 定义的接口（即 <strong>gRPC Server</strong>，通常称为 <strong>CRI shim</strong>）。容器运行时在启动 <strong>gRPC server</strong> 时需要监听在本地的 <strong>Unix Socket</strong> （Windows 使用 tcp 格式）。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a class="link"   href="https://www.kubernetes.org.cn/1079.html" >https://www.kubernetes.org.cn/1079.html<i class="fas fa-external-link-alt"></i></a></p><p><a class="link"   href="https://www.cnblogs.com/justinli/p/11578951.html" >https://www.cnblogs.com/justinli/p/11578951.html<i class="fas fa-external-link-alt"></i></a>    </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt; 节点的底层由一个叫做&lt;strong&gt;容器运行时&lt;/strong&gt;的软件进行支撑，它主</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之使用Rancher2.0搭建Kubernetes集群</title>
    <link href="http://example.com/2022/08/25/k8s%E4%B9%8B%E4%BD%BF%E7%94%A8Rancher2-0%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/"/>
    <id>http://example.com/2022/08/25/k8s%E4%B9%8B%E4%BD%BF%E7%94%A8Rancher2-0%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/</id>
    <published>2022-08-25T01:41:51.000Z</published>
    <updated>2022-08-25T01:47:13.677Z</updated>
    
    <content type="html"><![CDATA[<p>中文文档：<a class="link"   href="https://docs.rancher.cn/docs/rancher2" >https://docs.rancher.cn/docs/rancher2<i class="fas fa-external-link-alt"></i></a></p><h2 id="安装Rancher2-0"><a href="#安装Rancher2-0" class="headerlink" title="安装Rancher2.0"></a>安装Rancher2.0</h2><p>使用下面命令，我们快速的安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 rancher【没有的话会从后台拉取】</span></span><br><span class="line">docker run -d -p 80:80 -p 443:443 rancher/rancher:v2.0.0</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201123160929063.png"                      alt="image-20201123160929063"                ></p><p>我们可以来查看我们的日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs  eloquent_curie</span><br></pre></td></tr></table></figure><p>同时，我们可以直接访问我们新建的Rancher集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://192.168.177.150/</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123161958206.png"                      alt="image-20201123161958206"                ></p><p>第一次登录，需要我们填写密码，我们自己的密码后，点击下一步，完成后即可进入到我们的控制台</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123180845117.png"                      alt="image-20201123180845117"                ></p><h2 id="导入K8S集群"><a href="#导入K8S集群" class="headerlink" title="导入K8S集群"></a>导入K8S集群</h2><p>在我们安装好Rancher2.0后，我们就可以导入我们的K8S集群进行管理了</p><p>首先我们点击 Add Cluster ，然后选择 IMPORT 导入我们的集群</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201123194902242.png"                      alt="image-20201123194902242"                ></p><p>然后会有Add Cluster页面，下面我们通过命令来添加</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123194958021.png"                      alt="image-20201123194958021"                ></p><p>我们首先选择上面这条，在我们的master节点上执行，将我们的集群被Rancher接管</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://192.168.177.130/v3/import/6pqf9w75fmx4pt94tpbpklxd2t5qkq2fm9v6dgl6w8z6rc8727bpdk.yaml</span><br></pre></td></tr></table></figure><p>如果执行命令有问题的话，我们可以提前把脚本下载下来，然后拷贝到里面的 rancher.yaml</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f rancher.yaml</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123200210728.png"                      alt="image-20201123200210728"                ></p><p>在执行上述命令，可能会出现这个问题，我们只需要把里面的 extensions&#x2F;v1beta1 修改成  apps&#x2F;v1 即可</p><p>修改完成后，再次执行即可</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123200337426.png"                      alt="image-20201123200337426"                ></p><p>我们通过下面命令，查看我们创建的pods</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods  -n cattle-system</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201123200558313.png"                      alt="image-20201123200558313"                ></p><p>执行完上述操作后，我们到Rancher的UI界面，点击Done，即可看到我们的集群被成功导入</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123200834363.png"                      alt="image-20201123200834363"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;中文文档：&lt;a class=&quot;link&quot;   href=&quot;https://docs.rancher.cn/docs/rancher2&quot; &gt;https://docs.rancher.cn/docs/rancher2&lt;i class=&quot;fas fa-external-link-</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之配置默认存储类</title>
    <link href="http://example.com/2022/08/25/k8s%E4%B9%8B%E9%85%8D%E7%BD%AE%E9%BB%98%E8%AE%A4%E5%AD%98%E5%82%A8%E7%B1%BB/"/>
    <id>http://example.com/2022/08/25/k8s%E4%B9%8B%E9%85%8D%E7%BD%AE%E9%BB%98%E8%AE%A4%E5%AD%98%E5%82%A8%E7%B1%BB/</id>
    <published>2022-08-25T01:39:17.000Z</published>
    <updated>2022-08-25T01:41:10.383Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今天在配置Kubesphere的时候，出现了下面的错误</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123095552088.png"                      alt="image-20201123095552088"                ></p><p>经过排查，发现是这个原因</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123095637801.png"                      alt="image-20201123095637801"                ></p><p>我通过下面命令，查看Kubernetes集群中的默认存储类</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get storageclass</span><br></pre></td></tr></table></figure><p>发现空空如也，所以问题应该就出现在这里了~，下面我们给k8s集群安装上默认的存储类</p><h2 id="安装nfs"><a href="#安装nfs" class="headerlink" title="安装nfs"></a>安装nfs</h2><p>我们使用的是nfs来作为k8s的存储类</p><p>首先找一台新的服务器，作为nfs服务端，然后进行 nfs的安装 【服务器：192.168.177.141】</p><p>然后使用命令安装nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>首先创建存放数据的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /data/k8s</span><br></pre></td></tr></table></figure><p>设置挂载路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开文件</span></span><br><span class="line">vim /etc/exports</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">/data/k8s *(rw,no_root_squash)</span><br></pre></td></tr></table></figure><h2 id="node节点上安装"><a href="#node节点上安装" class="headerlink" title="node节点上安装"></a>node节点上安装</h2><p>然后需要在k8s集群node节点上安装nfs，这里需要在 node1 和 node2节点上安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行完成后，会自动帮我们挂载上</p><h2 id="启动nfs"><a href="#启动nfs" class="headerlink" title="启动nfs"></a>启动nfs</h2><p>在node节点上配置完成后，我们就接着到刚刚nfs服务器，启动我们的nfs</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start nfs</span><br></pre></td></tr></table></figure><h2 id="配置StorageClass"><a href="#配置StorageClass" class="headerlink" title="配置StorageClass"></a>配置StorageClass</h2><p>要使用StorageClass，我们就得安装对应的自动配置程序，比如上面我们使用的是nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner，这个程序使用我们已经配置的nfs服务器，来自动创建持久卷，也就是自动帮我们创建PV</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">自动创建的 PV 以<span class="variable">$&#123;namespace&#125;</span>-<span class="variable">$&#123;pvcName&#125;</span>-<span class="variable">$&#123;pvName&#125;</span>这样的命名格式创建在 NFS 服务器上的共享数据目录中</span><br><span class="line">而当这个 PV 被回收后会以archieved-<span class="variable">$&#123;namespace&#125;</span>-<span class="variable">$&#123;pvcName&#125;</span>-<span class="variable">$&#123;pvName&#125;</span>这样的命名格式存在 NFS 服务器上。</span><br></pre></td></tr></table></figure><p>当然在部署nfs-client之前，我们需要先成功安装上 nfs 服务器，上面已经安装好了，服务地址是192.168.177.141，共享数据目录是&#x2F;data&#x2F;k8s&#x2F;，然后接下来我们部署 nfs-client 即可，我们也可以直接参考 <a class="link"   href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" >nfs-client 文档<i class="fas fa-external-link-alt"></i></a>，进行安装即可。</p><h3 id="配置Deployment"><a href="#配置Deployment" class="headerlink" title="配置Deployment"></a>配置Deployment</h3><p>首先配置 Deployment，将里面的对应的参数替换成我们自己的 nfs 配置（nfs-client.yaml）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">fuseim.pri/ifs</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="number">192.168</span><span class="number">.177</span><span class="number">.141</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_PATH</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/data/k8s</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">          <span class="attr">nfs:</span></span><br><span class="line">            <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.177</span><span class="number">.141</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/k8s</span></span><br></pre></td></tr></table></figure><h3 id="替换配置"><a href="#替换配置" class="headerlink" title="替换配置"></a>替换配置</h3><p>将环境变量 NFS_SERVER 和 NFS_PATH 替换，当然也包括下面的 nfs 配置，我们可以看到我们这里使用了一个名为 nfs-client-provisioner 的serviceAccount，所以我们也需要创建一个 sa，然后绑定上对应的权限：（nfs-client-sa.yaml）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    resources: [<span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">    verbs: [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  - apiGroups: [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    resources: [<span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">    verbs: [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line">  - apiGroups: [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    resources: [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">    verbs: [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  - apiGroups: [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    resources: [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">    verbs: [<span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  - apiGroups: [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    resources: [<span class="string">&quot;endpoints&quot;</span>]</span><br><span class="line">    verbs: [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;patch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>我们这里新建的一个名为 nfs-client-provisioner 的ServiceAccount，然后绑定了一个名为 nfs-client-provisioner-runner 的ClusterRole，而该ClusterRole声明了一些权限，其中就包括对persistentvolumes的增、删、改、查等权限，所以我们可以利用该ServiceAccount来自动创建 PV。</p><h3 id="创建StorageClass对象"><a href="#创建StorageClass对象" class="headerlink" title="创建StorageClass对象"></a>创建StorageClass对象</h3><p>nfs-client 的 Deployment 声明完成后，我们就可以来创建一个StorageClass对象了：（nfs-client-class.yaml）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">course-nfs-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">fuseim.pri/ifs</span> <span class="comment"># or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;</span></span><br></pre></td></tr></table></figure><p>我们声明了一个名为 course-nfs-storage 的StorageClass对象，注意下面的provisioner对应的值一定要和上面的Deployment下面的 PROVISIONER_NAME 这个环境变量的值一样</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">course-nfs-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">fuseim.pri/ifs</span> <span class="comment"># or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="创建资源对象"><a href="#创建资源对象" class="headerlink" title="创建资源对象"></a>创建资源对象</h3><p>在我们准备好上述的配置文件后，我们就可以开始创建我们的资源对象了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f nfs-client.yaml</span><br><span class="line">kubectl create -f nfs-client-sa.yaml</span><br><span class="line">kubectl create -f nfs-client-class.yaml</span><br></pre></td></tr></table></figure><p>创建完成后，使用下面命令来查看资源状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br><span class="line"><span class="comment"># 查看存储类</span></span><br><span class="line">kubectl get storageclass</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201123104358758.png"                      alt="image-20201123104358758"                ></p><p>我们可以设置这个 course-nfs-storage 的 StorageClass 为 Kubernetes 的默认存储后端，我们可以用 kubectl patch 命令来更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch storageclass course-nfs-storage -p <span class="string">&#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>执行完命令后，我们默认存储类就配置成功了~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;今天在配置Kubesphere的时候，出现了下面的错误&lt;/p&gt;
&lt;p&gt;&lt;img  
                     lazyloa</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之可视化界面kubesphere</title>
    <link href="http://example.com/2022/08/25/k8s%E4%B9%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2kubesphere/"/>
    <id>http://example.com/2022/08/25/k8s%E4%B9%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2kubesphere/</id>
    <published>2022-08-25T01:33:26.000Z</published>
    <updated>2022-08-25T01:36:40.372Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Kubernetes也提供了默认的dashboard页面，但是功能不是很强大，这里就不使用了</p><p>而是采用Kubesphere大桶全部的devops链路，通过kubesphere集成了很多套件</p><ul><li><a class="link"   href="https://kubesphere.io/zh/" >https://kubesphere.io/zh/<i class="fas fa-external-link-alt"></i></a> ：集群要求高</li><li><a class="link"   href="https://kuboard.cn/%EF%BC%9A%E5%BC%80%E6%BA%90kuboard%E4%B9%9F%E4%B8%8D%E9%94%99%EF%BC%8C%E9%9B%86%E7%BE%A4%E8%A6%81%E6%B1%82%E4%B8%8D%E9%AB%98%E3%80%90%E8%BD%BB%E9%87%8F%E7%BA%A7%E3%80%91" >https://kuboard.cn/：开源kuboard也不错，集群要求不高【轻量级】<i class="fas fa-external-link-alt"></i></a></li></ul><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>KubeSphere是一款面向云原生设计的开源项目，在目前主流容器调度平台Kubernetes之上构建的分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h3><p><a class="link"   href="https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/" >https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/<i class="fas fa-external-link-alt"></i></a></p><ul><li>Kubernetes 版本必须为 “1.15.x，1.16.x，1.17.x 或 1.18.x”；</li><li>确保您的计算机满足最低硬件要求：CPU &gt; 1 核，内存 &gt; 2 G；</li><li>在安装之前，需要配置 Kubernetes 集群中的默认存储类；</li><li>当使用 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数启动时，在 kube-apiserver 中会激活 CSR 签名功能。 请参阅 <a class="link"   href="https://github.com/kubesphere/kubesphere/issues/1925#issuecomment-591698309" >RKE 安装问题<i class="fas fa-external-link-alt"></i></a>；</li><li>有关在 Kubernetes 上安装 KubeSphere 的前提条件的详细信息，请参阅<a class="link"   href="https://kubesphere.com.cn/docs/installing-on-kubernetes/introduction/prerequisites/" >前提条件<i class="fas fa-external-link-alt"></i></a>。</li></ul><h3 id="安装helm"><a href="#安装helm" class="headerlink" title="安装helm"></a>安装helm</h3><p>下面我们需要在 <strong>master</strong> 节点安装 <strong>helm</strong></p><p>Helm是Kubernetes的包管理器。包管理器类似于我们在 <strong>Ubuntu</strong> 中使用的 <strong>apt</strong>。<strong>Centos</strong> 中使用的 <strong>yum</strong> 或者<strong>Python</strong> 中的 <strong>pip</strong> 一样，能快速查找、下载和安装软件包。Helm由客户端组件helm和服务端组件Tiller组成，能够将一组K8S资源打包统一管理，是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p><p>安装3.0的 helm 首先我们需要去 <a class="link"   href="https://helm.sh/docs/intro/quickstart/" >官网下载<i class="fas fa-external-link-alt"></i></a></p><ul><li>第一步，<a class="link"   href="https://github.com/helm/helm/releases" >下载helm<i class="fas fa-external-link-alt"></i></a>安装压缩文件，上传到linux系统中</li><li>第二步，解压helm压缩文件，把解压后的helm目录复制到 usr&#x2F;bin 目录中</li><li>使用命令：helm</li></ul><h2 id="部署KubeSphere"><a href="#部署KubeSphere" class="headerlink" title="部署KubeSphere"></a>部署KubeSphere</h2><h3 id="安装前"><a href="#安装前" class="headerlink" title="安装前"></a>安装前</h3><p>如果您的服务器无法访问 GitHub，则可以分别复制 <a class="link"   href="https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml" >kubesphere-installer.yaml<i class="fas fa-external-link-alt"></i></a> 和 <a class="link"   href="https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml" >cluster-configuration.yaml<i class="fas fa-external-link-alt"></i></a> 中的内容并将其粘贴到本地文件中。然后，您可以对本地文件使用 <code>kubectl apply -f</code> 来安装 KubeSphere。</p><p>同时查看k8s集群的默认存储类</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get storageclass</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201123094120860.png"                      alt="image-20201123094120860"                ></p><p>如果没有默认存储类，那么就需要安装默认的存储类，参考博客：<a class="link"   href="http://moguit.cn/#/info?blogOid=575" >Kubernetes配置默认存储类<i class="fas fa-external-link-alt"></i></a></p><p>因为我安装的是 <strong>nfs</strong>，所以在安装了 <strong>nfs</strong> 服务器启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start nfs</span><br></pre></td></tr></table></figure><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><p>如果无法正常访问github，可以提前把文件下载到本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml</span><br></pre></td></tr></table></figure><p>如果下载到了本地，可以这样安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">kubectl apply -f kubesphere-installer.yaml</span><br><span class="line">kubectl apply -f cluster-configuration.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载</span></span><br><span class="line">kubectl delete -f kubesphere-installer.yaml</span><br><span class="line">kubectl delete -f cluster-configuration.yaml</span><br></pre></td></tr></table></figure><h3 id="检查安装日志"><a href="#检查安装日志" class="headerlink" title="检查安装日志"></a>检查安装日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=<span class="string">&#x27;&#123;.items[0].metadata.name&#125;&#x27;</span>) -f</span><br></pre></td></tr></table></figure><p>然后在查看pod运行状况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kubesphere-system</span><br></pre></td></tr></table></figure><p>能够发现，我们还有两个容器正在创建</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20210110104812804.png"                      alt="image-20210110104812804"                ></p><p>使用 <code>kubectl get pod --all-namespaces</code> 查看所有 Pod 是否在 KubeSphere 的相关命名空间中正常运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure><p>能够发现所有的节点已经成功运行</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20220113102423851.png"                      alt="image-20210113102423851"                ></p><p>如果是，请通过以下命令检查控制台的端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc/ks-console -n kubesphere-system</span><br></pre></td></tr></table></figure><p>能够看到我们的服务确保在安全组中打开了端口 30880，并通过 NodePort（IP：30880）</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221123152147871.png"                      alt="image-20201123152147871"                ></p><p>使用默认帐户和密码（admin&#x2F;P@88w0rd）访问 Web 控制台。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图形化页面  admin  P@88w0rd</span></span><br><span class="line">http://192.168.177.130:30880/</span><br></pre></td></tr></table></figure><p>登录控制台后，您可以在组件中检查不同组件的状态。如果要使用相关服务，可能需要等待某些组件启动并运行。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/kubesphere-components.png"                      alt="kubesphere-components-zh"                ></p><h2 id="错误排查"><a href="#错误排查" class="headerlink" title="错误排查"></a>错误排查</h2><h3 id="错误1"><a href="#错误1" class="headerlink" title="错误1"></a>错误1</h3><p>kubesphere无法登录，提示 account is not active</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20210110110018004.png"                      alt="image-20210110110018004"                ></p><p>kubesphere 安装完成后会创建默认账户admin&#x2F;P@88w0rd，待ks-controller-manager启动就绪，user controller 会将 user CRD中定义的password加密，user会被转换为active状态，至此账户才可以正常登录。</p><p>当安装完成后遇到默认账户无法登录，看到account is not active相关错误提示时，需要检查ks-controller-manager的运行状态和日志。常见问题及解决方式如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kubesphere-system get ValidatingWebhookConfiguration users.iam.kubesphere.io -o yaml &gt;&gt; users.iam.kubesphere.io.yaml</span><br><span class="line"></span><br><span class="line">kubectl -n kubesphere-system get secret ks-controller-manager-webhook-cert -o yaml &gt;&gt; ks-controller-manager-webhook-cert.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># edit ca as pr</span></span><br><span class="line">kubectl -n kubesphere-system apply -f ks-controller-manager-webhook-cert.yaml</span><br><span class="line">kubectl -n kubesphere-system apply -f users.iam.kubesphere.io.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># restart</span></span><br><span class="line">kubectl -n kubesphere-system rollout restart deploy ks-controller-manager</span><br></pre></td></tr></table></figure><p>来源：<a class="link"   href="https://kubesphere.com.cn/forum/d/2217-account-is-not-active" >https://kubesphere.com.cn/forum/d/2217-account-is-not-active<i class="fas fa-external-link-alt"></i></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Kubernetes也提供了默认的dashboard页面，但是功能不是很强大，这里就不使用了&lt;/p&gt;
&lt;p&gt;而是采用Kubesphere大桶</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之容器交付</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E5%AE%B9%E5%99%A8%E4%BA%A4%E4%BB%98/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E5%AE%B9%E5%99%A8%E4%BA%A4%E4%BB%98/</id>
    <published>2022-08-24T08:01:57.000Z</published>
    <updated>2022-08-25T01:28:20.121Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何在k8s集群中部署Java项目"><a href="#如何在k8s集群中部署Java项目" class="headerlink" title="如何在k8s集群中部署Java项目"></a>如何在k8s集群中部署Java项目</h1><h2 id="容器交付流程"><a href="#容器交付流程" class="headerlink" title="容器交付流程"></a>容器交付流程</h2><ul><li>开发代码阶段<ul><li>编写代码</li><li>编写Dockerfile【打镜像做准备】</li></ul></li><li>持续交付&#x2F;集成<ul><li>代码编译打包</li><li>制作镜像</li><li>上传镜像仓库</li></ul></li><li>应用部署<ul><li>环境准备</li><li>Pod</li><li>Service</li><li>Ingress</li></ul></li><li>运维<ul><li>监控</li><li>故障排查</li><li>应用升级</li></ul></li></ul><h2 id="k8s部署Java项目流程"><a href="#k8s部署Java项目流程" class="headerlink" title="k8s部署Java项目流程"></a>k8s部署Java项目流程</h2><ul><li>制作镜像【Dockerfile】</li><li>上传到镜像仓库【Dockerhub、阿里云、网易】</li><li>控制器部署镜像【Deployment】</li><li>对外暴露应用【Service、Ingress】</li><li>运维【监控、升级】</li></ul><h2 id="k8s部署Java项目"><a href="#k8s部署Java项目" class="headerlink" title="k8s部署Java项目"></a>k8s部署Java项目</h2><h3 id="准备Java项目"><a href="#准备Java项目" class="headerlink" title="准备Java项目"></a>准备Java项目</h3><p>第一步，准备java项目，把java进行打包【jar包或者war包】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121213239222.png"                      alt="image-20201121213239222"                ></p><h3 id="依赖环境"><a href="#依赖环境" class="headerlink" title="依赖环境"></a>依赖环境</h3><p>在打包java项目的时候，我们首先需要两个环境</p><ul><li>java环境【JDK】</li><li>maven环境</li></ul><p>然后把java项目打包成jar包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121213654216.png"                      alt="image-20201121213654216"                ></p><h3 id="编写Dockerfile文件"><a href="#编写Dockerfile文件" class="headerlink" title="编写Dockerfile文件"></a>编写Dockerfile文件</h3><p>Dockerfile 内容如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM openjdk:8-jdk-alpine</span><br><span class="line">VOLUME /tmp</span><br><span class="line">ADD ./target/demojenkins.jar demojenkins.jar</span><br><span class="line">ENTRYPOINT [<span class="string">&quot;java&quot;</span>,<span class="string">&quot;-jar&quot;</span>,<span class="string">&quot;/demojenkins.jar&quot;</span>, <span class="string">&quot;&amp;&quot;</span>]</span><br></pre></td></tr></table></figure><h3 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h3><p>在我们创建好Dockerfile文件后，我们就可以制作镜像了</p><p>我们首先将我们的项目，放到我们的服务器上</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121214251023.png"                      alt="image-20201121214251023"                ></p><p>然后执行下面命令打包镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t java-demo-01:latest .</span><br></pre></td></tr></table></figure><p>等待一段后，即可制作完成我们的镜像</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121214701015.png"                      alt="image-20201121214701015"                ></p><p>最后通过下面命令，即可查看我们的镜像了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images;</span><br></pre></td></tr></table></figure><h3 id="启动镜像"><a href="#启动镜像" class="headerlink" title="启动镜像"></a>启动镜像</h3><p>在我们制作完成镜像后，我们就可以启动我们的镜像了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 8111:8111 java-demo-01:latest -t</span><br></pre></td></tr></table></figure><p>启动完成后，我们通过浏览器进行访问，即可看到我们的java程序</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.177.130:8111/user</span><br></pre></td></tr></table></figure><h3 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h3><p>下面我们需要将我们制作好的镜像，上传到镜像服务器中【阿里云、DockerHub】</p><p>首先我们需要到 阿里云 <a class="link"   href="https://cr.console.aliyun.com/cn-hangzhou/instances/repositories" >容器镜像服务<i class="fas fa-external-link-alt"></i></a>，然后开始创建镜像仓库</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121223435851.png"                      alt="image-20201121223435851"                ></p><p>然后选择本地仓库</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121223516789.png"                      alt="image-20201121223516789"                ></p><p>我们点击我们刚刚创建的镜像仓库，就能看到以下的信息</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121224233092.png"                      alt="image-20201121224233092"                ></p><h4 id="登录镜像服务器"><a href="#登录镜像服务器" class="headerlink" title="登录镜像服务器"></a>登录镜像服务器</h4><p>使用命令登录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login --username=XXXXXXX@163.com registry.cn-shenzhen.aliyuncs.com</span><br></pre></td></tr></table></figure><p>然后输入刚刚我们开放时候的注册的密码</p><h4 id="镜像添加版本号"><a href="#镜像添加版本号" class="headerlink" title="镜像添加版本号"></a>镜像添加版本号</h4><p>下面为我们的镜像添加版本号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例</span></span><br><span class="line">docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/mogublog/java-project-01:[镜像版本号]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line">docker tag 33f11349c27d registry.cn-shenzhen.aliyuncs.com/mogublog/java-project-01:1.0.0</span><br></pre></td></tr></table></figure><p>操作完成后</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121224609890.png"                      alt="image-20201121224609890"                ></p><h4 id="推送镜像-1"><a href="#推送镜像-1" class="headerlink" title="推送镜像"></a>推送镜像</h4><p>在我们添加版本号信息后，我们就可以推送我们的镜像到阿里云了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push registry.cn-shenzhen.aliyuncs.com/mogublog/java-project-01:1.0.0</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121224714068.png"                      alt="image-20201121224714068"                ></p><p>操作完成后，我们在我们的阿里云镜像服务，就能看到推送上来的镜像了</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121224858651.png"                      alt="image-20201121224858651"                ></p><h3 id="控制器部署镜像"><a href="#控制器部署镜像" class="headerlink" title="控制器部署镜像"></a>控制器部署镜像</h3><p>在我们推送镜像到服务器后，就可以通过控制器部署镜像了</p><p>首先我们需要根据刚刚的镜像，导出yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导出yaml</span></span><br><span class="line">kubectl create deployment  javademo1 --image=registry.cn-</span><br><span class="line">shenzhen.aliyuncs.com/mogublog/java-project-01:1.0.0 --dry-run -o yaml &gt; javademo1.yaml</span><br></pre></td></tr></table></figure><p>导出后的 javademo1.yaml 如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: javademo1</span><br><span class="line">  name: javademo1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: javademo1</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: javademo1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: registry.cn-shenzhen.aliyuncs.com/mogublog/java-project-01:1.0.0</span><br><span class="line">        name: java-project-01</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure><p>然后通过下面命令，通过yaml创建我们的deployment</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">kubectl apply -f javademo1.yaml</span><br><span class="line"><span class="comment"># 查看 pods</span></span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121225413122.png"                      alt="image-20201121225413122"                ></p><p>或者我们可以进行扩容，多创建几个副本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment javademo1 --replicas=3</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121225600554.png"                      alt="image-20201121225600554"                ></p><p>然后我们还需要对外暴露端口【通过service 或者 Ingress】</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对外暴露端口</span></span><br><span class="line">kubectl expose deployment javademo1 --port=8111  --target-port=8111 --<span class="built_in">type</span>=NodePort</span><br><span class="line"><span class="comment"># 查看对外端口号</span></span><br><span class="line">kubectl get svc</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121225818003.png"                      alt="image-20201121225818003"                ></p><p>然后通过下面的地址访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对内访问</span></span><br><span class="line">curl http://10.106.103.242:8111/user</span><br><span class="line"><span class="comment"># 对外访问</span></span><br><span class="line">http://192.168.177.130:32190/user</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何在k8s集群中部署Java项目&quot;&gt;&lt;a href=&quot;#如何在k8s集群中部署Java项目&quot; class=&quot;headerlink&quot; title=&quot;如何在k8s集群中部署Java项目&quot;&gt;&lt;/a&gt;如何在k8s集群中部署Java项目&lt;/h1&gt;&lt;h2 id=&quot;容器交付流</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之搭建高可用集群</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</id>
    <published>2022-08-24T07:11:44.000Z</published>
    <updated>2022-08-25T01:27:32.183Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前我们搭建的集群，只有一个master节点，当master节点宕机的时候，通过node将无法继续访问，而master主要是管理作用，所以整个集群将无法提供服务</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121164522945.png"                      alt="image-20201121164522945"                ></p><h2 id="高可用集群"><a href="#高可用集群" class="headerlink" title="高可用集群"></a>高可用集群</h2><p>下面我们就需要搭建一个多master节点的高可用集群，不会存在单点故障问题</p><p>但是在node 和 master节点之间，需要存在一个 LoadBalancer组件，作用如下：</p><ul><li>负载</li><li>检查master节点的状态</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121164931760.png"                      alt="image-20201121164931760"                ></p><p>对外有一个统一的VIP：虚拟ip来对外进行访问</p><h2 id="高可用集群技术细节"><a href="#高可用集群技术细节" class="headerlink" title="高可用集群技术细节"></a>高可用集群技术细节</h2><p>高可用集群技术细节如下所示：</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121165325194.png"                      alt="image-20201121165325194"                ></p><ul><li>keepalived：配置虚拟ip，检查节点的状态</li><li>haproxy：负载均衡服务【类似于nginx】</li><li>apiserver：</li><li>controller：</li><li>manager：</li><li>scheduler：</li></ul><h2 id="高可用集群步骤"><a href="#高可用集群步骤" class="headerlink" title="高可用集群步骤"></a>高可用集群步骤</h2><p>我们采用2个master节点，一个node节点来搭建高可用集群，下面给出了每个节点需要做的事情</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121170351461.png"                      alt="image-20201121170351461"                ></p><h2 id="初始化操作"><a href="#初始化操作" class="headerlink" title="初始化操作"></a>初始化操作</h2><p>我们需要在这三个节点上进行操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭防火墙</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭selinux</span></span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config  </span><br><span class="line"><span class="comment"># 临时关闭</span></span><br><span class="line">setenforce 0  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭swap</span></span><br><span class="line"><span class="comment"># 临时</span></span><br><span class="line">swapoff -a </span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">sed -ri <span class="string">&#x27;s/.*swap.*/#&amp;/&#x27;</span> /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据规划设置主机名【master1节点上操作】</span></span><br><span class="line">hostnamectl set-hostname master1</span><br><span class="line"><span class="comment"># 根据规划设置主机名【master2节点上操作】</span></span><br><span class="line">hostnamectl set-hostname master1</span><br><span class="line"><span class="comment"># 根据规划设置主机名【node1节点操作】</span></span><br><span class="line">hostnamectl set-hostname node1</span><br><span class="line"></span><br><span class="line"><span class="comment"># r添加hosts</span></span><br><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/hosts &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">192.168.44.158  k8smaster</span></span><br><span class="line"><span class="string">192.168.44.155 master01.k8s.io master1</span></span><br><span class="line"><span class="string">192.168.44.156 master02.k8s.io master2</span></span><br><span class="line"><span class="string">192.168.44.157 node01.k8s.io node1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将桥接的IPv4流量传递到iptables的链【3个节点上都执行】</span></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/sysctl.d/k8s.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-ip6tables = 1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables = 1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生效</span></span><br><span class="line">sysctl --system  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 时间同步</span></span><br><span class="line">yum install ntpdate -y</span><br><span class="line">ntpdate time.windows.com</span><br></pre></td></tr></table></figure><h2 id="部署keepAlived"><a href="#部署keepAlived" class="headerlink" title="部署keepAlived"></a>部署keepAlived</h2><p>下面我们需要在所有的master节点【master1和master2】上部署keepAlive</p><h3 id="安装相关包"><a href="#安装相关包" class="headerlink" title="安装相关包"></a>安装相关包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装相关工具</span></span><br><span class="line">yum install -y conntrack-tools libseccomp libtool-ltdl</span><br><span class="line"><span class="comment"># 安装keepalived</span></span><br><span class="line">yum install -y keepalived</span><br></pre></td></tr></table></figure><h3 id="配置master节点"><a href="#配置master节点" class="headerlink" title="配置master节点"></a>配置master节点</h3><p>添加master1的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/keepalived/keepalived.conf &lt;&lt;<span class="string">EOF </span></span><br><span class="line"><span class="string">! Configuration File for keepalived</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">global_defs &#123;</span></span><br><span class="line"><span class="string">   router_id k8s</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_script check_haproxy &#123;</span></span><br><span class="line"><span class="string">    script &quot;killall -0 haproxy&quot;</span></span><br><span class="line"><span class="string">    interval 3</span></span><br><span class="line"><span class="string">    weight -2</span></span><br><span class="line"><span class="string">    fall 10</span></span><br><span class="line"><span class="string">    rise 2</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_instance VI_1 &#123;</span></span><br><span class="line"><span class="string">    state MASTER </span></span><br><span class="line"><span class="string">    interface ens33 </span></span><br><span class="line"><span class="string">    virtual_router_id 51</span></span><br><span class="line"><span class="string">    priority 250</span></span><br><span class="line"><span class="string">    advert_int 1</span></span><br><span class="line"><span class="string">    authentication &#123;</span></span><br><span class="line"><span class="string">        auth_type PASS</span></span><br><span class="line"><span class="string">        auth_pass ceb1b3ec013d66163d6ab</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    virtual_ipaddress &#123;</span></span><br><span class="line"><span class="string">        192.168.44.158</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    track_script &#123;</span></span><br><span class="line"><span class="string">        check_haproxy</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>添加master2的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/keepalived/keepalived.conf &lt;&lt;<span class="string">EOF </span></span><br><span class="line"><span class="string">! Configuration File for keepalived</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">global_defs &#123;</span></span><br><span class="line"><span class="string">   router_id k8s</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_script check_haproxy &#123;</span></span><br><span class="line"><span class="string">    script &quot;killall -0 haproxy&quot;</span></span><br><span class="line"><span class="string">    interval 3</span></span><br><span class="line"><span class="string">    weight -2</span></span><br><span class="line"><span class="string">    fall 10</span></span><br><span class="line"><span class="string">    rise 2</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_instance VI_1 &#123;</span></span><br><span class="line"><span class="string">    state BACKUP </span></span><br><span class="line"><span class="string">    interface ens33 </span></span><br><span class="line"><span class="string">    virtual_router_id 51</span></span><br><span class="line"><span class="string">    priority 200</span></span><br><span class="line"><span class="string">    advert_int 1</span></span><br><span class="line"><span class="string">    authentication &#123;</span></span><br><span class="line"><span class="string">        auth_type PASS</span></span><br><span class="line"><span class="string">        auth_pass ceb1b3ec013d66163d6ab</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    virtual_ipaddress &#123;</span></span><br><span class="line"><span class="string">        192.168.44.158</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    track_script &#123;</span></span><br><span class="line"><span class="string">        check_haproxy</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h3 id="启动和检查"><a href="#启动和检查" class="headerlink" title="启动和检查"></a>启动和检查</h3><p>在两台master节点都执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动keepalived</span></span><br><span class="line">systemctl start keepalived.service</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived.service</span><br><span class="line"><span class="comment"># 查看启动状态</span></span><br><span class="line">systemctl status keepalived.service</span><br></pre></td></tr></table></figure><p>启动后查看master的网卡信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip a s ens33</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121171619497.png"                      alt="image-20201121171619497"                ></p><h2 id="部署haproxy"><a href="#部署haproxy" class="headerlink" title="部署haproxy"></a>部署haproxy</h2><p>haproxy主要做负载的作用，将我们的请求分担到不同的node节点上</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在两个master节点安装 haproxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装haproxy</span></span><br><span class="line">yum install -y haproxy</span><br><span class="line"><span class="comment"># 启动 haproxy</span></span><br><span class="line">systemctl start haproxy</span><br><span class="line"><span class="comment"># 开启自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br></pre></td></tr></table></figure><p>启动后，我们查看对应的端口是否包含 16443</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp | grep haproxy</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121181803128.png"                      alt="image-20201121181803128"                ></p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>两台master节点的配置均相同，配置中声明了后端代理的两个master节点服务器，指定了haproxy运行的端口为16443等，因此16443端口为集群的入口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/haproxy/haproxy.cfg &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># Global settings</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">global</span></span><br><span class="line"><span class="string">    # to have these messages end up in /var/log/haproxy.log you will</span></span><br><span class="line"><span class="string">    # need to:</span></span><br><span class="line"><span class="string">    # 1) configure syslog to accept network log events.  This is done</span></span><br><span class="line"><span class="string">    #    by adding the &#x27;-r&#x27; option to the SYSLOGD_OPTIONS in</span></span><br><span class="line"><span class="string">    #    /etc/sysconfig/syslog</span></span><br><span class="line"><span class="string">    # 2) configure local2 events to go to the /var/log/haproxy.log</span></span><br><span class="line"><span class="string">    #   file. A line like the following can be added to</span></span><br><span class="line"><span class="string">    #   /etc/sysconfig/syslog</span></span><br><span class="line"><span class="string">    #</span></span><br><span class="line"><span class="string">    #    local2.*                       /var/log/haproxy.log</span></span><br><span class="line"><span class="string">    #</span></span><br><span class="line"><span class="string">    log         127.0.0.1 local2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    chroot      /var/lib/haproxy</span></span><br><span class="line"><span class="string">    pidfile     /var/run/haproxy.pid</span></span><br><span class="line"><span class="string">    maxconn     4000</span></span><br><span class="line"><span class="string">    user        haproxy</span></span><br><span class="line"><span class="string">    group       haproxy</span></span><br><span class="line"><span class="string">    daemon </span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">    # turn on stats unix socket</span></span><br><span class="line"><span class="string">    stats socket /var/lib/haproxy/stats</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># common defaults that all the &#x27;listen&#x27; and &#x27;backend&#x27; sections will</span></span><br><span class="line"><span class="string"># use if not designated in their block</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------  </span></span><br><span class="line"><span class="string">defaults</span></span><br><span class="line"><span class="string">    mode                    http</span></span><br><span class="line"><span class="string">    log                     global</span></span><br><span class="line"><span class="string">    option                  httplog</span></span><br><span class="line"><span class="string">    option                  dontlognull</span></span><br><span class="line"><span class="string">    option http-server-close</span></span><br><span class="line"><span class="string">    option forwardfor       except 127.0.0.0/8</span></span><br><span class="line"><span class="string">    option                  redispatch</span></span><br><span class="line"><span class="string">    retries                 3</span></span><br><span class="line"><span class="string">    timeout http-request    10s</span></span><br><span class="line"><span class="string">    timeout queue           1m</span></span><br><span class="line"><span class="string">    timeout connect         10s</span></span><br><span class="line"><span class="string">    timeout client          1m</span></span><br><span class="line"><span class="string">    timeout server          1m</span></span><br><span class="line"><span class="string">    timeout http-keep-alive 10s</span></span><br><span class="line"><span class="string">    timeout check           10s</span></span><br><span class="line"><span class="string">    maxconn                 3000</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># kubernetes apiserver frontend which proxys to the backends</span></span><br><span class="line"><span class="string">#--------------------------------------------------------------------- </span></span><br><span class="line"><span class="string">frontend kubernetes-apiserver</span></span><br><span class="line"><span class="string">    mode                 tcp</span></span><br><span class="line"><span class="string">    bind                 *:16443</span></span><br><span class="line"><span class="string">    option               tcplog</span></span><br><span class="line"><span class="string">    default_backend      kubernetes-apiserver    </span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># round robin balancing between the various backends</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">backend kubernetes-apiserver</span></span><br><span class="line"><span class="string">    mode        tcp</span></span><br><span class="line"><span class="string">    balance     roundrobin</span></span><br><span class="line"><span class="string">    server      master01.k8s.io   192.168.44.155:6443 check</span></span><br><span class="line"><span class="string">    server      master02.k8s.io   192.168.44.156:6443 check</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># collection haproxy statistics message</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">listen stats</span></span><br><span class="line"><span class="string">    bind                 *:1080</span></span><br><span class="line"><span class="string">    stats auth           admin:awesomePassword</span></span><br><span class="line"><span class="string">    stats refresh        5s</span></span><br><span class="line"><span class="string">    stats realm          HAProxy\ Statistics</span></span><br><span class="line"><span class="string">    stats uri            /admin?stats</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h2 id="安装Docker、Kubeadm、kubectl"><a href="#安装Docker、Kubeadm、kubectl" class="headerlink" title="安装Docker、Kubeadm、kubectl"></a>安装Docker、Kubeadm、kubectl</h2><p>所有节点安装Docker&#x2F;kubeadm&#x2F;kubelet ，Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker</p><h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>首先配置一下Docker的阿里yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;/etc/yum.repos.d/docker.repo&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[docker-ce-edge]</span></span><br><span class="line"><span class="string">name=Docker CE Edge - \$basearch</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/\$basearch/edge</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后yum方式安装docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum安装</span></span><br><span class="line">yum -y install docker-ce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看docker版本</span></span><br><span class="line">docker --version  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动docker</span></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><p>配置docker的镜像源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/docker/daemon.json &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后重启docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="添加kubernetes软件源"><a href="#添加kubernetes软件源" class="headerlink" title="添加kubernetes软件源"></a>添加kubernetes软件源</h3><p>然后我们还需要配置一下yum的k8s软件源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">[kubernetes]</span></span><br><span class="line"><span class="string">name=Kubernetes</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=0</span></span><br><span class="line"><span class="string">repo_gpgcheck=0</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h3 id="安装kubeadm，kubelet和kubectl"><a href="#安装kubeadm，kubelet和kubectl" class="headerlink" title="安装kubeadm，kubelet和kubectl"></a>安装kubeadm，kubelet和kubectl</h3><p>由于版本更新频繁，这里指定版本号部署：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装kubelet、kubeadm、kubectl，同时指定版本</span></span><br><span class="line">yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br></pre></td></tr></table></figure><h2 id="部署Kubernetes-Master【master节点】"><a href="#部署Kubernetes-Master【master节点】" class="headerlink" title="部署Kubernetes Master【master节点】"></a>部署Kubernetes Master【master节点】</h2><h3 id="创建kubeadm配置文件"><a href="#创建kubeadm配置文件" class="headerlink" title="创建kubeadm配置文件"></a>创建kubeadm配置文件</h3><p>在具有vip的master上进行初始化操作，这里为master1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/kubernetes/manifests -p</span><br><span class="line"><span class="comment"># 到manifests目录</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/kubernetes/manifests/</span><br><span class="line"><span class="comment"># 新建yaml文件</span></span><br><span class="line">vi kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>yaml内容如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">apiServer:</span><br><span class="line">  certSANs:</span><br><span class="line">    - master1</span><br><span class="line">    - master2</span><br><span class="line">    - master.k8s.io</span><br><span class="line">    - 192.168.44.158</span><br><span class="line">    - 192.168.44.155</span><br><span class="line">    - 192.168.44.156</span><br><span class="line">    - 127.0.0.1</span><br><span class="line">  extraArgs:</span><br><span class="line">    authorization-mode: Node,RBAC</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">&quot;master.k8s.io:16443&quot;</span></span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: </span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:    </span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.16.3</span><br><span class="line">networking: </span><br><span class="line">  dnsDomain: cluster.local  </span><br><span class="line">  podSubnet: 10.244.0.0/16</span><br><span class="line">  serviceSubnet: 10.1.0.0/16</span><br><span class="line">scheduler: &#123;&#125;</span><br></pre></td></tr></table></figure><p>然后我们在 master1 节点执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>执行完成后，就会在拉取我们的进行了【需要等待…】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121194928988.png"                      alt="image-20201121194928988"                ></p><p>按照提示配置环境变量，使用kubectl工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行下方命令</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"><span class="comment"># 查看节点</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"><span class="comment"># 查看pod</span></span><br><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><p><strong>按照提示保存以下内容，一会要使用：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token jv5z7n.3y1zi95p952y9p65 \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:403bca185c2f3a4791685013499e7ce58f9848e2213e27194b75a2e3293d8812 \</span><br><span class="line">    --control-plane </span><br></pre></td></tr></table></figure><blockquote><p>–control-plane ： 只有在添加master节点的时候才有</p></blockquote><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">kubectl get cs</span><br><span class="line"><span class="comment"># 查看pod</span></span><br><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><h2 id="安装集群网络"><a href="#安装集群网络" class="headerlink" title="安装集群网络"></a>安装集群网络</h2><p>从官方地址获取到flannel的yaml，在master1上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> flannel</span><br><span class="line"><span class="built_in">cd</span> flannel</span><br><span class="line"><span class="comment"># 下载yaml文件</span></span><br><span class="line">wget -c https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>安装flannel网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f kube-flannel.yml </span><br></pre></td></tr></table></figure><p>检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><h2 id="master2节点加入集群"><a href="#master2节点加入集群" class="headerlink" title="master2节点加入集群"></a>master2节点加入集群</h2><h3 id="复制密钥及相关文件"><a href="#复制密钥及相关文件" class="headerlink" title="复制密钥及相关文件"></a>复制密钥及相关文件</h3><p>从master1复制密钥及相关文件到master2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh root@192.168.44.156 mkdir -p /etc/kubernetes/pki/etcd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp /etc/kubernetes/admin.conf root@192.168.44.156:/etc/kubernetes</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># scp /etc/kubernetes/pki/&#123;ca.*,sa.*,front-proxy-ca.*&#125; root@192.168.44.156:/etc/kubernetes/pki</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># scp /etc/kubernetes/pki/etcd/ca.* root@192.168.44.156:/etc/kubernetes/pki/etcd</span></span><br></pre></td></tr></table></figure><h3 id="master2加入集群"><a href="#master2加入集群" class="headerlink" title="master2加入集群"></a>master2加入集群</h3><p>执行在master1上init后输出的join命令,需要带上参数<code>--control-plane</code>表示把master控制节点加入集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b     --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba --control-plane</span><br></pre></td></tr></table></figure><p>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line"></span><br><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="加入Kubernetes-Node"><a href="#加入Kubernetes-Node" class="headerlink" title="加入Kubernetes Node"></a>加入Kubernetes Node</h2><p>在node1上执行</p><p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b     --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba</span><br></pre></td></tr></table></figure><p><strong>集群网络重新安装，因为添加了新的node节点</strong></p><p>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="测试kubernetes集群"><a href="#测试kubernetes集群" class="headerlink" title="测试kubernetes集群"></a>测试kubernetes集群</h2><p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建nginx deployment</span></span><br><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get pod,svc</span><br></pre></td></tr></table></figure><p>然后我们通过任何一个节点，都能够访问我们的nginx页面</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;之前我们搭建的集群，只有一个master节点，当master节点宕机的时候，通过node将无法继续访问，而master主要是管理作用，所以整</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之集群资源监控</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7/</id>
    <published>2022-08-24T02:53:12.000Z</published>
    <updated>2022-08-25T01:27:28.981Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="监控指标"><a href="#监控指标" class="headerlink" title="监控指标"></a>监控指标</h3><p>一个好的系统，主要监控以下内容</p><ul><li>集群监控<ul><li>节点资源利用率</li><li>节点数</li><li>运行Pods</li></ul></li><li>Pod监控<ul><li>容器指标</li><li>应用程序【程序占用多少CPU、内存】</li></ul></li></ul><h3 id="监控平台"><a href="#监控平台" class="headerlink" title="监控平台"></a>监控平台</h3><p>使用普罗米修斯【prometheus】 + Grafana 搭建监控平台</p><ul><li><p>prometheus【定时搜索被监控服务的状态】</p><ul><li>开源的</li><li>监控、报警、数据库</li><li>以HTTP协议周期性抓取被监控组件状态</li><li>不需要复杂的集成过程，使用http接口接入即可</li></ul></li><li><p>Grafana</p><ul><li>开源的数据分析和可视化工具</li><li>支持多种数据源</li></ul></li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20221120082257441.png"                      alt="image-20201120082257441"                ></p><h2 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h2><p>首先需要部署一个守护进程</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120083606298.png"                      alt="image-20201120083606298"                ></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: node-exporter</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: node-exporter</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: prom/node-exporter</span><br><span class="line">        name: node-exporter</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9100</span><br><span class="line">          protocol: TCP</span><br><span class="line">          name: http</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 9100</span><br><span class="line">    nodePort: 31672</span><br><span class="line">    protocol: TCP</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: node-exporter</span><br></pre></td></tr></table></figure><p>然后执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f node-exporter.yaml</span><br></pre></td></tr></table></figure><p>执行完，发现会报错</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120084034160.png"                      alt="image-20201120084034160"                ></p><p>这是因为版本不一致的问题，因为发布的正式版本，而这个属于测试版本</p><p>所以我们找到第一行，然后把内容修改为如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line"><span class="comment"># 修改后 【正式版本发布后，测试版本不能使用】</span></span><br><span class="line">apiVersion: apps/v1</span><br></pre></td></tr></table></figure><p>创建完成后的效果</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120085721454.png"                      alt="image-20201120085721454"                ></p><p>然后通过yaml的方式部署prometheus</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120083107594.png"                      alt="image-20201120083107594"                ></p><ul><li>configmap：定义一个configmap：存储一些配置文件【不加密】</li><li>prometheus.deploy.yaml：部署一个deployment【包括端口号，资源限制】</li><li>prometheus.svc.yaml：对外暴露的端口</li><li>rbac-setup.yaml：分配一些角色的权限</li></ul><p>下面我们进入目录下，首先部署 rbac-setup.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f rbac-setup.yaml</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120090002150.png"                      alt="image-20201120090002150"                ></p><p>然后分别部署</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署configmap</span></span><br><span class="line">kubectl create -f configmap.yaml</span><br><span class="line"><span class="comment"># 部署deployment</span></span><br><span class="line">kubectl create -f prometheus.deploy.yml</span><br><span class="line"><span class="comment"># 部署svc</span></span><br><span class="line">kubectl create -f prometheus.svc.yml</span><br></pre></td></tr></table></figure><p>部署完成后，我们使用下面命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120093213576.png"                      alt="image-20201120093213576"                ></p><p>在我们部署完成后，即可看到 prometheus 的 pod了，然后通过下面命令，能够看到对应的端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n kube-system</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121091348752.png"                      alt="image-20201121091348752"                ></p><p>通过这个，我们可以看到 <code>prometheus</code> 对外暴露的端口为 30003，访问页面即可对应的图形化界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.177.130:30003</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121091508851.png"                      alt="image-20201121091508851"                ></p><p>在上面我们部署完prometheus后，我们还需要来部署grafana</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f grafana-deploy.yaml</span><br></pre></td></tr></table></figure><p>然后执行完后，发现下面的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: unable to recognize <span class="string">&quot;grafana-deploy.yaml&quot;</span>: no matches <span class="keyword">for</span> kind <span class="string">&quot;Deployment&quot;</span> <span class="keyword">in</span> version <span class="string">&quot;extensions/v1beta1&quot;</span></span><br></pre></td></tr></table></figure><p>我们需要修改如下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加selector</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: grafana</span><br><span class="line">      component: core</span><br></pre></td></tr></table></figure><p>修改完成后，我们继续执行上述代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建deployment</span></span><br><span class="line">kubectl create -f grafana-deploy.yaml</span><br><span class="line"><span class="comment"># 创建svc</span></span><br><span class="line">kubectl create -f grafana-svc.yaml</span><br><span class="line"><span class="comment"># 创建 ing</span></span><br><span class="line">kubectl create -f grafana-ing.yaml</span><br></pre></td></tr></table></figure><p>我们能看到，我们的grafana正在</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120110426534.png"                      alt="image-20201120110426534"                ></p><h3 id="配置数据源"><a href="#配置数据源" class="headerlink" title="配置数据源"></a>配置数据源</h3><p>下面我们需要开始打开 Grafana，然后配置数据源，导入数据显示模板</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n kube-system</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120111949197.png"                      alt="image-20201120111949197"                ></p><p>我们可以通过 ip + 30431 访问我们的 grafana 图形化页面</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201120112048887.png"                      alt="image-20201120112048887"                ></p><p>然后输入账号和密码：admin admin</p><p>进入后，我们就需要配置 prometheus 的数据源</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092012018.png"                      alt="image-20201121092012018"                ></p><p> 和 对应的IP【这里IP是我们的ClusterIP】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092053215.png"                      alt="image-20201121092053215"                ></p><h3 id="设置显示数据的模板"><a href="#设置显示数据的模板" class="headerlink" title="设置显示数据的模板"></a>设置显示数据的模板</h3><p>选择Dashboard，导入我们的模板</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092312118.png"                      alt="image-20201121092312118"                ></p><p>然后输入 315 号模板</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092418180.png"                      alt="image-20201121092418180"                ></p><p>然后选择 prometheus数据源 mydb，导入即可</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092443266.png"                      alt="image-20201121092443266"                ></p><p>导入后的效果如下所示</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201121092610154.png"                      alt="image-20201121092610154"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;h3 id=&quot;监控指标&quot;&gt;&lt;a href=&quot;#监控指标&quot; class=&quot;headerlink&quot; title=&quot;监控指标&quot;&gt;&lt;/a&gt;监控指标&lt;/h</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之持久化存储</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</id>
    <published>2022-08-24T02:26:45.000Z</published>
    <updated>2022-08-25T01:25:09.448Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前我们有提到数据卷：<code>emptydir</code> ，是本地存储，pod重启，数据就不存在了，需要对数据持久化存储</p><p>对于数据持久化存储【pod重启，数据还存在】，有两种方式</p><ul><li>nfs：网络存储【通过一台服务器来存储】</li></ul><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="持久化服务器上操作"><a href="#持久化服务器上操作" class="headerlink" title="持久化服务器上操作"></a>持久化服务器上操作</h3><ul><li>找一台新的服务器nfs服务端，安装nfs</li><li>设置挂载路径</li></ul><p>使用命令安装nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>首先创建存放数据的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /data/nfs</span><br></pre></td></tr></table></figure><p>设置挂载路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开文件</span></span><br><span class="line">vim /etc/exports</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">/data/nfs *(rw,no_root_squash)</span><br></pre></td></tr></table></figure><p>执行完成后，即部署完我们的持久化服务器</p><h3 id="Node节点上操作"><a href="#Node节点上操作" class="headerlink" title="Node节点上操作"></a>Node节点上操作</h3><p>然后需要在k8s集群node节点上安装nfs，这里需要在 node1 和 node2节点上安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行完成后，会自动帮我们挂载上</p><h3 id="启动nfs服务端"><a href="#启动nfs服务端" class="headerlink" title="启动nfs服务端"></a>启动nfs服务端</h3><p>下面我们回到nfs服务端，启动我们的nfs服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">systemctl start nfs</span><br><span class="line"><span class="comment"># 或者使用以下命令进行启动</span></span><br><span class="line">service nfs-server start</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/24/qnCwJT9V4tSFWRh.png"                      alt="image-20201119082047766"                ></p><h3 id="K8s集群部署应用"><a href="#K8s集群部署应用" class="headerlink" title="K8s集群部署应用"></a>K8s集群部署应用</h3><p>最后我们在k8s集群上部署应用，使用nfs持久化存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个pv文件</span></span><br><span class="line"><span class="built_in">mkdir</span> pv</span><br><span class="line"><span class="comment"># 进入</span></span><br><span class="line"><span class="built_in">cd</span> pv</span><br></pre></td></tr></table></figure><p>然后创建一个yaml文件  <code>nfs-nginx.yaml</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/24/qnCwJT9V4tSFWRh.png"                                     ></p><p>通过这个方式，就挂载到了刚刚我们的nfs数据节点下的 &#x2F;data&#x2F;nfs 目录</p><p>最后就变成了：  &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html    -&gt;  192.168.44.134&#x2F;data&#x2F;nfs   内容是对应的</p><p>我们通过这个 yaml文件，创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nfs-nginx.yaml</span><br></pre></td></tr></table></figure><p>创建完成后，我们也可以查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod nginx-dep1</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119083444454.png"                      alt="image-20201119083444454"                ></p><p>可以看到，我们的pod已经成功创建出来了，同时下图也是出于Running状态</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119083514247.png"                      alt="image-20201119083514247"                ></p><p>下面我们就可以进行测试了，比如现在nfs服务节点上添加数据，然后在看数据是否存在 pod中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入pod中查看</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it nginx-dep1 bash</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119095847548.png"                      alt="image-20201119095847548"                ></p><h2 id="PV和PVC"><a href="#PV和PVC" class="headerlink" title="PV和PVC"></a>PV和PVC</h2><p>对于上述的方式，我们都知道，我们的ip 和端口是直接放在我们的容器上的，这样管理起来可能不方便</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119082317625.png"                      alt="image-20201119082317625"                ></p><p>所以这里就需要用到 pv  和 pvc的概念了，方便我们配置和管理我们的 ip 地址等元信息</p><p>PV：持久化存储，对存储的资源进行抽象，对外提供可以调用的地方【生产者】</p><p>PVC：用于调用，不需要关心内部实现细节【消费者】</p><p>PV 和 PVC 使得 K8S 集群具备了存储的逻辑抽象能力。使得在配置Pod的逻辑里可以忽略对实际后台存储<br>技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟<br>计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变<br>化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集<br>群的使用者即服务的管理员来配置。</p><h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><ul><li>PVC绑定PV</li><li>定义PVC</li><li>定义PV【数据卷定义，指定数据存储服务器的ip、路径、容量和匹配模式】</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>创建一个 pvc.yaml</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119101753419.png"                      alt="image-20201119101753419"                ></p><p>第一部分是定义一个 deployment，做一个部署</p><ul><li>副本数：3</li><li>挂载路径</li><li>调用：是通过pvc的模式</li></ul><p>然后定义pvc</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119101843498.png"                      alt="image-20201119101843498"                ></p><p>然后在创建一个 <code>pv.yaml</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119101957777.png"                      alt="image-20201119101957777"                ></p><p>然后就可以创建pod了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f pv.yaml</span><br></pre></td></tr></table></figure><p>然后我们就可以通过下面命令，查看我们的 pv  和 pvc之间的绑定关系</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pv, pvc</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119102332786.png"                      alt="image-20201119102332786"                ></p><p>到这里为止，我们就完成了我们 pv 和 pvc的绑定操作，通过之前的方式，进入pod中查看内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubect <span class="built_in">exec</span> -it nginx-dep1 bash</span><br></pre></td></tr></table></figure><p>然后查看  &#x2F;usr&#x2F;share&#x2F;nginx.html</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119102448226.png"                      alt="image-20201119102448226"                ></p><p>也同样能看到刚刚的内容，其实这种操作和之前我们的nfs是一样的，只是多了一层pvc绑定pv的操作</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;之前我们有提到数据卷：&lt;code&gt;emptydir&lt;/code&gt; ，是本地存储，pod重启，数据就不存在了，需要对数据持久化存储&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之核心技术Helm</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFHelm/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFHelm/</id>
    <published>2022-08-24T01:36:30.000Z</published>
    <updated>2022-08-25T01:24:25.141Z</updated>
    
    <content type="html"><![CDATA[<p>Helm就是一个包管理工具【类似于npm】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/892532-20180224212352306-705544441.png"                      alt="892532-20180224212352306-705544441"                ></p><h2 id="为什么引入Helm"><a href="#为什么引入Helm" class="headerlink" title="为什么引入Helm"></a>为什么引入Helm</h2><p>首先在原来项目中都是基于yaml文件来进行部署发布的，而目前项目大部分微服务化或者模块化，会分成很多个组件来部署，每个组件可能对应一个deployment.yaml,一个service.yaml,一个Ingress.yaml还可能存在各种依赖关系，这样一个项目如果有5个组件，很可能就有15个不同的yaml文件，这些yaml分散存放，如果某天进行项目恢复的话，很难知道部署顺序，依赖关系等，而所有这些包括</p><ul><li>基于yaml配置的集中存放</li><li>基于项目的打包</li><li>组件间的依赖</li></ul><p>但是这种方式部署，会有什么问题呢？</p><ul><li>如果使用之前部署单一应用，少数服务的应用，比较合适</li><li>但如果部署微服务项目，可能有几十个服务，每个服务都有一套yaml文件，需要维护大量的yaml文件，版本管理特别不方便</li></ul><p>Helm的引入，就是为了解决这个问题</p><ul><li>使用Helm可以把这些YAML文件作为整体管理</li><li>实现YAML文件高效复用</li><li>使用helm应用级别的版本管理</li></ul><h2 id="Helm介绍"><a href="#Helm介绍" class="headerlink" title="Helm介绍"></a>Helm介绍</h2><p>Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum&#x2F;apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。</p><p>Helm有三个重要概念</p><ul><li>helm：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理</li><li>Chart：应用描述，一系列用于描述k8s资源相关文件的集合</li><li>Release：基于Chart的部署实体，一个chart被Helm运行后将会生成对应的release，将在K8S中创建出真实的运行资源对象。也就是应用级别的版本管理</li><li>Repository：用于发布和存储Chart的仓库</li></ul><h2 id="Helm组件及架构"><a href="#Helm组件及架构" class="headerlink" title="Helm组件及架构"></a>Helm组件及架构</h2><p>Helm采用客户端&#x2F;服务端架构，有如下组件组成</p><ul><li>Helm CLI是Helm客户端，可以在本地执行</li><li>Tiller是服务器端组件，在Kubernetes集群上运行，并管理Kubernetes应用程序</li><li>Repository是Chart仓库，Helm客户端通过HTTP协议来访问仓库中Chart索引文件和压缩包</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201119095458328.png"                      alt="image-20201119095458328"                ></p><h2 id="Helm-v3变化"><a href="#Helm-v3变化" class="headerlink" title="Helm v3变化"></a>Helm v3变化</h2><p>2019年11月13日，Helm团队发布了Helm v3的第一个稳定版本</p><p>该版本主要变化如下</p><ul><li><p>架构变化</p><ul><li>最明显的变化是Tiller的删除</li><li>V3版本删除Tiller</li><li>relesase可以在不同命名空间重用</li></ul></li></ul><p>V3之前</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118171523403.png"                      alt="image-20201118171523403"                ></p><p> V3版本</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118171956054.png"                      alt="image-20201118171956054"                ></p><h2 id="helm配置"><a href="#helm配置" class="headerlink" title="helm配置"></a>helm配置</h2><p>首先我们需要去 <a class="link"   href="https://helm.sh/docs/intro/quickstart/" >官网下载<i class="fas fa-external-link-alt"></i></a></p><ul><li>第一步，<a class="link"   href="https://github.com/helm/helm/releases" >下载helm<i class="fas fa-external-link-alt"></i></a>安装压缩文件，上传到linux系统中</li><li>第二步，解压helm压缩文件，把解压后的helm目录复制到 usr&#x2F;bin 目录中</li><li>使用命令：helm</li></ul><p>我们都知道yum需要配置yum源，那么helm就就要配置helm源</p><h2 id="helm仓库"><a href="#helm仓库" class="headerlink" title="helm仓库"></a>helm仓库</h2><p>添加仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo add 仓库名  仓库地址 </span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置微软源</span></span><br><span class="line">helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line"><span class="comment"># 配置阿里源</span></span><br><span class="line">helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line"><span class="comment"># 配置google源</span></span><br><span class="line">helm repo add google https://kubernetes-charts.storage.googleapis.com/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><p>然后可以查看我们添加的仓库地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看全部</span></span><br><span class="line">helm repo list</span><br><span class="line"><span class="comment"># 查看某个</span></span><br><span class="line">helm search repo stable</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118195732281.png"                      alt="image-20201118195732281"                ></p><p>或者可以删除我们添加的源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo remove stable</span><br></pre></td></tr></table></figure><h2 id="helm基本命令"><a href="#helm基本命令" class="headerlink" title="helm基本命令"></a>helm基本命令</h2><ul><li>chart install</li><li>chart upgrade</li><li>chart rollback</li></ul><h2 id="使用helm快速部署应用"><a href="#使用helm快速部署应用" class="headerlink" title="使用helm快速部署应用"></a>使用helm快速部署应用</h2><h3 id="使用命令搜索应用"><a href="#使用命令搜索应用" class="headerlink" title="使用命令搜索应用"></a>使用命令搜索应用</h3><p>首先我们使用命令，搜索我们需要安装的应用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜索 weave仓库</span></span><br><span class="line">helm search repo weave</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118200603643.png"                      alt="image-20201118200603643"                ></p><h3 id="根据搜索内容选择安装"><a href="#根据搜索内容选择安装" class="headerlink" title="根据搜索内容选择安装"></a>根据搜索内容选择安装</h3><p>搜索完成后，使用命令进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install ui aliyun/weave-scope</span><br></pre></td></tr></table></figure><p>可以通过下面命令，来下载yaml文件【如果】</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f weave-scope.yaml</span><br></pre></td></tr></table></figure><p>安装完成后，通过下面命令即可查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm list</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118203727585.png"                      alt="image-20201118203727585"                ></p><p>同时可以通过下面命令，查看更新具体的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm status ui</span><br></pre></td></tr></table></figure><p>但是我们通过查看 svc状态，发现没有对象暴露端口</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118205031343.png"                      alt="image-20201118205031343"                ></p><p>所以我们需要修改service的yaml文件，添加NodePort</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit svc ui-weave-scope</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118205129431.png"                      alt="image-20201118205129431"                ></p><p>这样就可以对外暴露端口了</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118205147631.png"                      alt="image-20201118205147631"                ></p><p>然后我们通过 ip + 32185 即可访问</p><h3 id="如果自己创建Chart"><a href="#如果自己创建Chart" class="headerlink" title="如果自己创建Chart"></a>如果自己创建Chart</h3><p>使用命令，自己创建Chart</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm create mychart</span><br></pre></td></tr></table></figure><p>创建完成后，我们就能看到在当前文件夹下，创建了一个 mychart目录</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118210755621.png"                      alt="image-20201118210755621"                ></p><h4 id="目录格式"><a href="#目录格式" class="headerlink" title="目录格式"></a>目录格式</h4><ul><li>templates：编写yaml文件存放到这个目录</li><li>values.yaml：存放的是全局的yaml文件</li><li>chart.yaml：当前chart属性配置信息</li></ul><h3 id="在templates文件夹创建两个文件"><a href="#在templates文件夹创建两个文件" class="headerlink" title="在templates文件夹创建两个文件"></a>在templates文件夹创建两个文件</h3><p>我们创建以下两个</p><ul><li>deployment.yaml</li><li>service.yaml</li></ul><p>我们可以通过下面命令创建出yaml文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导出deployment.yaml</span></span><br><span class="line">kubectl create deployment web1 --image=nginx --dry-run -o yaml &gt; deployment.yaml</span><br><span class="line"><span class="comment"># 导出service.yaml 【可能需要创建 deployment，不然会报错】</span></span><br><span class="line">kubectl expose deployment web1 --port=80 --target-port=80 --<span class="built_in">type</span>=NodePort --dry-run -o yaml &gt; service.yaml</span><br></pre></td></tr></table></figure><h3 id="安装mychart"><a href="#安装mychart" class="headerlink" title="安装mychart"></a>安装mychart</h3><p>执行命令创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install web1 mychart</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118213120916.png"                      alt="image-20201118213120916"                ></p><h3 id="应用升级"><a href="#应用升级" class="headerlink" title="应用升级"></a>应用升级</h3><p>当我们修改了mychart中的东西后，就可以进行升级操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm upgrade web1 mychart</span><br></pre></td></tr></table></figure><h2 id="chart模板使用"><a href="#chart模板使用" class="headerlink" title="chart模板使用"></a>chart模板使用</h2><p>通过传递参数，动态渲染模板，yaml内容动态从传入参数生成</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118213630083.png"                      alt="image-20201118213630083"                ></p><p>刚刚我们创建mychart的时候，看到有values.yaml文件，这个文件就是一些全局的变量，然后在templates中能取到变量的值，下面我们可以利用这个，来完成动态模板</p><ul><li>在values.yaml定义变量和值</li><li>具体yaml文件，获取定义变量值</li><li>yaml文件中大题有几个地方不同<ul><li>image</li><li>tag</li><li>label</li><li>port</li><li>replicas</li></ul></li></ul><h3 id="定义变量和值"><a href="#定义变量和值" class="headerlink" title="定义变量和值"></a>定义变量和值</h3><p>在values.yaml定义变量和值</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118214050899.png"                      alt="image-20201118214050899"                ></p><h3 id="获取变量和值"><a href="#获取变量和值" class="headerlink" title="获取变量和值"></a>获取变量和值</h3><p>我们通过表达式形式 使用全局变量  <code>&#123;&#123;.Values.变量名称&#125;&#125; </code></p><p>例如： <code>&#123;&#123;.Release.Name&#125;&#125;</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118214413203.png"                      alt="image-20201118214413203"                ></p><h3 id="安装应用"><a href="#安装应用" class="headerlink" title="安装应用"></a>安装应用</h3><p>在我们修改完上述的信息后，就可以尝试的创建应用了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install --dry-run web2 mychart</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118214727058.png"                      alt="image-20201118214727058"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Helm就是一个包管理工具【类似于npm】&lt;/p&gt;
&lt;p&gt;&lt;img  
                     lazyload
                     src=&quot;/images/loading.svg&quot;
                     dat</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之使用kubeadm-ha脚本一键安装K8S</title>
    <link href="http://example.com/2022/08/24/k8s%E4%B9%8B%E4%BD%BF%E7%94%A8kubeadm-ha%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85K8S/"/>
    <id>http://example.com/2022/08/24/k8s%E4%B9%8B%E4%BD%BF%E7%94%A8kubeadm-ha%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85K8S/</id>
    <published>2022-08-23T21:17:21.000Z</published>
    <updated>2022-08-25T01:31:00.473Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用kubeadm-ha脚本一键安装K8S"><a href="#使用kubeadm-ha脚本一键安装K8S" class="headerlink" title="使用kubeadm-ha脚本一键安装K8S"></a>使用kubeadm-ha脚本一键安装K8S</h1><blockquote><p>Github地址：<a class="link"   href="https://github.com/TimeBye/kubeadm-ha" >https://github.com/TimeBye/kubeadm-ha<i class="fas fa-external-link-alt"></i></a></p></blockquote><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>官网的安装说明也很简单但是还有些细节还是没有提到，所以我自己照着官网的教程 补充了一些细节</p><h3 id="硬件系统要求"><a href="#硬件系统要求" class="headerlink" title="硬件系统要求"></a>硬件系统要求</h3><ul><li>Master节点：2C4G +</li><li>Worker节点：2C4G +</li></ul><p>使用centos7.7安装请按上面配置准备好3台centos,1台作为Master节点,2台Worker节点</p><p>本方式为1主2worker的配置</p><p>这是我的各个节点的配置</p><table><thead><tr><th>主机名</th><th>ip</th><th>配置</th></tr></thead><tbody><tr><td>k8s-master</td><td>192.168.177.130</td><td>2C4G</td></tr><tr><td>k8s-node1</td><td>192.168.177.131</td><td>2C2G</td></tr><tr><td>k8s-node2</td><td>192.168.177.132</td><td>2C2G</td></tr></tbody></table><h3 id="centos准备"><a href="#centos准备" class="headerlink" title="centos准备"></a>centos准备</h3><p><code>在安装之前需要准备一些基础的软件环境用于下载一键安装k8s的脚本和编辑配置</code></p><h4 id="centos网络准备"><a href="#centos网络准备" class="headerlink" title="centos网络准备"></a>centos网络准备</h4><p>安装时需要连接互联网下载各种软件 所以需要保证每个节点都可以访问外网</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping baidu.com</span><br></pre></td></tr></table></figure><p>建议关闭 <strong>CentOS</strong> 的防火墙</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld  &amp;&amp; systemctl <span class="built_in">disable</span> firewalld &amp;&amp; systemctl status firewalld </span><br></pre></td></tr></table></figure><p>同时需要保证各个节点间可以相互ping通</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping 其他节点ip</span><br></pre></td></tr></table></figure><h4 id="CentOS软件准备"><a href="#CentOS软件准备" class="headerlink" title="CentOS软件准备"></a>CentOS软件准备</h4><p>用 <strong>ssh</strong> 连接到 <strong>Master</strong> 节点上安装 Git</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install git -y</span><br></pre></td></tr></table></figure><h2 id="部署k8s前配置"><a href="#部署k8s前配置" class="headerlink" title="部署k8s前配置"></a>部署k8s前配置</h2><h4 id="下载部署脚本"><a href="#下载部署脚本" class="headerlink" title="下载部署脚本"></a>下载部署脚本</h4><p>在Master节点clone安装脚本 <a class="link"   href="https://github.com/TimeBye/kubeadm-ha" >脚本地址<i class="fas fa-external-link-alt"></i></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth 1 https://github.com/TimeBye/kubeadm-ha</span><br></pre></td></tr></table></figure><p>进入到下载的部署脚本的目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd kubeadm-ha</span><br></pre></td></tr></table></figure><h4 id="安装-Ansible-运行环境"><a href="#安装-Ansible-运行环境" class="headerlink" title="安装 Ansible 运行环境"></a>安装 Ansible 运行环境</h4><p>在master节点安装Ansible环境</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./install-ansible.sh</span><br></pre></td></tr></table></figure><h4 id="修改安装的配置文件"><a href="#修改安装的配置文件" class="headerlink" title="修改安装的配置文件"></a>修改安装的配置文件</h4><p>由于我是一个master两个node的方式构建的centos所以我们需要修改example&#x2F;hosts.s-master.ip.ini 文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi example/hosts.s-master.ip.ini </span><br></pre></td></tr></table></figure><p>具体要修改的就是 ip 和密码 其他的保持默认</p><p>我的hosts.s-master.ip.ini 文件预览</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">; 将所有节点信息在这里填写</span></span><br><span class="line"><span class="comment">;    第一个字段                  为远程服务器内网IP</span></span><br><span class="line"><span class="comment">;    第二个字段 ansible_port     为节点 sshd 监听端口</span></span><br><span class="line"><span class="comment">;    第三个字段 ansible_user     为节点远程登录用户名</span></span><br><span class="line"><span class="comment">;    第四个字段 ansible_ssh_pass 为节点远程登录用户密码</span></span><br><span class="line"><span class="section">[all]</span></span><br><span class="line">192.168.177.130 <span class="attr">ansible_port</span>=<span class="number">22</span> ansible_user=<span class="string">&quot;root&quot;</span> ansible_ssh_pass=<span class="string">&quot;moxi&quot;</span></span><br><span class="line">192.168.177.131 <span class="attr">ansible_port</span>=<span class="number">22</span> ansible_user=<span class="string">&quot;root&quot;</span> ansible_ssh_pass=<span class="string">&quot;moxi&quot;</span></span><br><span class="line">192.168.177.132 <span class="attr">ansible_port</span>=<span class="number">22</span> ansible_user=<span class="string">&quot;root&quot;</span> ansible_ssh_pass=<span class="string">&quot;moxi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 单 master 节点不需要进行负载均衡，lb节点组留空。</span></span><br><span class="line"><span class="section">[lb]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 注意etcd集群必须是1,3,5,7...奇数个节点</span></span><br><span class="line"><span class="section">[etcd]</span></span><br><span class="line">192.168.177.130</span><br><span class="line">192.168.177.131</span><br><span class="line">192.168.177.132</span><br><span class="line"></span><br><span class="line"><span class="section">[kube-master]</span></span><br><span class="line">192.168.177.130</span><br><span class="line"></span><br><span class="line"><span class="section">[kube-worker]</span></span><br><span class="line">192.168.177.130</span><br><span class="line">192.168.177.131</span><br><span class="line">192.168.177.132</span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续添加master节点使用</span></span><br><span class="line"><span class="section">[new-master]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续添加worker节点使用</span></span><br><span class="line"><span class="section">[new-worker]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续添加etcd节点使用</span></span><br><span class="line"><span class="section">[new-etcd]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续删除worker角色使用</span></span><br><span class="line"><span class="section">[del-worker]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续删除master角色使用</span></span><br><span class="line"><span class="section">[del-master]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续删除etcd角色使用</span></span><br><span class="line"><span class="section">[del-etcd]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 预留组，后续删除节点使用</span></span><br><span class="line"><span class="section">[del-node]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">;-------------------------------------- 以下为基础信息配置 ------------------------------------;</span></span><br><span class="line"><span class="section">[all:vars]</span></span><br><span class="line"><span class="comment">; 是否跳过节点物理资源校验，Master节点要求2c2g以上，Worker节点要求2c4g以上</span></span><br><span class="line"><span class="attr">skip_verify_node</span>=<span class="literal">true</span></span><br><span class="line"><span class="comment">; kubernetes版本</span></span><br><span class="line"><span class="attr">kube_version</span>=<span class="string">&quot;1.18.14&quot;</span></span><br><span class="line"><span class="comment">; 负载均衡器</span></span><br><span class="line"><span class="comment">;   有 nginx、openresty、haproxy、envoy  和 slb 可选，默认使用 nginx</span></span><br><span class="line"><span class="comment">;   为什么单 master 集群 apiserver 也使用了负载均衡请参与此讨论： https://github.com/TimeBye/kubeadm-ha/issues/8</span></span><br><span class="line"><span class="attr">lb_mode</span>=<span class="string">&quot;nginx&quot;</span></span><br><span class="line"><span class="comment">; 使用负载均衡后集群 apiserver ip，设置 lb_kube_apiserver_ip 变量，则启用负载均衡器 + keepalived</span></span><br><span class="line"><span class="comment">; lb_kube_apiserver_ip=&quot;192.168.56.15&quot;</span></span><br><span class="line"><span class="comment">; 使用负载均衡后集群 apiserver port</span></span><br><span class="line"><span class="attr">lb_kube_apiserver_port</span>=<span class="string">&quot;8443&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 网段选择：pod 和 service 的网段不能与服务器网段重叠，</span></span><br><span class="line"><span class="comment">; 若有重叠请配置 `kube_pod_subnet` 和 `kube_service_subnet` 变量设置 pod 和 service 的网段，示例参考：</span></span><br><span class="line"><span class="comment">;    如果服务器网段为：10.0.0.1/8</span></span><br><span class="line"><span class="comment">;       pod 网段可设置为：192.168.0.0/18</span></span><br><span class="line"><span class="comment">;       service 网段可设置为 192.168.64.0/18</span></span><br><span class="line"><span class="comment">;    如果服务器网段为：172.16.0.1/12</span></span><br><span class="line"><span class="comment">;       pod 网段可设置为：10.244.0.0/18</span></span><br><span class="line"><span class="comment">;       service 网段可设置为 10.244.64.0/18</span></span><br><span class="line"><span class="comment">;    如果服务器网段为：192.168.0.1/16</span></span><br><span class="line"><span class="comment">;       pod 网段可设置为：10.244.0.0/18</span></span><br><span class="line"><span class="comment">;       service 网段可设置为 10.244.64.0/18</span></span><br><span class="line"><span class="comment">; 集群pod ip段，默认掩码位 18 即 16384 个ip</span></span><br><span class="line"><span class="attr">kube_pod_subnet</span>=<span class="string">&quot;10.244.0.0/18&quot;</span></span><br><span class="line"><span class="comment">; 集群service ip段</span></span><br><span class="line"><span class="attr">kube_service_subnet</span>=<span class="string">&quot;10.244.64.0/18&quot;</span></span><br><span class="line"><span class="comment">; 分配给节点的 pod 子网掩码位，默认为 24 即 256 个ip，故使用这些默认值可以纳管 16384/256=64 个节点。</span></span><br><span class="line"><span class="attr">kube_network_node_prefix</span>=<span class="string">&quot;24&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; node节点最大 pod 数。数量与分配给节点的 pod 子网有关，ip 数应大于 pod 数。</span></span><br><span class="line"><span class="comment">; https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr</span></span><br><span class="line"><span class="attr">kube_max_pods</span>=<span class="string">&quot;110&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 集群网络插件，目前支持flannel,calico</span></span><br><span class="line"><span class="attr">network_plugin</span>=<span class="string">&quot;calico&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">; 若服务器磁盘分为系统盘与数据盘，请修改以下路径至数据盘自定义的目录。</span></span><br><span class="line"><span class="comment">; Kubelet 根目录</span></span><br><span class="line"><span class="attr">kubelet_root_dir</span>=<span class="string">&quot;/var/lib/kubelet&quot;</span></span><br><span class="line"><span class="comment">; docker容器存储目录</span></span><br><span class="line"><span class="attr">docker_storage_dir</span>=<span class="string">&quot;/var/lib/docker&quot;</span></span><br><span class="line"><span class="comment">; Etcd 数据根目录</span></span><br><span class="line"><span class="attr">etcd_data_dir</span>=<span class="string">&quot;/var/lib/etcd&quot;</span></span><br></pre></td></tr></table></figure><h4 id="升级内核"><a href="#升级内核" class="headerlink" title="升级内核"></a>升级内核</h4><p>修改完配置文件后建议升级内核</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i example/hosts.s-master.ip.ini 00-kernel.yml</span><br></pre></td></tr></table></figure><p>内核升级完毕后重启所有节点 在master node1 node2上执行</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><h2 id="开始部署k8s"><a href="#开始部署k8s" class="headerlink" title="开始部署k8s"></a>开始部署k8s</h2><p>等待所有的节点重启完成后进入脚本目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd kubeadm-ha</span><br></pre></td></tr></table></figure><h3 id="执行一键部署命令"><a href="#执行一键部署命令" class="headerlink" title="执行一键部署命令"></a>执行一键部署命令</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i example/hosts.s-master.ip.ini 90-init-cluster.yml</span><br></pre></td></tr></table></figure><h3 id="查看节点运行情况"><a href="#查看节点运行情况" class="headerlink" title="查看节点运行情况"></a>查看节点运行情况</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure><p>等待所有节点ready 即为创建成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAME             STATUS   ROLES                AGE     VERSION</span><br><span class="line">192.168.28.128   Ready    etcd,worker          2m57s   v1.18.14</span><br><span class="line">192.168.28.80    Ready    etcd,master,worker   3m29s   v1.18.14</span><br><span class="line">192.168.28.89    Ready    etcd,worker          2m57s   v1.18.14</span><br></pre></td></tr></table></figure><h3 id="集群重置"><a href="#集群重置" class="headerlink" title="集群重置"></a>集群重置</h3><p>如果部署失败了，想要重置整个集群【包括数据】，执行下面脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i example/hosts.s-master.ip.ini 99-reset-cluster.yml</span><br></pre></td></tr></table></figure><h2 id="部署kuboard"><a href="#部署kuboard" class="headerlink" title="部署kuboard"></a>部署kuboard</h2><h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>因为我们需要拉取镜像，所以需要在服务器提前安装好Docker，首先配置一下Docker的阿里yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;/etc/yum.repos.d/docker.repo&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[docker-ce-edge]</span></span><br><span class="line"><span class="string">name=Docker CE Edge - \$basearch</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/\$basearch/edge</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后yum方式安装docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum安装</span></span><br><span class="line">yum -y install docker-ce</span><br><span class="line"><span class="comment"># 查看docker版本</span></span><br><span class="line">docker --version  </span><br><span class="line"><span class="comment"># 开机自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line"><span class="comment"># 启动docker</span></span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><p>配置docker的镜像源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/docker/daemon.json &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后重启docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h2 id="安装Kuboard【可选】"><a href="#安装Kuboard【可选】" class="headerlink" title="安装Kuboard【可选】"></a>安装Kuboard【可选】</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>Kuboard</strong> 是一款免费的 <strong>Kubernetes</strong> 图形化管理工具，力图帮助用户快速在 <strong>Kubernetes</strong> 上落地微服务。</p><p>Kuboard文档：<a class="link"   href="https://kuboard.cn/" >https://kuboard.cn/<i class="fas fa-external-link-alt"></i></a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>在master节点执行</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml</span><br><span class="line">kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml</span><br></pre></td></tr></table></figure><p>查看 Kuboard 运行状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出结果如下所示。注意：如果是 <code>ContainerCreating</code> 那么需要等待一会</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">kuboard-74c645f5df-cmrbc   1/1     Running   0          80s</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="访问Kuboard"><a href="#访问Kuboard" class="headerlink" title="访问Kuboard"></a>访问Kuboard</h3><p>Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567；您可以按如下方式访问 Kuboard。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 格式</span></span><br><span class="line">http://任意一个Worker节点的IP地址:32567/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例如，我的访问地址如下所示</span></span><br><span class="line">http://192.168.177.130:32567/</span><br></pre></td></tr></table></figure><p>页面如下所示：</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20210107211525789.png"                      alt="image-20210107211525789"                ></p><p>第一次访问需要输入token 我们获取一下 <strong>token</strong>， <code>在master节点执行</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>) -o go-template=<span class="string">&#x27;&#123;&#123;.data.token&#125;&#125;&#x27;</span> | <span class="built_in">base64</span> -d)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>获取到的 <strong>token</strong>，然后粘贴到框中，我的 <strong>token</strong> 格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6ImY1eUZlc0RwUlZha0E3LWZhWXUzUGljNDM3SE0zU0Q4dzd5R3JTdXM2WEUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJvYXJkLXVzZXItdG9rZW4tMmJsamsiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3Vib2FyZC11c2VyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzhlZDRmNDktNzM0Zi00MjU1LTljODUtMWI5MGI4MzU4ZWMzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmt1Ym9hcmQtdXNlciJ9.MujbwGnkL_qa3H14oKDT1zZ5Fzt16pWoaY52nT7fV5B2nNIRsB3Esd18S8ztHUJZLRGxAhBwu-utToi2YBb8pH9RfIeSXMezFZ6QhBbp0n5xYWeYETQYKJmes2FRcW-6jrbpvXlfUuPXqsbRX8qrnmSVEbcAms22CSSVhUbTz1kz8C7b1C4lpSGGuvdpNxgslNFZTFrcImpelpGSaIGEMUk1qdjKMROw8bV83pga4Y41Y6rJYE3hdnCkUA8w2SZOYuF2kT1DuZuKq3A53iLsvJ6Ps-gpli2HcoiB0NkeI_fJORXmYfcj5N2Csw6uGUDiBOr1T4Dto-i8SaApqmdcXg</span><br></pre></td></tr></table></figure><p>最后即可进入 <strong>kuboard</strong> 的 <strong>dashboard</strong> 界面</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20210107211713726.png"                      alt="image-20210107211713726"                ></p><h3 id="卸载Kuboard"><a href="#卸载Kuboard" class="headerlink" title="卸载Kuboard"></a>卸载Kuboard</h3><p>当我们 <strong>kuboard</strong> 不想使用的时候，我们就可以直接卸载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f https://kuboard.cn/install-script/kuboard.yaml</span><br><span class="line">kubectl delete -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml</span><br></pre></td></tr></table></figure><h2 id="Rancher部署【可选】"><a href="#Rancher部署【可选】" class="headerlink" title="Rancher部署【可选】"></a>Rancher部署【可选】</h2><blockquote><p>kuboard和rancher建议部署其中一个</p></blockquote><h3 id="helm安装"><a href="#helm安装" class="headerlink" title="helm安装"></a>helm安装</h3><p>使用helm部署rancher会方便很多，所以需要安装helm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -O http://rancher-mirror.cnrancher.com/helm/v3.2.4/helm-v3.2.4-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf helm-v3.2.4-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">mv</span> linux-amd64/helm /usr/local/bin</span><br></pre></td></tr></table></figure><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm version</span><br></pre></td></tr></table></figure><p>输入以下内容说明helm安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">version.BuildInfo&#123;Version:<span class="string">&quot;v3.2.4&quot;</span>, GitCommit:<span class="string">&quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688&quot;</span>, GitTreeState:<span class="string">&quot;clean&quot;</span>, GoVersion:<span class="string">&quot;go1.13.12&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="添加rancher-chart仓库"><a href="#添加rancher-chart仓库" class="headerlink" title="添加rancher chart仓库"></a>添加rancher chart仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm repo add rancher-stable http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/stable</span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><h3 id="安装rancher"><a href="#安装rancher" class="headerlink" title="安装rancher"></a>安装rancher</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">helm install rancher rancher-stable/rancher \</span><br><span class="line"> --create-namespace\</span><br><span class="line"> --namespace cattle-system \</span><br><span class="line"> --<span class="built_in">set</span> hostname=rancher.local.com</span><br></pre></td></tr></table></figure><h5 id="等待-Rancher-运行："><a href="#等待-Rancher-运行：" class="headerlink" title="等待 Rancher 运行："></a>等待 Rancher 运行：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n cattle-system rollout status deploy/rancher</span><br></pre></td></tr></table></figure><p>输出信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;rancher&quot;</span> rollout to finish: 0 of 3 updated replicas are available...</span><br><span class="line">deployment <span class="string">&quot;rancher&quot;</span> successfully rolled out</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用kubeadm-ha脚本一键安装K8S&quot;&gt;&lt;a href=&quot;#使用kubeadm-ha脚本一键安装K8S&quot; class=&quot;headerlink&quot; title=&quot;使用kubeadm-ha脚本一键安装K8S&quot;&gt;&lt;/a&gt;使用kubeadm-ha脚本一键安装K8S&lt;/</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之核心技术Ingress</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFIngress/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFIngress/</id>
    <published>2022-08-23T08:30:37.000Z</published>
    <updated>2022-08-25T01:24:17.195Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>原来我们需要将端口号对外暴露，通过 ip + 端口号就可以进行访问</p><p>原来是使用Service中的NodePort来实现</p><ul><li>在每个节点上都会启动端口</li><li>在访问的时候通过任何节点，通过ip + 端口号就能实现访问</li></ul><p>但是NodePort还存在一些缺陷</p><ul><li>因为端口不能重复，所以每个端口只能使用一次，一个端口对应一个应用</li><li>实际访问中都是用域名，根据不同域名跳转到不同端口服务中</li></ul><h2 id="Ingress和Pod关系"><a href="#Ingress和Pod关系" class="headerlink" title="Ingress和Pod关系"></a>Ingress和Pod关系</h2><p>pod 和 ingress 是通过service进行关联的，而ingress作为统一入口，由service关联一组pod中</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118102637839.png"                      alt="image-20201118102637839"                ></p><ul><li>首先service就是关联我们的pod</li><li>然后ingress作为入口，首先需要到service，然后发现一组pod</li><li>发现pod后，就可以做负载均衡等操作</li></ul><h2 id="Ingress工作流程"><a href="#Ingress工作流程" class="headerlink" title="Ingress工作流程"></a>Ingress工作流程</h2><p>在实际的访问中，我们都是需要维护很多域名， a.com  和  b.com</p><p>然后不同的域名对应的不同的Service，然后service管理不同的pod</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118102858617.png"                      alt="image-20201118102858617"                ></p><p>需要注意，ingress不是内置的组件，需要我们单独的安装</p><h2 id="使用Ingress"><a href="#使用Ingress" class="headerlink" title="使用Ingress"></a>使用Ingress</h2><p>步骤如下所示</p><ul><li>部署ingress Controller【需要下载官方的】</li><li>创建ingress规则【对哪个Pod、名称空间配置规则】</li></ul><h3 id="创建Nginx-Pod"><a href="#创建Nginx-Pod" class="headerlink" title="创建Nginx Pod"></a>创建Nginx Pod</h3><p>创建一个nginx应用，然后对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pod</span></span><br><span class="line">kubectl create deployment web --image=nginx</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p>对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --target-port=80 --<span class="built_in">type</span>:NodePort</span><br></pre></td></tr></table></figure><h3 id="部署-ingress-controller"><a href="#部署-ingress-controller" class="headerlink" title="部署 ingress controller"></a>部署 ingress controller</h3><p>下面我们来通过yaml的方式，部署我们的ingress，配置文件如下所示</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118105427248.png"                      alt="image-20201118105427248"                ></p><p>这个文件里面，需要注意的是 hostNetwork: true，改成ture是为了让后面访问到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ingress-con.yaml</span><br></pre></td></tr></table></figure><p>通过这种方式，其实我们在外面就能访问，这里还需要在外面添加一层</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ingress-con.yaml</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118111256631.png"                      alt="image-20201118111256631"                ></p><p>最后通过下面命令，查看是否成功部署 ingress</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n ingress-nginx</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118111424735.png"                      alt="image-20201118111424735"                ></p><h3 id="创建ingress规则文件"><a href="#创建ingress规则文件" class="headerlink" title="创建ingress规则文件"></a>创建ingress规则文件</h3><p>创建ingress规则文件，ingress-h.yaml</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118111700534.png"                      alt="image-20201118111700534"                ></p><h3 id="添加域名访问规则"><a href="#添加域名访问规则" class="headerlink" title="添加域名访问规则"></a>添加域名访问规则</h3><p>在windows 的 hosts文件，添加域名访问规则【因为我们没有域名解析，所以只能这样做】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118112029820.png"                      alt="image-20201118112029820"                ></p><p>最后通过域名就能访问</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118112212519.png"                      alt="image-20201118112212519"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;原来我们需要将端口号对外暴露，通过 ip + 端口号就可以进行访问&lt;/p&gt;
&lt;p&gt;原来是使用Service中的NodePort来实现&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之集群安全机制</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6/</id>
    <published>2022-08-23T07:55:34.000Z</published>
    <updated>2022-08-25T01:23:17.487Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>当我们访问K8S集群时，需要经过三个步骤完成具体操作</p><ul><li>认证</li><li>鉴权【授权】</li><li>准入控制</li></ul><p>进行访问的时候，都需要经过 apiserver， apiserver做统一协调，比如门卫</p><ul><li>访问过程中，需要证书、token、或者用户名和密码</li><li>如果访问pod需要serviceAccount</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118092356107.png"                      alt="image-20201118092356107"                ></p><h3 id="认证"><a href="#认证" class="headerlink" title="认证"></a>认证</h3><p>对外不暴露8080端口，只能内部访问，对外使用的端口6443</p><p>客户端身份认证常用方式</p><ul><li>https证书认证，基于ca证书</li><li>http token认证，通过token来识别用户</li><li>http基本认证，用户名 + 密码认证</li></ul><h3 id="鉴权"><a href="#鉴权" class="headerlink" title="鉴权"></a>鉴权</h3><p>基于RBAC进行鉴权操作</p><p>基于角色访问控制</p><h3 id="准入控制"><a href="#准入控制" class="headerlink" title="准入控制"></a>准入控制</h3><p>就是准入控制器的列表，如果列表有请求内容就通过，没有的话 就拒绝</p><h2 id="RBAC介绍"><a href="#RBAC介绍" class="headerlink" title="RBAC介绍"></a>RBAC介绍</h2><p>基于角色的访问控制，为某个角色设置访问内容，然后用户分配该角色后，就拥有该角色的访问权限</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118093949893.png"                      alt="image-20201118093949893"                ></p><p>k8s中有默认的几个角色</p><ul><li>role：特定命名空间访问权限</li><li>ClusterRole：所有命名空间的访问权限</li></ul><p>角色绑定</p><ul><li>roleBinding：角色绑定到主体</li><li>ClusterRoleBinding：集群角色绑定到主体</li></ul><p>主体</p><ul><li>user：用户</li><li>group：用户组</li><li>serviceAccount：服务账号</li></ul><h2 id="RBAC实现鉴权"><a href="#RBAC实现鉴权" class="headerlink" title="RBAC实现鉴权"></a>RBAC实现鉴权</h2><ul><li>创建命名空间</li></ul><h3 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h3><p>我们可以首先查看已经存在的命名空间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get namespace</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118094516426.png"                      alt="image-20201118094516426"                ></p><p>然后我们创建一个自己的命名空间  roledemo</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create ns roledemo</span><br></pre></td></tr></table></figure><h3 id="命名空间创建Pod"><a href="#命名空间创建Pod" class="headerlink" title="命名空间创建Pod"></a>命名空间创建Pod</h3><p>为什么要创建命名空间？因为如果不创建命名空间的话，默认是在default下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx -n roledemo</span><br></pre></td></tr></table></figure><h3 id="创建角色"><a href="#创建角色" class="headerlink" title="创建角色"></a>创建角色</h3><p>我们通过 rbac-role.yaml进行创建</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118094851338.png"                      alt="image-20201118094851338"                ></p><p>tip：这个角色只对pod 有 get、list权限</p><p>然后通过 yaml创建我们的role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">kubectl apply -f rbac-role.yaml</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get role -n roledemo</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118095141786.png"                      alt="image-20201118095141786"                ></p><h3 id="创建角色绑定"><a href="#创建角色绑定" class="headerlink" title="创建角色绑定"></a>创建角色绑定</h3><p>我们还是通过 role-rolebinding.yaml 的方式，来创建我们的角色绑定</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118095248052.png"                      alt="image-20201118095248052"                ></p><p>然后创建我们的角色绑定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建角色绑定</span></span><br><span class="line">kubectl apply -f rbac-rolebinding.yaml</span><br><span class="line"><span class="comment"># 查看角色绑定</span></span><br><span class="line">kubectl get role, rolebinding -n roledemo</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118095357067.png"                      alt="image-20201118095357067"                ></p><h3 id="使用证书识别身份"><a href="#使用证书识别身份" class="headerlink" title="使用证书识别身份"></a>使用证书识别身份</h3><p>我们首先得有一个 rbac-user.sh 证书脚本</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118095541427.png"                      alt="image-20201118095541427"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118095627954.png"                      alt="image-20201118095627954"                ></p><p>这里包含了很多证书文件，在TSL目录下，需要复制过来</p><p>通过下面命令执行我们的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./rbac-user.sh</span><br></pre></td></tr></table></figure><p>最后我们进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用get命令查看 pod 【有权限】</span></span><br><span class="line">kubectl get pods -n roledemo</span><br><span class="line"><span class="comment"># 用get命令查看svc 【没权限】</span></span><br><span class="line">kubectl get svc -n roledmeo</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118100051043.png"                      alt="image-20201118100051043"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;当我们访问K8S集群时，需要经过三个步骤完成具体操作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;认证&lt;/li&gt;
&lt;li&gt;鉴权【授权】&lt;/li&gt;
&lt;li&gt;准入</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之配置管理</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/</id>
    <published>2022-08-23T07:08:46.000Z</published>
    <updated>2022-08-25T01:22:37.228Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h2><p>Secret的主要作用就是加密数据，然后存在etcd里面，让Pod容器以挂载Volume方式进行访问</p><p>场景：用户名 和 密码进行加密</p><p>一般场景的是对某个字符串进行base64编码 进行加密</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">&#x27;admin&#x27;</span> | <span class="built_in">base64</span></span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117212037668.png"                      alt="image-20201117212037668"                ></p><h3 id="变量形式挂载到Pod"><a href="#变量形式挂载到Pod" class="headerlink" title="变量形式挂载到Pod"></a>变量形式挂载到Pod</h3><ul><li>创建secret加密数据的yaml文件    secret.yaml</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117212124476.png"                      alt="image-20201117212124476"                ></p><p>然后使用下面命令创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f secret.yaml</span><br></pre></td></tr></table></figure><p>通过get命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118084010980.png"                      alt="image-20201118084010980"                ></p><p>然后我们通过下面的命令，进入到我们的容器内部</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it mypod bash</span><br></pre></td></tr></table></figure><p>然后我们就可以输出我们的值，这就是以变量的形式挂载到我们的容器中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出用户</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$SECRET_USERNAME</span></span><br><span class="line"><span class="comment"># 输出密码</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$SECRET_PASSWORD</span></span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118084137942.png"                      alt="image-20201118084137942"                ></p><p>最后如果我们要删除这个Pod，就可以使用这个命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f secret-val.yaml</span><br></pre></td></tr></table></figure><h3 id="数据卷形式挂载"><a href="#数据卷形式挂载" class="headerlink" title="数据卷形式挂载"></a>数据卷形式挂载</h3><p>首先我们创建一个 secret-val.yaml 文件</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118084321590.png"                      alt="image-20201118084321590"                ></p><p>然后创建我们的 Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据配置创建容器</span></span><br><span class="line">kubectl apply -f secret-val.yaml</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it mypod bash</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line"><span class="built_in">ls</span> /etc/foo</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118084707478.png"                      alt="image-20201118084707478"                ></p><h2 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h2><p>ConfigMap作用是存储不加密的数据到etcd中，让Pod以变量或数据卷Volume挂载到容器中</p><p>应用场景：配置文件</p><h3 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"></a>创建配置文件</h3><p>首先我们需要创建一个配置文件 <code>redis.properties</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis.port=127.0.0.1</span><br><span class="line">redis.port=6379</span><br><span class="line">redis.password=123456</span><br></pre></td></tr></table></figure><h3 id="创建ConfigMap"><a href="#创建ConfigMap" class="headerlink" title="创建ConfigMap"></a>创建ConfigMap</h3><p>我们使用命令创建configmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap redis-config --from-file=redis.properties</span><br></pre></td></tr></table></figure><p>然后查看详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe cm redis-config</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118085503534.png"                      alt="image-20201118085503534"                ></p><h3 id="Volume数据卷形式挂载"><a href="#Volume数据卷形式挂载" class="headerlink" title="Volume数据卷形式挂载"></a>Volume数据卷形式挂载</h3><p>首先我们需要创建一个 <code>cm.yaml</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118085847424.png"                      alt="image-20201118085847424"                ></p><p>然后使用该yaml创建我们的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">kubectl apply -f cm.yaml</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118090634869.png"                      alt="image-20201118090634869"                ></p><p>最后我们通过命令就可以查看结果输出了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs mypod</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118090712780.png"                      alt="image-20201118090712780"                ></p><h3 id="以变量的形式挂载Pod"><a href="#以变量的形式挂载Pod" class="headerlink" title="以变量的形式挂载Pod"></a>以变量的形式挂载Pod</h3><p>首先我们也有一个 myconfig.yaml文件，声明变量信息，然后以configmap创建</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118090911260.png"                      alt="image-20201118090911260"                ></p><p>然后我们就可以创建我们的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pod</span></span><br><span class="line">kubectl apply -f myconfig.yaml</span><br><span class="line"><span class="comment"># 获取</span></span><br><span class="line">kubectl get cm</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118091042287.png"                      alt="image-20201118091042287"                ></p><p>然后我们创建完该pod后，我们就需要在创建一个  config-var.yaml 来使用我们的配置信息</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118091249520.png"                      alt="image-20201118091249520"                ></p><p>最后我们查看输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs mypod</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201118091448252.png"                      alt="image-20201118091448252"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Secret&quot;&gt;&lt;a href=&quot;#Secret&quot; class=&quot;headerlink&quot; title=&quot;Secret&quot;&gt;&lt;/a&gt;Secret&lt;/h2&gt;&lt;p&gt;Secret的主要作用就是加密数据，然后存在etcd里面，让Pod容器以挂载Volume方式进行访问&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之控制器Controller详解</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E6%8E%A7%E5%88%B6%E5%99%A8Controller%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E6%8E%A7%E5%88%B6%E5%99%A8Controller%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-08-23T03:23:05.000Z</published>
    <updated>2022-08-25T01:15:40.919Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Statefulset"><a href="#Statefulset" class="headerlink" title="Statefulset"></a>Statefulset</h2><p>Statefulset主要是用来部署有状态应用</p><p>对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。</p><h3 id="无状态应用"><a href="#无状态应用" class="headerlink" title="无状态应用"></a>无状态应用</h3><p>我们原来使用 deployment，部署的都是无状态的应用，那什么是无状态应用？</p><ul><li>认为Pod都是一样的</li><li>没有顺序要求</li><li>不考虑应用在哪个node上运行</li><li>能够进行随意伸缩和扩展</li></ul><h3 id="有状态应用"><a href="#有状态应用" class="headerlink" title="有状态应用"></a>有状态应用</h3><p>上述的因素都需要考虑到</p><ul><li>让每个Pod独立的</li><li>让每个Pod独立的，保持Pod启动顺序和唯一性</li><li>唯一的网络标识符，持久存储</li><li>有序，比如mysql中的主从</li></ul><p>适合StatefulSet的业务包括数据库服务MySQL 和 PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务</p><p>StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。</p><p>使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供<br>高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。</p><h3 id="部署有状态应用"><a href="#部署有状态应用" class="headerlink" title="部署有状态应用"></a>部署有状态应用</h3><p>无头service， ClusterIp：none</p><p>这里就需要使用 StatefulSet部署有状态应用</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117202950336.png"                      alt="image-20201117202950336"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117203130867.png"                      alt="image-20201117203130867"                ></p><p>然后通过查看pod，能否发现每个pod都有唯一的名称</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117203217016.png"                      alt="image-20201117203217016"                ></p><p>然后我们在查看service，发现是无头的service</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117203245641.png"                      alt="image-20201117203245641"                ></p><p>这里有状态的约定，肯定不是简简单单通过名称来进行约定，而是更加复杂的操作</p><ul><li>deployment：是有身份的，有唯一标识</li><li>statefulset：根据主机名 + 按照一定规则生成域名</li></ul><p>每个pod有唯一的主机名，并且有唯一的域名</p><ul><li>格式：主机名称.service名称.名称空间.svc.cluster.local</li><li>举例：nginx-statefulset-0.default.svc.cluster.local</li></ul><h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2><p>DaemonSet 即后台支撑型服务，主要是用来部署守护进程</p><p>长期伺服型和批处理型的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类的Pod运行；而后台支撑型服务的核心关注点在K8S集群中的节点(物理机或虚拟机)，要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点，也可能是通过 nodeSelector选定的一些特定节点。典型的后台支撑型服务包括：存储、日志和监控等。在每个节点上支撑K8S集群运行的服务。</p><p>守护进程在我们每个节点上，运行的是同一个pod，新加入的节点也同样运行在同一个pod里面</p><ul><li>例子：在每个node节点安装数据采集工具</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117204430836.png"                      alt="image-20201117204430836"                ></p><p>这里是不是一个FileBeat镜像，主要是为了做日志采集工作</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117204810350.png"                      alt="image-20201117204810350"                ></p><p>进入某个 Pod里面，进入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it ds-test-cbk6v bash</span><br></pre></td></tr></table></figure><p>通过该命令后，我们就能看到我们内部收集的日志信息了</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117204912838.png"                      alt="image-20201117204912838"                ></p><h2 id="Job和CronJob"><a href="#Job和CronJob" class="headerlink" title="Job和CronJob"></a>Job和CronJob</h2><p>一次性任务 和 定时任务</p><ul><li>一次性任务：一次性执行完就结束</li><li>定时任务：周期性执行</li></ul><p>Job是K8S中用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别就是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的 spec.completions 策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功行任务保证有N个任务全部成功；工作队列性任务根据应用确定的全局成功而标志成功。</p><h3 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h3><p>Job也即一次性任务</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117205635945.png"                      alt="image-20201117205635945"                ></p><p>使用下面命令，能够看到目前已经存在的Job</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get <span class="built_in">jobs</span></span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117205948374.png"                      alt="image-20201117205948374"                ></p><p>在计算完成后，通过命令查看，能够发现该任务已经完成</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210031725.png"                      alt="image-20201117210031725"                ></p><p>我们可以通过查看日志，查看到一次性任务的结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs pi-qpqff</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210110343.png"                      alt="image-20201117210110343"                ></p><h3 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h3><p>定时任务，cronjob.yaml如下所示</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210309069.png"                      alt="image-20201117210309069"                ></p><p>这里面的命令就是每个一段时间，这里是通过 cron 表达式配置的，通过 schedule字段</p><p>然后下面命令就是每个一段时间输出 </p><p>我们首先用上述的配置文件，创建一个定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f cronjob.yaml</span><br></pre></td></tr></table></figure><p>创建完成后，我们就可以通过下面命令查看定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get cronjobs</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210611783.png"                      alt="image-20201117210611783"                ></p><p>我们可以通过日志进行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs hello-1599100140-wkn79</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210722556.png"                      alt="image-20201117210722556"                ></p><p>然后每次执行，就会多出一个 pod</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117210751068.png"                      alt="image-20201117210751068"                ></p><h2 id="删除svc-和-statefulset"><a href="#删除svc-和-statefulset" class="headerlink" title="删除svc 和 statefulset"></a>删除svc 和 statefulset</h2><p>使用下面命令，可以删除我们添加的svc 和 statefulset</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete svc web</span><br><span class="line"></span><br><span class="line">kubectl delete statefulset --all</span><br></pre></td></tr></table></figure><h2 id="Replication-Controller"><a href="#Replication-Controller" class="headerlink" title="Replication Controller"></a>Replication Controller</h2><p>Replication Controller 简称 <strong>RC</strong>，是K8S中的复制控制器。RC是K8S集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。</p><p>即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有一个Pod在运行。RC是K8S中较早期的技术概念，只适用于长期伺服型的业务类型，比如控制Pod提供高可用的Web服务。</p><h3 id="Replica-Set"><a href="#Replica-Set" class="headerlink" title="Replica Set"></a>Replica Set</h3><p>Replica Set 检查 RS，也就是副本集。RS是新一代的RC，提供同样高可用能力，区别主要在于RS后来居上，能够支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数来使用</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Statefulset&quot;&gt;&lt;a href=&quot;#Statefulset&quot; class=&quot;headerlink&quot; title=&quot;Statefulset&quot;&gt;&lt;/a&gt;Statefulset&lt;/h2&gt;&lt;p&gt;Statefulset主要是用来部署有状态应用&lt;/p&gt;
&lt;p&gt;对于S</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之核心技术Service</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFService/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFService/</id>
    <published>2022-08-23T01:57:16.000Z</published>
    <updated>2022-08-25T01:14:43.454Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面我们了解到 Deployment 只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。</p><p>要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的后端服务实例。在K8S集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。</p><p>在K8S集群中，微服务的负载均衡是由kube-proxy实现的。kube-proxy是k8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8S的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。</p><h2 id="Service存在的意义"><a href="#Service存在的意义" class="headerlink" title="Service存在的意义"></a>Service存在的意义</h2><h3 id="防止Pod失联【服务发现】"><a href="#防止Pod失联【服务发现】" class="headerlink" title="防止Pod失联【服务发现】"></a>防止Pod失联【服务发现】</h3><p>因为Pod每次创建都对应一个IP地址，而这个IP地址是短暂的，每次随着Pod的更新都会变化，假设当我们的前端页面有多个Pod时候，同时后端也多个Pod，这个时候，他们之间的相互访问，就需要通过注册中心，拿到Pod的IP地址，然后去访问对应的Pod</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117093606710.png"                      alt="image-20201117093606710"                ></p><h3 id="定义Pod访问策略【负载均衡】"><a href="#定义Pod访问策略【负载均衡】" class="headerlink" title="定义Pod访问策略【负载均衡】"></a>定义Pod访问策略【负载均衡】</h3><p>页面前端的Pod访问到后端的Pod，中间会通过Service一层，而Service在这里还能做负载均衡，负载均衡的策略有很多种实现策略，例如：</p><ul><li>随机</li><li>轮询</li><li>响应比</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117093902459.png"                      alt="image-20201117093902459"                ></p><h2 id="Pod和Service的关系"><a href="#Pod和Service的关系" class="headerlink" title="Pod和Service的关系"></a>Pod和Service的关系</h2><p>这里Pod 和 Service 之间还是根据 label 和 selector 建立关联的 【和Controller一样】</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117094142491.png"                      alt="image-20201117094142491"                ></p><p>我们在访问service的时候，其实也是需要有一个ip地址，这个ip肯定不是pod的ip地址，而是 虚拟IP <code>vip</code> </p><h2 id="Service常用类型"><a href="#Service常用类型" class="headerlink" title="Service常用类型"></a>Service常用类型</h2><p>Service常用类型有三种</p><ul><li>ClusterIp：集群内部访问</li><li>NodePort：对外访问应用使用</li><li>LoadBalancer：对外访问应用使用，公有云</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>我们可以导出一个文件 包含service的配置信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --target-port=80 --dry-run -o yaml &gt; service.yaml</span><br></pre></td></tr></table></figure><p>service.yaml 如下所示</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>如果我们没有做设置的话，默认使用的是第一种方式 ClusterIp，也就是只能在集群内部使用，我们可以添加一个type字段，用来设置我们的service类型</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>修改完命令后，我们使用创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f service.yaml</span><br></pre></td></tr></table></figure><p>然后能够看到，已经成功修改为 NodePort类型了，最后剩下的一种方式就是LoadBalanced：对外访问应用使用公有云</p><p>node一般是在内网进行部署，而外网一般是不能访问到的，那么如何访问的呢？</p><ul><li>找到一台可以通过外网访问机器，安装nginx，反向代理</li><li>手动把可以访问的节点添加到nginx中</li></ul><p>如果我们使用LoadBalancer，就会有负载均衡的控制器，类似于nginx的功能，就不需要自己添加到nginx上</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;前面我们了解到 Deployment 只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之核心技术Controller</title>
    <link href="http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFController/"/>
    <id>http://example.com/2022/08/23/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFController/</id>
    <published>2022-08-23T01:28:56.000Z</published>
    <updated>2022-08-25T01:13:10.511Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li>什么是Controller</li><li>Pod和Controller的关系</li><li>Deployment控制器应用场景</li><li>yaml文件字段说明</li><li>Deployment控制器部署应用</li><li>升级回滚</li><li>弹性伸缩</li></ul><h2 id="什么是Controller"><a href="#什么是Controller" class="headerlink" title="什么是Controller"></a>什么是Controller</h2><p>Controller是在集群上管理和运行容器的对象，Controller是实际存在的，Pod是虚拟机的</p><h2 id="Pod和Controller的关系"><a href="#Pod和Controller的关系" class="headerlink" title="Pod和Controller的关系"></a>Pod和Controller的关系</h2><p>Pod是通过Controller实现应用的运维，比如弹性伸缩，滚动升级等</p><p>Pod 和 Controller之间是通过label标签来建立关系，同时Controller又被称为控制器工作负载</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116092431237.png"                      alt="image-20201116092431237"                ></p><h2 id="Deployment控制器应用"><a href="#Deployment控制器应用" class="headerlink" title="Deployment控制器应用"></a>Deployment控制器应用</h2><ul><li>Deployment控制器可以部署无状态应用</li><li>管理Pod和ReplicaSet</li><li>部署，滚动升级等功能</li><li>应用场景：web服务，微服务</li></ul><p>Deployment表示用户对K8S集群的一次更新操作。Deployment是一个比RS( Replica Set, RS) 应用模型更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧RS中的副本数减少到0的复合操作。</p><p>这样一个复合操作用一个RS是不好描述的，所以用一个更通用的Deployment来描述。以K8S的发展方向，未来对所有长期伺服型的业务的管理，都会通过Deployment来管理。</p><h2 id="Deployment部署应用"><a href="#Deployment部署应用" class="headerlink" title="Deployment部署应用"></a>Deployment部署应用</h2><p>之前我们也使用Deployment部署过应用，如下代码所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectrl create deployment web --image=nginx</span><br></pre></td></tr></table></figure><p>但是上述代码不是很好的进行复用，因为每次我们都需要重新输入代码，所以我们都是通过YAML进行配置</p><p>但是我们可以尝试使用上面的代码创建一个镜像【只是尝试，不会创建】</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx --dry-run -o yaml &gt; nginx.yaml</span><br></pre></td></tr></table></figure><p>然后输出一个yaml配置文件 <code>nginx.yml</code> ，配置文件如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116093638951.png"                      alt="image-20201116093638951"                ></p><h3 id="使用YAML创建Pod"><a href="#使用YAML创建Pod" class="headerlink" title="使用YAML创建Pod"></a>使用YAML创建Pod</h3><p>通过刚刚的代码，我们已经生成了YAML文件，下面我们就可以使用该配置文件快速创建Pod镜像了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx.yaml</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116094046007.png"                      alt="image-20201116094046007"                ></p><p>但是因为这个方式创建的，我们只能在集群内部进行访问，所以我们还需要对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --<span class="built_in">type</span>=NodePort --target-port=80 --name=web1</span><br></pre></td></tr></table></figure><p>关于上述命令，有几个参数</p><ul><li>–port：就是我们内部的端口号</li><li>–target-port：就是暴露外面访问的端口号</li><li>–name：名称</li><li>–type：类型</li></ul><p>同理，我们一样可以导出对应的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --<span class="built_in">type</span>=NodePort --target-port=80 --name=web1 -o yaml &gt; web1.yaml</span><br></pre></td></tr></table></figure><p>得到的web1.yaml如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">&quot;2020-11-16T02:26:53Z&quot;</span></span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:metadata:</span><br><span class="line">        f:labels:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          f:app: &#123;&#125;</span><br><span class="line">      f:spec:</span><br><span class="line">        f:externalTrafficPolicy: &#123;&#125;</span><br><span class="line">        f:ports:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          k:&#123;<span class="string">&quot;port&quot;</span>:80,<span class="string">&quot;protocol&quot;</span>:<span class="string">&quot;TCP&quot;</span>&#125;:</span><br><span class="line">            .: &#123;&#125;</span><br><span class="line">            f:port: &#123;&#125;</span><br><span class="line">            f:protocol: &#123;&#125;</span><br><span class="line">            f:targetPort: &#123;&#125;</span><br><span class="line">        f:selector:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          f:app: &#123;&#125;</span><br><span class="line">        f:sessionAffinity: &#123;&#125;</span><br><span class="line">        f:<span class="built_in">type</span>: &#123;&#125;</span><br><span class="line">    manager: kubectl</span><br><span class="line">    operation: Update</span><br><span class="line">    time: <span class="string">&quot;2020-11-16T02:26:53Z&quot;</span></span><br><span class="line">  name: web2</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">&quot;113693&quot;</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/services/web2</span><br><span class="line">  uid: d570437d-a6b4-4456-8dfb-950f09534516</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 10.104.174.145</span><br><span class="line">  externalTrafficPolicy: Cluster</span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 32639</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure><p>然后我们可以通过下面的命令来查看对外暴露的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods,svc</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116104021357.png"                      alt="image-20201116104021357"                ></p><p>然后我们访问对应的url，即可看到 nginx了 <code>http://192.168.177.130:32639/</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116104131968.png"                      alt="image-20201116104131968"                ></p><h2 id="升级回滚和弹性伸缩"><a href="#升级回滚和弹性伸缩" class="headerlink" title="升级回滚和弹性伸缩"></a>升级回滚和弹性伸缩</h2><ul><li>升级：  假设从版本为1.14 升级到 1.15 ，这就叫应用的升级【升级可以保证服务不中断】</li><li>回滚：从版本1.15 变成 1.14，这就叫应用的回滚</li><li>弹性伸缩：我们根据不同的业务场景，来改变Pod的数量对外提供服务，这就是弹性伸缩</li></ul><h3 id="应用升级和回滚"><a href="#应用升级和回滚" class="headerlink" title="应用升级和回滚"></a>应用升级和回滚</h3><p>首先我们先创建一个 1.14版本的Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.14</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure><p>我们先指定版本为1.14，然后开始创建我们的Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx.yaml</span><br></pre></td></tr></table></figure><p>同时，我们使用docker images命令，就能看到我们成功拉取到了一个 1.14版本的镜像</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/23/IAsYKHfiPc43BD8.png"                      alt="image-20201116105710966"                ></p><p>我们使用下面的命令，可以将nginx从 1.14 升级到 1.15</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image deployment web nginx=nginx:1.15</span><br></pre></td></tr></table></figure><p>在我们执行完命令后，能看到升级的过程</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/23/IAsYKHfiPc43BD8.png"                                     ></p><ul><li>首先是开始的nginx 1.14版本的Pod在运行，然后 1.15版本的在创建</li><li>然后在1.15版本创建完成后，就会暂停1.14版本</li><li>最后把1.14版本的Pod移除，完成我们的升级</li></ul><p>我们在下载 1.15版本，容器就处于ContainerCreating状态，然后下载完成后，就用 1.15版本去替换1.14版本了，这么做的好处就是：升级可以保证服务不中断</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116111614085.png"                      alt="image-20201116111614085"                ></p><p>我们到我们的node2节点上，查看我们的 docker images;</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116111315000.png"                      alt="image-20201116111315000"                ></p><p>能够看到，我们已经成功拉取到了 1.15版本的nginx了</p><h4 id="查看升级状态"><a href="#查看升级状态" class="headerlink" title="查看升级状态"></a>查看升级状态</h4><p>下面可以，查看升级状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout status deployment web</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/23/gtfxZRDHXB6vLpW.png"                      alt="image-20201116112139645"                ></p><h4 id="查看历史版本"><a href="#查看历史版本" class="headerlink" title="查看历史版本"></a>查看历史版本</h4><p>我们还可以查看历史版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment web</span><br></pre></td></tr></table></figure><h4 id="应用回滚"><a href="#应用回滚" class="headerlink" title="应用回滚"></a>应用回滚</h4><p>我们可以使用下面命令，完成回滚操作，也就是回滚到上一个版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment web</span><br></pre></td></tr></table></figure><p>然后我们就可以查看状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout status deployment web</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201116112524601.png"                      alt="image-20201116112524601"                ></p><p>同时我们还可以回滚到指定版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment web --to-revision=2</span><br></pre></td></tr></table></figure><h3 id="弹性伸缩"><a href="#弹性伸缩" class="headerlink" title="弹性伸缩"></a>弹性伸缩</h3><p>弹性伸缩，也就是我们通过命令一下创建多个副本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment web --replicas=10</span><br></pre></td></tr></table></figure><p>能够清晰看到，我们一下创建了10个副本</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201117092841865.png"                      alt="image-20201117092841865"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;内容&quot;&gt;&lt;a href=&quot;#内容&quot; class=&quot;headerlink&quot; title=&quot;内容&quot;&gt;&lt;/a&gt;内容&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;什么是Controller&lt;/li&gt;
&lt;li&gt;Pod和Controller的关系&lt;/li&gt;
&lt;li&gt;Deployment控制器应</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之核心技术Pod</title>
    <link href="http://example.com/2022/08/22/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFPod/"/>
    <id>http://example.com/2022/08/22/k8s%E4%B9%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFPod/</id>
    <published>2022-08-22T09:18:52.000Z</published>
    <updated>2022-08-25T01:11:34.189Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Pod概述"><a href="#Pod概述" class="headerlink" title="Pod概述"></a>Pod概述</h2><p>Pod是K8S系统中可以创建和管理的最小单元，是资源对象模型中由用户创建或部署的最小资源对象模型，也是在K8S上运行容器化应用的资源对象，其它的资源对象都是用来支撑或者扩展Pod对象功能的，比如控制器对象是用来管控Pod对象的，Service或者Ingress资源对象是用来暴露Pod引用对象的，PersistentVolume资源对象是用来为Pod提供存储等等，K8S不会直接处理容器，而是Pod，Pod是由一个或多个container组成。</p><p>Pod是Kubernetes的最重要概念，每一个Pod都有一个特殊的被称为 “根容器”的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114185528215.png"                      alt="image-20201114185528215"                ></p><h3 id="Pod基本概念"><a href="#Pod基本概念" class="headerlink" title="Pod基本概念"></a>Pod基本概念</h3><ul><li>最小部署的单元</li><li>Pod里面是由一个或多个容器组成【一组容器的集合】</li><li>一个pod中的容器是共享网络命名空间</li><li>Pod是短暂的</li><li>每个Pod包含一个或多个紧密相关的用户业务容器</li></ul><h3 id="Pod存在的意义"><a href="#Pod存在的意义" class="headerlink" title="Pod存在的意义"></a>Pod存在的意义</h3><ul><li>创建容器使用docker，一个docker对应一个容器，一个容器运行一个应用进程</li><li>Pod是多进程设计，运用多个应用程序，也就是一个Pod里面有多个容器，而一个容器里面运行一个应用程序</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114190018948.png"                      alt="image-20201114190018948"                ></p><ul><li>Pod的存在是为了亲密性应用<ul><li>两个应用之间进行交互</li><li>网络之间的调用【通过127.0.0.1 或 socket】</li><li>两个应用之间需要频繁调用</li></ul></li></ul><p>Pod是在K8S集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。同时Pod对多容器的支持是K8S中最基础的设计理念。在生产环境中，通常是由不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。</p><p>Pod是K8S集群中所有业务类型的基础，可以把Pod看作运行在K8S集群上的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8S的业务主要可以分为以下几种</p><ul><li>长期伺服型：long-running</li><li>批处理型：batch</li><li>节点后台支撑型：node-daemon</li><li>有状态应用型：stateful application</li></ul><p>上述的几种类型，分别对应的小机器人控制器为：Deployment、Job、DaemonSet 和 StatefulSet  (后面将介绍控制器)</p><h2 id="Pod实现机制"><a href="#Pod实现机制" class="headerlink" title="Pod实现机制"></a>Pod实现机制</h2><p>主要有以下两大机制</p><ul><li>共享网络</li><li>共享存储</li></ul><h3 id="共享网络"><a href="#共享网络" class="headerlink" title="共享网络"></a>共享网络</h3><p>容器本身之间相互隔离的，一般是通过 <strong>namespace</strong> 和 <strong>group</strong> 进行隔离，那么Pod里面的容器如何实现通信？</p><ul><li>首先需要满足前提条件，也就是容器都在同一个<strong>namespace</strong>之间</li></ul><p>关于Pod实现原理，首先会在Pod会创建一个根容器： <code>pause容器</code>，然后我们在创建业务容器 【nginx，redis 等】，在我们创建业务容器的时候，会把它添加到 <code>info容器</code> 中</p><p>而在 <code>info容器</code> 中会独立出  ip地址，mac地址，port 等信息，然后实现网络的共享</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114190913859.png"                      alt="image-20201114190913859"                ></p><p>完整步骤如下</p><ul><li>通过 Pause 容器，把其它业务容器加入到Pause容器里，让所有业务容器在同一个名称空间中，可以实现网络共享</li></ul><h3 id="共享存储"><a href="#共享存储" class="headerlink" title="共享存储"></a>共享存储</h3><p>Pod持久化数据，专门存储到某个地方中</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114193124160.png"                      alt="image-20201114193124160"                ></p><p>使用 Volumn数据卷进行共享存储，案例如下所示</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114193341993.png"                      alt="image-20201114193341993"                ></p><h2 id="Pod镜像拉取策略"><a href="#Pod镜像拉取策略" class="headerlink" title="Pod镜像拉取策略"></a>Pod镜像拉取策略</h2><p>我们以具体实例来说，拉取策略就是 <code>imagePullPolicy</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114193605230.png"                      alt="image-20201114193605230"                ></p><p>拉取策略主要分为了以下几种</p><ul><li>IfNotPresent：默认值，镜像在宿主机上不存在才拉取</li><li>Always：每次创建Pod都会重新拉取一次镜像</li><li>Never：Pod永远不会主动拉取这个镜像</li></ul><h2 id="Pod资源限制"><a href="#Pod资源限制" class="headerlink" title="Pod资源限制"></a>Pod资源限制</h2><p>也就是我们Pod在进行调度的时候，可以对调度的资源进行限制，例如我们限制 Pod调度是使用的资源是 2C4G，那么在调度对应的node节点时，只会占用对应的资源，对于不满足资源的节点，将不会进行调度</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114194057920.png"                      alt="image-20201114194057920"                ></p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>我们在下面的地方进行资源的限制</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114194245517.png"                      alt="image-20201114194245517"                ></p><p>这里分了两个部分</p><ul><li>request：表示调度所需的资源</li><li>limits：表示最大所占用的资源</li></ul><h2 id="Pod重启机制"><a href="#Pod重启机制" class="headerlink" title="Pod重启机制"></a>Pod重启机制</h2><p>因为Pod中包含了很多个容器，假设某个容器出现问题了，那么就会触发Pod重启机制</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114194722125.png"                      alt="image-20201114194722125"                ></p><p>重启策略主要分为以下三种</p><ul><li>Always：当容器终止退出后，总是重启容器，默认策略 【nginx等，需要不断提供服务】</li><li>OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。</li><li>Never：当容器终止退出，从不重启容器 【批量任务】</li></ul><h2 id="Pod健康检查"><a href="#Pod健康检查" class="headerlink" title="Pod健康检查"></a>Pod健康检查</h2><p>通过容器检查，原来我们使用下面的命令来检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br></pre></td></tr></table></figure><p>但是有的时候，程序可能出现了 <strong>Java</strong> 堆内存溢出，程序还在运行，但是不能对外提供服务了，这个时候就不能通过 容器检查来判断服务是否可用了</p><p>这个时候就可以使用应用层面的检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存活检查，如果检查失败，将杀死容器，根据Pod的restartPolicy【重启策略】来操作</span></span><br><span class="line">livenessProbe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 就绪检查，如果检查失败，Kubernetes会把Pod从Service endpoints中剔除</span></span><br><span class="line">readinessProbe</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114195807564.png"                      alt="image-20201114195807564"                ></p><p>Probe支持以下三种检查方式</p><ul><li>http Get：发送HTTP请求，返回200 - 400 范围状态码为成功</li><li>exec：执行Shell命令返回状态码是0为成功</li><li>tcpSocket：发起TCP Socket建立成功</li></ul><h2 id="Pod调度策略"><a href="#Pod调度策略" class="headerlink" title="Pod调度策略"></a>Pod调度策略</h2><h3 id="创建Pod流程"><a href="#创建Pod流程" class="headerlink" title="创建Pod流程"></a>创建Pod流程</h3><ul><li>首先创建一个pod，然后创建一个API Server 和 Etcd【把创建出来的信息存储在etcd中】</li><li>然后创建 Scheduler，监控API Server是否有新的Pod，如果有的话，会通过调度算法，把pod调度某个node上</li><li>在node节点，会通过 <code>kubelet -- apiserver </code> 读取etcd 拿到分配在当前node节点上的pod，然后通过docker创建容器</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114201611308.png"                      alt="image-20201114201611308"                ></p><h3 id="影响Pod调度的属性"><a href="#影响Pod调度的属性" class="headerlink" title="影响Pod调度的属性"></a>影响Pod调度的属性</h3><p>Pod资源限制对Pod的调度会有影响</p><h4 id="根据request找到足够node节点进行调度"><a href="#根据request找到足够node节点进行调度" class="headerlink" title="根据request找到足够node节点进行调度"></a>根据request找到足够node节点进行调度</h4><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114194245517.png"                      alt="image-20201114194245517"                ></p><h4 id="节点选择器标签影响Pod调度"><a href="#节点选择器标签影响Pod调度" class="headerlink" title="节点选择器标签影响Pod调度"></a>节点选择器标签影响Pod调度</h4><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114202456151.png"                      alt="image-20201114202456151"                ></p><p>关于节点选择器，其实就是有两个环境，然后环境之间所用的资源配置不同</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114202643905.png"                      alt="image-20201114202643905"                ></p><p>我们可以通过以下命令，给我们的节点新增标签，然后节点选择器就会进行调度了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node node1 env_role=prod</span><br></pre></td></tr></table></figure><h4 id="节点亲和性"><a href="#节点亲和性" class="headerlink" title="节点亲和性"></a>节点亲和性</h4><p>节点亲和性 <strong>nodeAffinity</strong> 和 之前nodeSelector 基本一样的，根据节点上标签约束来决定Pod调度到哪些节点上</p><ul><li>硬亲和性：约束条件必须满足</li><li>软亲和性：尝试满足，不保证</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114203433939.png"                      alt="image-20201114203433939"                ></p><p>支持常用操作符：in、NotIn、Exists、Gt、Lt、DoesNotExists</p><p>反亲和性：就是和亲和性刚刚相反，如 NotIn、DoesNotExists等</p><h2 id="污点和污点容忍"><a href="#污点和污点容忍" class="headerlink" title="污点和污点容忍"></a>污点和污点容忍</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>nodeSelector 和 NodeAffinity，都是Prod调度到某些节点上，属于Pod的属性，是在调度的时候实现的。</p><p>Taint 污点：节点不做普通分配调度，是节点属性</p><h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h3><ul><li>专用节点【限制ip】</li><li>配置特定硬件的节点【固态硬盘】</li><li>基于Taint驱逐【在node1不放，在node2放】</li></ul><h3 id="查看污点情况"><a href="#查看污点情况" class="headerlink" title="查看污点情况"></a>查看污点情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node k8smaster | grep Taint</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114204124819.png"                      alt="image-20201114204124819"                ></p><p>污点值有三个</p><ul><li>NoSchedule：一定不被调度</li><li>PreferNoSchedule：尽量不被调度【也有被调度的几率】</li><li>NoExecute：不会调度，并且还会驱逐Node已有Pod</li></ul><h3 id="未节点添加污点"><a href="#未节点添加污点" class="headerlink" title="未节点添加污点"></a>未节点添加污点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node [node] key=value:污点的三个值</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role=<span class="built_in">yes</span>:NoSchedule</span><br></pre></td></tr></table></figure><h3 id="删除污点"><a href="#删除污点" class="headerlink" title="删除污点"></a>删除污点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role:NoSchedule-</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114210022883.png"                      alt="image-20201114210022883"                ></p><h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>我们现在创建多个Pod，查看最后分配到Node上的情况</p><p>首先我们创建一个 nginx 的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx</span><br></pre></td></tr></table></figure><p>然后使用命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114204917548.png"                      alt="image-20201114204917548"                ></p><p>我们可以非常明显的看到，这个Pod已经被分配到 k8snode1 节点上了</p><p>下面我们把pod复制5份，在查看情况pod情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment web --replicas=5</span><br></pre></td></tr></table></figure><p>我们可以发现，因为master节点存在污点的情况，所以节点都被分配到了 node1 和 node2节点上</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114205135282.png"                      alt="image-20201114205135282"                ></p><p>我们可以使用下面命令，把刚刚我们创建的pod都删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployment web</span><br></pre></td></tr></table></figure><p>现在给了更好的演示污点的用法，我们现在给 node1节点打上污点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role=<span class="built_in">yes</span>:NoSchedule</span><br></pre></td></tr></table></figure><p>然后我们查看污点是否成功添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node k8snode1 | grep Taint</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114205516154.png"                      alt="image-20201114205516154"                ></p><p>然后我们在创建一个 pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建nginx pod</span></span><br><span class="line">kubectl create deployment web --image=nginx</span><br><span class="line"><span class="comment"># 复制五次</span></span><br><span class="line">kubectl scale deployment web --replicas=5</span><br></pre></td></tr></table></figure><p>然后我们在进行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p>我们能够看到现在所有的pod都被分配到了 k8snode2上，因为刚刚我们给node1节点设置了污点</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114205654867.png"                      alt="image-20201114205654867"                ></p><p>最后我们可以删除刚刚添加的污点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role:NoSchedule-</span><br></pre></td></tr></table></figure><h3 id="污点容忍"><a href="#污点容忍" class="headerlink" title="污点容忍"></a>污点容忍</h3><p>污点容忍就是某个节点可能被调度，也可能不被调度</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114210146123.png"                      alt="image-20201114210146123"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Pod概述&quot;&gt;&lt;a href=&quot;#Pod概述&quot; class=&quot;headerlink&quot; title=&quot;Pod概述&quot;&gt;&lt;/a&gt;Pod概述&lt;/h2&gt;&lt;p&gt;Pod是K8S系统中可以创建和管理的最小单元，是资源对象模型中由用户创建或部署的最小资源对象模型，也是在K8S上运行</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之YAML文件详解</title>
    <link href="http://example.com/2022/08/22/k8s%E4%B9%8BYAML%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2022/08/22/k8s%E4%B9%8BYAML%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-08-22T07:57:21.000Z</published>
    <updated>2022-08-25T01:10:17.503Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>k8s 集群中对资源管理和资源对象编排部署都可以通过声明样式（YAML）文件来解决，也就是可以把需要对资源对象操作编辑到YAML 格式文件中，我们把这种文件叫做资源清单文件，通过kubectl 命令直接使用资源清单文件就可以实现对大量的资源对象进行编排部署了。一般在我们开发的时候，都是通过配置YAML文件来部署集群的。</p><p>YAML文件：就是资源清单文件，用于资源编排</p><h2 id="YAML文件介绍"><a href="#YAML文件介绍" class="headerlink" title="YAML文件介绍"></a>YAML文件介绍</h2><h3 id="YAML概述"><a href="#YAML概述" class="headerlink" title="YAML概述"></a>YAML概述</h3><p>YAML ：仍是一种标记语言。为了强调这种语言以数据做为中心，而不是以标记语言为重点。</p><p>YAML 是一个可读性高，用来表达数据序列的格式。</p><h3 id="YAML-基本语法"><a href="#YAML-基本语法" class="headerlink" title="YAML 基本语法"></a>YAML 基本语法</h3><ul><li>使用空格做为缩进</li><li>缩进的空格数目不重要，只要相同层级的元素左侧对齐即可</li><li>低版本缩进时不允许使用Tab 键，只允许使用空格</li><li>使用#标识注释，从这个字符一直到行尾，都会被解释器忽略</li><li>使用 — 表示新的yaml文件开始</li></ul><h3 id="YAML-支持的数据结构"><a href="#YAML-支持的数据结构" class="headerlink" title="YAML 支持的数据结构"></a>YAML 支持的数据结构</h3><h4 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h4><p>键值对的集合，又称为映射(mapping) &#x2F; 哈希（hashes） &#x2F; 字典（dictionary）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对象类型：对象的一组键值对，使用冒号结构表示</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">Tom</span></span><br><span class="line"><span class="attr">age:</span> <span class="number">18</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yaml 也允许另一种写法，将所有键值对写成一个行内对象</span></span><br><span class="line"><span class="attr">hash:</span> &#123;<span class="attr">name:</span> <span class="string">Tom</span>, <span class="attr">age:</span> <span class="number">18</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组类型：一组连词线开头的行，构成一个数组</span></span><br><span class="line">People</span><br><span class="line">- Tom</span><br><span class="line">- Jack</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数组也可以采用行内表示法</span></span><br><span class="line">People: [Tom, Jack]</span><br></pre></td></tr></table></figure><h2 id="YAML文件组成部分"><a href="#YAML文件组成部分" class="headerlink" title="YAML文件组成部分"></a>YAML文件组成部分</h2><p>主要分为了两部分，一个是控制器的定义 和 被控制的对象</p><h3 id="控制器的定义"><a href="#控制器的定义" class="headerlink" title="控制器的定义"></a>控制器的定义</h3><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114110444032.png"                      alt="image-20201114110444032"                ></p><h3 id="被控制的对象"><a href="#被控制的对象" class="headerlink" title="被控制的对象"></a>被控制的对象</h3><p>包含一些 镜像，版本、端口等</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://s2.loli.net/2022/08/22/3OTlVXjvmxgsPwz.png"                      alt="image-20201114110600165"                ></p><h3 id="属性说明"><a href="#属性说明" class="headerlink" title="属性说明"></a>属性说明</h3><p>在一个YAML文件的控制器定义中，有很多属性名称</p><table><thead><tr><th align="center">属性名称</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">apiVersion</td><td align="center">API版本</td></tr><tr><td align="center">kind</td><td align="center">资源类型</td></tr><tr><td align="center">metadata</td><td align="center">资源元数据</td></tr><tr><td align="center">spec</td><td align="center">资源规格</td></tr><tr><td align="center">replicas</td><td align="center">副本数量</td></tr><tr><td align="center">selector</td><td align="center">标签选择器</td></tr><tr><td align="center">template</td><td align="center">Pod模板</td></tr><tr><td align="center">metadata</td><td align="center">Pod元数据</td></tr><tr><td align="center">spec</td><td align="center">Pod规格</td></tr><tr><td align="center">containers</td><td align="center">容器配置</td></tr></tbody></table><h2 id="如何快速编写YAML文件"><a href="#如何快速编写YAML文件" class="headerlink" title="如何快速编写YAML文件"></a>如何快速编写YAML文件</h2><p>一般来说，我们很少自己手写YAML文件，因为这里面涉及到了很多内容，我们一般都会借助工具来创建</p><h3 id="使用kubectl-create命令"><a href="#使用kubectl-create命令" class="headerlink" title="使用kubectl create命令"></a>使用kubectl create命令</h3><p>这种方式一般用于资源没有部署的时候，我们可以直接创建一个YAML配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试运行,并不会真正的创建镜像</span></span><br><span class="line">kubectl create deployment web --image=nginx -o yaml --dry-run</span><br></pre></td></tr></table></figure><p>或者我们可以输出到一个文件中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx -o yaml --dry-run &gt; hello.yaml</span><br></pre></td></tr></table></figure><p>然后我们就在文件中直接修改即可</p><h3 id="使用kubectl-get命令导出yaml文件"><a href="#使用kubectl-get命令导出yaml文件" class="headerlink" title="使用kubectl get命令导出yaml文件"></a>使用kubectl get命令导出yaml文件</h3><p>可以首先查看一个目前已经部署的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114113115649.png"                      alt="image-20201114113115649"                ></p><p>然后我们导出 nginx的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy nginx -o=yaml --<span class="built_in">export</span> &gt; nginx.yaml</span><br></pre></td></tr></table></figure><p>然后会生成一个 <code>nginx.yaml</code> 的配置文件</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114184538797.png"                      alt="image-20201114184538797"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;k8s 集群中对资源管理和资源对象编排部署都可以通过声明样式（YAML）文件来解决，也就是可以把需要对资源对象操作编辑到YAML 格式文件中</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之集群管理工具kubectl</title>
    <link href="http://example.com/2022/08/22/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7kubectl/"/>
    <id>http://example.com/2022/08/22/k8s%E4%B9%8B%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7kubectl/</id>
    <published>2022-08-22T07:06:20.000Z</published>
    <updated>2022-08-25T01:09:45.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署</p><h2 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h2><p>命令格式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [<span class="built_in">command</span>] [<span class="built_in">type</span>] [name] [flags]</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>command：指定要对资源执行的操作，例如create、get、describe、delete</li><li>type：指定资源类型，资源类型是大小写敏感的，开发者能够以单数 、复数 和 缩略的形式</li></ul><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod pod1</span><br><span class="line">kubectl get pods pod1</span><br><span class="line">kubectl get po pod1</span><br></pre></td></tr></table></figure><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/ainianxu/image/master/image-20201114095544185.png"                      alt="image-20201114095544185"                ></p><ul><li>name：指定资源的名称，名称也是大小写敏感的，如果省略名称，则会显示所有的资源，例如</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><ul><li>flags：指定可选的参数，例如，可用 -s 或者 -server参数指定Kubernetes API server的地址和端口</li></ul><h2 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h2><h3 id="kubectl-help-获取更多信息"><a href="#kubectl-help-获取更多信息" class="headerlink" title="kubectl help 获取更多信息"></a>kubectl help 获取更多信息</h3><p>通过 help命令，能够获取帮助信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取kubectl的命令</span></span><br><span class="line">kubectl --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取某个命令的介绍和使用</span></span><br><span class="line">kubectl get --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><p>常见的基础命令</p><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">create</td><td align="center">通过文件名或标准输入创建资源</td></tr><tr><td align="center">expose</td><td align="center">将一个资源公开为一个新的Service</td></tr><tr><td align="center">run</td><td align="center">在集群中运行一个特定的镜像</td></tr><tr><td align="center">set</td><td align="center">在对象上设置特定的功能</td></tr><tr><td align="center">get</td><td align="center">显示一个或多个资源</td></tr><tr><td align="center">explain</td><td align="center">文档参考资料</td></tr><tr><td align="center">edit</td><td align="center">使用默认的编辑器编辑一个资源</td></tr><tr><td align="center">delete</td><td align="center">通过文件名，标准输入，资源名称或标签来删除资源</td></tr></tbody></table><h3 id="部署命令"><a href="#部署命令" class="headerlink" title="部署命令"></a>部署命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">rollout</td><td align="center">管理资源的发布</td></tr><tr><td align="center">rolling-update</td><td align="center">对给定的复制控制器滚动更新</td></tr><tr><td align="center">scale</td><td align="center">扩容或缩容Pod数量，Deployment、ReplicaSet、RC或Job</td></tr><tr><td align="center">autoscale</td><td align="center">创建一个自动选择扩容或缩容并设置Pod数量</td></tr></tbody></table><h3 id="集群管理命令"><a href="#集群管理命令" class="headerlink" title="集群管理命令"></a>集群管理命令</h3><table><thead><tr><th>命令</th><th>介绍</th></tr></thead><tbody><tr><td>certificate</td><td>修改证书资源</td></tr><tr><td>cluster-info</td><td>显示集群信息</td></tr><tr><td>top</td><td>显示资源(CPU&#x2F;M)</td></tr><tr><td>cordon</td><td>标记节点不可调度</td></tr><tr><td>uncordon</td><td>标记节点可被调度</td></tr><tr><td>drain</td><td>驱逐节点上的应用，准备下线维护</td></tr><tr><td>taint</td><td>修改节点taint标记</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="故障和调试命令"><a href="#故障和调试命令" class="headerlink" title="故障和调试命令"></a>故障和调试命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">describe</td><td align="center">显示特定资源或资源组的详细信息</td></tr><tr><td align="center">logs</td><td align="center">在一个Pod中打印一个容器日志，如果Pod只有一个容器，容器名称是可选的</td></tr><tr><td align="center">attach</td><td align="center">附加到一个运行的容器</td></tr><tr><td align="center">exec</td><td align="center">执行命令到容器</td></tr><tr><td align="center">port-forward</td><td align="center">转发一个或多个</td></tr><tr><td align="center">proxy</td><td align="center">运行一个proxy到Kubernetes API Server</td></tr><tr><td align="center">cp</td><td align="center">拷贝文件或目录到容器中</td></tr><tr><td align="center">auth</td><td align="center">检查授权</td></tr></tbody></table><h3 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">apply</td><td align="center">通过文件名或标准输入对资源应用配置</td></tr><tr><td align="center">patch</td><td align="center">使用补丁修改、更新资源的字段</td></tr><tr><td align="center">replace</td><td align="center">通过文件名或标准输入替换一个资源</td></tr><tr><td align="center">convert</td><td align="center">不同的API版本之间转换配置文件</td></tr><tr><td align="center">label</td><td align="center">更新资源上的标签</td></tr><tr><td align="center">annotate</td><td align="center">更新资源上的注释</td></tr><tr><td align="center">completion</td><td align="center">用于实现kubectl工具自动补全</td></tr><tr><td align="center">api-versions</td><td align="center">打印受支持的API版本</td></tr><tr><td align="center">config</td><td align="center">修改kubeconfig文件（用于访问API，比如配置认证信息）</td></tr><tr><td align="center">help</td><td align="center">所有命令帮助</td></tr><tr><td align="center">plugin</td><td align="center">运行一个命令行插件</td></tr><tr><td align="center">version</td><td align="center">打印客户端和服务版本信息</td></tr></tbody></table><h3 id="目前使用的命令"><a href="#目前使用的命令" class="headerlink" title="目前使用的命令"></a>目前使用的命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个nginx镜像</span></span><br><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对外暴露端口</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看资源</span></span><br><span class="line">kubectl get pod, svc</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署&lt;/</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s之Kubeadm和二进制方式对比</title>
    <link href="http://example.com/2022/08/22/k8s%E4%B9%8BKubeadm%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94/"/>
    <id>http://example.com/2022/08/22/k8s%E4%B9%8BKubeadm%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94/</id>
    <published>2022-08-22T02:36:21.000Z</published>
    <updated>2022-08-24T09:03:36.732Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubeadm方式搭建K8S集群"><a href="#Kubeadm方式搭建K8S集群" class="headerlink" title="Kubeadm方式搭建K8S集群"></a>Kubeadm方式搭建K8S集群</h2><ul><li><p>安装虚拟机，在虚拟机安装Linux操作系统【3台虚拟机】</p></li><li><p>对操作系统初始化操作</p></li><li><p>所有节点安装Docker、kubeadm、kubelet、kubectl【包含master和slave节点】</p><ul><li>安装docker、使用yum，不指定版本默认安装最新的docker版本</li><li>修改docker仓库地址，yum源地址，改为阿里云地址</li><li>安装kubeadm，kubelet 和 kubectl<ul><li>k8s已经发布最新的1.19版本，可以指定版本安装，不指定安装最新版本</li><li><code>yum install -y kubelet kubeadm kubectl</code></li></ul></li></ul></li><li><p>在master节点执行初始化命令操作</p><ul><li><code>kubeadm init</code></li><li>默认拉取镜像地址 K8s.gcr.io国内地址，需要使用国内地址</li></ul></li><li><p>安装网络插件(CNI)</p><ul><li><code>kubectl apply -f kube-flannel.yml</code></li><li></li></ul></li><li><p>在所有的node节点上，使用join命令，把node添加到master节点上</p></li><li><p>测试kubernetes集群</p></li></ul><h2 id="二进制方式搭建K8S集群"><a href="#二进制方式搭建K8S集群" class="headerlink" title="二进制方式搭建K8S集群"></a>二进制方式搭建K8S集群</h2><ul><li>安装虚拟机和操作系统，对操作系统进行初始化操作</li><li>生成cfssl 自签证书<ul><li><code>ca-key.pem</code>、<code>ca.pem</code></li><li><code>server-key.pem</code>、<code>server.pem</code></li></ul></li><li>部署Etcd集群<ul><li>部署的本质，就是把etcd集群交给 systemd 管理</li><li>把生成的证书复制过来，启动，设置开机启动</li></ul></li><li>为apiserver自签证书，生成过程和etcd类似</li><li>部署master组件，主要包含以下组件<ul><li>apiserver</li><li>controller-manager</li><li>scheduler</li><li>交给systemd管理，并设置开机启动</li><li>如果要安装最新的1.19版本，下载二进制文件进行安装</li></ul></li><li>部署node组件<ul><li>docker</li><li>kubelet</li><li>kube-proxy【需要批准kubelet证书申请加入集群】</li><li>交给systemd管理组件- 组件启动，设置开机启动</li></ul></li><li>批准kubelet证书申请 并加入集群</li><li>部署CNI网络插件</li><li>测试Kubernets集群【安装nginx测试】</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Kubeadm方式搭建K8S集群&quot;&gt;&lt;a href=&quot;#Kubeadm方式搭建K8S集群&quot; class=&quot;headerlink&quot; title=&quot;Kubeadm方式搭建K8S集群&quot;&gt;&lt;/a&gt;Kubeadm方式搭建K8S集群&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装虚拟机</summary>
      
    
    
    
    
    <category term="k8s" scheme="http://example.com/tags/k8s/"/>
    
  </entry>
  
</feed>
