[{"title":"机器学习 day01初识机器学习","url":"/2021/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day01%E5%88%9D%E8%AF%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","content":"01 监督学习\n监督学习是指我们给算法一个数据集，其中包含了正确的答案。算法的目的就是给出更多的正确答案。\n回归是指我们设法预测连续值的属性，可以应用在预测房子价格等方面。\n分类是指我们设法预测离散值的输出(0或1)，可以应用在判断账户是否被入侵等方面。\n\n02 无监督学习\n无监督学习也会给一个数据集，但是数据集不包括正确答案(里面的数据要么都有相同的标签要么都没有标签)。无监督学习会将数据分为一个个不同的簇，这就是聚类算法。\n聚类算法可以应用在|搜集新闻并将相关新闻组合在一起|星系形成理论|市场销售等领域\n\n\n建议使用Octave免费开源的软件，许多学习算法都可以用几行代码将其实现。例如svd函数(奇异值分解)已经作为线性代数常规函数内置在Octave中了。\n"},{"title":"机器学习 day03线性代数基础","url":"/2021/07/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day03%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80/","content":"01 矩阵与向量\n小写字母表示向量：a b c\n大写字母表示矩阵：A B C\n\n02 加法和标量乘法\n只有维度相同的矩阵才能相加\n一个数乘一个矩阵与一个矩阵乘一个数的结果相同\n\n\n03 矩阵向量乘法\n矩阵与向量相乘\n\n\n\n例子：\n\n\n\n将一个方程转化为矩阵向量相乘的形式\n\n\n04 矩阵相乘\n矩阵相乘计算原理：只需将第二个矩阵的每一列提取出来，计算矩阵与向量相乘，最终将其拼接成为最终答案。\n\n\n\n将三个方程转化为两个矩阵相乘的形式\n\n\n05 矩阵乘法特性\n不能使用乘法交换律（单位矩阵除外），会改变结果的维度及数值\n\n\n\n矩阵与实数一样，都符合乘法结合律\n\n\n06 逆和转置\n只有方阵才有逆矩阵\n\n\n\n矩阵的转置：将A的第一行变成A的转置的第一列，将A的第二行变成A的转置的第二列。。。。。。\n\n\n"},{"title":"机器学习 day02单变量线性回归","url":"/2021/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day02%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","content":"01 模型描述为了更好的描述监督学习问题，需要给出训练集并以此构建一个模型。\n\n下面先学习几个符号：\n\nm:代表的是训练集有几个\nx:代表的是输入的特征\ny:代表的是输出，也就是预测的目标变量\nh:代表假设函数，引导从x得到y的函数\n\n02 代价函数（平方误差函数）\n可以通过代价函数来衡量假设函数的准确性。\n\n代价函数取值越小，假设函数就越准确。\n\n\n\n\n代价函数有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。\n\n在线性回归中，我们要解决的是最小化问题\n\n代价函数是解决回归问题最常用的手段\n\n\n\n03 梯度下降梯度下降法：可以将代价函数最小化\n\n一般将两个参数初始化为0，再不断的改变参数的值，使得代价函数取值达到最小。\n\n\na：代表学习率（永远为正），用来控制使用梯度下降时，迈出步子的大小。下面是a太小与太大的情况。\n\n\n\n导数的含义：针对于只有一个参数的，两种不同初始点的情况分析。\n\n\n\n事实上，当取值越接近最优解时，梯度下降的幅度也就越小，因为导数始终向0靠拢。\n\n\n04 线性回归的梯度下降\n将代价函数带入梯度下降算法，并求偏导数可得：\n\n\n\n将所求的偏导数带回梯度下降算法\n\n\n\n通过梯度下降算法，一步步的进行，最终可以得到与数据最拟合的直线\n\n\n"},{"title":"机器学习 day04多变量线性回归","url":"/2021/07/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day04%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","content":"01 多元线性回归–多特征向量情况下的假设形式\n一些符号表示：\n\n\n\n简化下面等式的表达方法：向量内积转化\n\n\n02 多元梯度下降算法\n多元线性回归方程+代价函数+梯度下降函数\n\n\n\n单元及多元线性回归的梯度下降法对比\n\n\n03 多元梯度下降法–特征缩放\n特征缩放的目的只是为了运行更快。使特征值比较接近，使图像变得比较圆。以至于梯度下降的速度更快，收敛所需要的迭代次数更少，收敛更快。缩放前后对比图如下：\n\n\n\n特征值的取值别太大也别太小，与下面这个范围足够接近最好。\n\n\n\n均值归一化的工作：X &#x3D;（当前值-平均值）&#x2F;【（最大值-最小值）只要是这个范围左右就可以】\n\n\n04 多元梯度下降法–学习率\n梯度算法正常工作图如下：代价函数随迭代次数的变化，最终收敛。\n\n\n\n如果所得图像不是一直减小的，那么需要减小学习率，当然学习率也不能过小，否则梯度下降将会十分缓慢，迭代次数无限增加。\n\n\n\n得到一个不错的学习率：按照三的倍数来取值，尝试一系列的学习率，找到个太小的值，再找到另一个太大的值，然后取太大的值，或者比太大的值略小的比较合理的值\n\n\n05 特征和多项式回归\n特征可以根据自己的需求选择合适的特征，例如将两个不同的特征相乘得到一个新的特征\n如果只用多次函数，适当使用特征缩放将起到很好的效果：\n\n\n\n可以有多种合理的选择，比如也可以是平方根。\n\n\n06 正规方程–区别于迭代方法的直接解法\n正规方程：对代价函数求偏导数，并将其置0，就可以得到使代价函数最小的值。\n\n\n\n方程的形式及例子：\n\n\n\n使用正规方程就不用对特征进行缩放了。\n\n选择合适的算法（梯度下降还是正规方程）一般特征&lt;10000时选用正规方程直接求解，他们二者的优缺点：\n\n\n\n07 正规方程在矩阵不可逆情况下的解决方法\n在Octave中pinv（伪逆）与inv（逆）是求逆矩阵的，就算矩阵没有逆，pinv也会求出它的逆。\n\n首先看是否有多余的特征（两个特征线性相关），选择进行删除，直到没有多余的为止；再观察是否特征过多，选择没有影响的特征进行删除。\n\n\n\n"},{"title":"机器学习 day05Octave教程","url":"/2021/07/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day05Octave%E6%95%99%E7%A8%8B/","content":"01 基本操作\n注释表达：%\n&gt;&gt; 1~= 2 %注释ans = 1\n\n\n\n不等号表达：~&#x3D;\n&gt;&gt; 1~= 2ans = 1\n\n\n\n隐藏提示命令：PS1(‘&gt;&gt; ‘);\noctave:6&gt; PS1(&#x27;&gt;&gt; &#x27;);&gt;&gt;\n\n\n\n想分配一个变量，但是不想在屏幕上显示结果：只需在结尾加分号（;）\n&gt;&gt; a = pi;&gt;&gt; a = pia = 3.1416\n\n\n\n显示命令：disp( );\n&gt;&gt; disp(a);3.1416\n\n用disp( );显示字符串\n&gt;&gt; disp(sprintf(&#x27;2 decimals:%0.2f&#x27;,a)) %0.2f:小数点后两位2 decimals:3.14\n\n显示更多的小数点后几位：format long\n&gt;&gt; format long&gt;&gt; aa = 3.141592653589793\n\n\n\n显示少量的小数点后几位：format short\n&gt;&gt; format short&gt;&gt; aa = 3.1416\n\n\n\n建立一个矩阵：\n\n\n&gt;&gt; A = [1 2;3 4;5 6]A =   1   2   3   4   5   6&gt;&gt; A = [1 2;&gt; 3 4;&gt; 5 6]A =   1   2   3   4   5   6\n\n\n\n\n建立向量：\n\n&gt;&gt; v = [1 2 3] %行向量v =   1   2   3&gt;&gt; v = [1; 2; 3] %列向量v =   1   2   3&gt;&gt; v = 1:0.1:2 %从1开始每次增加0.1到2的行向量v =    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000    1.8000    1.9000    2.0000&gt;&gt; V = 1:6 %从1到6的整数V =   1   2   3   4   5   6\n\n\n生成单位矩阵\n\n&gt;&gt; ones(2,3) %2×3的ans =   1   1   1   1   1   1\n\n\n生成零矩阵\n\n&lt;&lt; zeros(2,3)ans =   0   0   0   0   0   0\n\n\n随机生成0到1的值\n\n&lt;&lt; rand(3,3)ans =   0.914686   0.131127   0.563647   0.296402   0.590713   0.134373   0.243013   0.477658   0.071331\n\n\n生成服从高斯分布的随机数：均值为0，标准差或者方差为1\n\n&lt;&lt;  w = randn(1,3)w =  -0.1177   2.0111   0.7261\n\n\n绘制直方图：hist( ):均值为-6，方差为10，标准差为根号10\n\n\n\n生成单位矩阵：eye( )\n\n&lt;&lt; eye(4)ans =Diagonal Matrix   1   0   0   0   0   1   0   0   0   0   1   0   0   0   0   1\n\n\n显示帮助：help eye;help help;按q退出\n\n显示矩阵大小:size( )\n\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt; size(A)ans =   3   2&lt;&lt; size(A,1) %显示行数ans = 3&lt;&lt; size(A,2) %显示列数ans = 2\n\n\n返回最大维度大小：length( )|一般对向量使\n\n&lt;&lt; length(A)ans = 3 %3×2，最大维度为3&lt;&lt; length([1;2;3;4;5])ans = 5\n\n02 移动数据\n显示当前在内存中存储的所有变量：who\nwhos显示更加完善的信息\n\n&lt;&lt; whoVariables visible from the current scope:A    ans&lt;&lt; whosVariables visible from the current scope:variables in scope: top scope   Attr Name        Size                     Bytes  Class   ==== ====        ====                     =====  =====        A           3x2                         48  double        ans         1x10                        10  charTotal is 16 elements using 58 bytes\n\n\n载入文件格式：load+文件；load(‘文件’)\n删除某个变量：clear+变量\n存储数据到硬盘中：save+存储文件名+数据\n在找矩阵某一个元素时，：表示这一行或者这一列的元素\n\n&lt;&lt; A(2,1)ans = 3&lt;&lt; A(2,:)ans =   3   4&lt;&lt; A(:,2)ans =   2   4   6\n\n\n索引操作：同时取得1、3行所有元素\n\n&lt;&lt; A([1 3],:)ans =   1   2   5   6\n\n\n将第二列用其他元素代替\n\n&lt;&lt; A(:,2) = [10; 11; 12]A =    1   10    3   11    5   12    \n\n\n在A的右侧附加一列\n\n&lt;&lt; A = [A,[100; 200; 300]]A =     1    10   100     3    11   200     5    12   300\n\n\n把A所有元素放在单独的一列\n\n&lt;&lt; A(:)ans =     1     3     5    10    11    12   100   200   300\n\n\n将两个矩阵结合在一起\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt;  B = [11 12; 13 14; 15 16]B =   11   12   13   14   15   16&lt;&lt; C = [A B] %左右结合C =    1    2   11   12    3    4   13   14    5    6   15   16&lt;&lt; C = [A;B] %上下结合C =    1    2    3    4    5    6   11   12   13   14   15   16\n\n03 计算数据\n两个矩阵相乘\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt;  B = [11 12; 13 14; 15 16]B =   11   12   13   14   15   16&lt;&lt; C = [1 1;2 2]C =   1   1   2   2&lt;&lt; A*Cans =    5    5   11   11   17   17\n\n\n两个矩阵对应数相乘\n\n&lt;&lt; A.*Bans =   11   24   39   56   75   96\n\n\nA的每个元素进行乘方\n\n&lt;&lt; A.^2ans =    1    4    9   16   25   36\n\n\n求V对应元素的倒数\n\n&lt;&lt; V = [1; 2; 3]V =   1   2   3&lt;&lt; 1 ./Vans =   1.0000   0.5000   0.3333\n\n\n以e为底，v中元素为指数的幂运算\n\n&lt;&lt; exp(V)ans =    2.7183    7.3891   20.0855\n\n\n求绝对值：abs( )\n求相反数直接加-；例如-V\n将V中每个元素+1\n\n&lt;&lt; V + ones(length(V),1) %与V +1等价ans =   2   3   4\n\n\n求A的转置\n\n&lt;&lt; A&#x27;ans =   1   3   5   2   4   6\n\n\n求X最大的数及其索引\n\n&lt;&lt; X = [1 15 2 0.5]X =    1.0000   15.0000    2.0000    0.5000&lt;&lt; val = max(X)val = 15&lt;&lt; [val,ind] = max(X)val = 15ind = 2%如果A是矩阵，那么将求出每一列最大值&lt;&lt; max(A)ans =   5   6\n\n\nX中每个元素与3比较，根据结果返回真和假\n\n&lt;&lt; X&lt;3ans =  1  0  1  1\n\n\n找到比3小的数并返回其索引\n\n&lt;&lt; find(X&lt;3)ans =   1   3   4\n\n\n生成一个幻方矩阵，每行、每列、每个对角线加起来都等于一个数\n\n&lt;&lt; B = magic(3)B =   8   1   6   3   5   7   4   9   2\n\n\n找出B中&gt;&#x3D;7的元素，并返回下标\n\n&lt;&lt; [r,c] = find(B &gt;=7)r =   1   3   2c =   1   2   3\n\n\nX中所有元素相加\n\n&lt;&lt; sum(X)ans = 18.500\n\n\nX中所有元素相乘\n\n&lt;&lt; prod(X)ans = 15\n\n\n对X中元素向下取整\n\n&lt;&lt; floor(X)ans =    1   15    2    0\n\n\n对X中元素向上取整\n\n&lt;&lt; ceil(X)ans =    1   15    2    1\n\n\n取两个随机矩阵中较大的数组合成一个较大值的矩阵\n\n&lt;&lt;  max(rand(3),rand(3))ans =   0.6957   0.9634   0.7179   0.8404   0.9784   0.8319   0.5236   0.8063   0.6697\n\n\n取每一列&#x2F;行最大值\n\n&lt;&lt; max(B,[],1)ans =   8   9   7&lt;&lt; max(B,[],2)ans =   8   7   9\n\n\n求每一行、每一列、对角线相加\n\n&lt;&lt; C = magic(9)C =   47   58   69   80    1   12   23   34   45   57   68   79    9   11   22   33   44   46   67   78    8   10   21   32   43   54   56   77    7   18   20   31   42   53   55   66    6   17   19   30   41   52   63   65   76   16   27   29   40   51   62   64   75    5   26   28   39   50   61   72   74    4   15   36   38   49   60   71   73    3   14   25   37   48   59   70   81    2   13   24   35&lt;&lt; sum(C,2)ans =   369   369   369   369   369   369   369   369   369&lt;&lt; sum(C,1)ans =   369   369   369   369   369   369   369   369   369&lt;&lt; sum(sum(C.*eye(9))) %正对角线相加ans = 369&lt;&lt; sum(sum(C.*flipud(eye(9)))) %副对角线相加，flipud表示使矩阵垂直翻转ans = 369\n\n04 数据绘制\n绘制一个正弦函数图像：plot( );\n\n&lt;&lt; t = [0:0.01:0.98];&lt;&lt; y1 = sin(2*pi*4*t);&lt;&lt; plot(t,y1);\n\n\n\n绘制一个余弦函数图像：plot( );\n\n&lt;&lt; y2 = cos(2*pi*4*t);&lt;&lt; plot(t,y2);\n\n\n\n在旧的图像上面绘制新的图像：hold on;\n\n&lt;&lt; t = [0:0.01:0.98];&lt;&lt; y1 = sin(2*pi*4*t);&lt;&lt; y2 = cos(2*pi*4*t);&lt;&lt; plot(t,y1);&lt;&lt; hold on;&lt;&lt; plot(t,y2,&#x27;r&#x27;);\n\n\n\n加上横与纵轴的标签，标记两条函数，输入标题，并保存到桌面\n\n&lt;&lt; xlabel(&#x27;time&#x27;)&lt;&lt; ylabel(&#x27;value&#x27;)&lt;&lt; legend(&#x27;sin&#x27;, &#x27;cos&#x27;)&lt;&lt; title(&#x27;my plot&#x27;)&lt;&lt; cd &#x27;C:\\Users\\1\\Desktop&#x27;; print -dpng &#x27;plot.png&#x27;\n\n\n\n\n为不同图像标号：就可以同时有两个图像\n\n&lt;&lt; figure(1); plot(t,y1);&lt;&lt; figure(2); plot(t,y2);\n\n\n\n将一个界面分为两个格子，其中正弦占第一个，余弦占第二个\n\nsubplot(1,2,1);&lt;&lt; plot(t,y1);&lt;&lt; subplot(1,2,2);&lt;&lt; plot(t,y2);\n\n\n\n设置横竖轴的范围\n\n&lt;&lt; plot(t,y1);&lt;&lt; axis([0.5 1 -1 1])\n\n\n\n可视化矩阵\n\n&lt;&lt; A = magic(5)A =   17   24    1    8   15   23    5    7   14   16    4    6   13   20   22   10   12   19   21    3   11   18   25    2    9&lt;&lt; imagesc(A)\n\n\n\n生成颜色图像、灰度分布图并在右边加入一个颜色分布\n\n&lt;&lt; imagesc(A), colorbar, colormap gray;\n\n\n\n添加环境路径，找文件时，即使不在文件环境，也可以使用其中的函数\n\naddpath(&#x27;C:\\Users\\1\\Desktop&#x27;)\n\n05 控制语句\noctave可以返回多个返回值\n\n函数定义\n\n\nfunction J = costFunctionJ(X,y, theta)  m = size(X,1);  predictions = X*theta;  sqrError = (predictions-y).^2;  J = 1/(2*m) * sum(sqrError);\n\n\n函数使用\n\n&lt;&lt; addpath(&#x27;C:\\Users\\1\\Desktop&#x27;)&lt;&lt; X = [1 1; 1 2; 1 3]X =   1   1   1   2   1   3&lt;&lt; y = [1; 2; 3]y =   1   2   3&lt;&lt; theta = [0;1];&lt;&lt; J = costFunctionJ(X,y, theta)J = 0&lt;&lt; theta = [0;0];&lt;&lt; J = costFunctionJ(X,y, theta)J = 2.3333&lt;&lt; (1^2+2^2+3^2)/(2*3)ans = 2.3333\n\n06 矢量\n非向量化与向量化的代码对比\n\n\n\n用C++语言及C++线性库所写代码\n\n\n\n向量化\n\n\n"},{"title":"机器学习 day06Logistic回归","url":"/2021/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day06Logistic%E5%9B%9E%E5%BD%92/","content":"01 分类\n针对于离散值来进行分类：y &#x3D; {0，1}\n0表示负类，没有什么东西\n1表示正类，有什么东西\n不建议将线性回归函数应用于分类情况中\n使用线性回归在分类问题，如果一个值远离其他值，将会使线性回归算法不够准确。\n\n\n\nLogistic回归算法的预测值一直介于0和1之间，并不会像线性回归算法大于1或者小于0\n\n02 假设陈述\n假设陈述：当有一个分类问题的时候，我们要使用哪个方程来表示我们的假设。\nLogistic函数的形式如下：对线性回归方程稍作修改。\n\n\n\n输出某个数字，我们会把这个数字当作对一个输入x，y&#x3D;1的概率估计\n\n\n03 决策界限\n决策界限可以帮助我们理解Logistic回归的假设函数在计算什么。\n可以从图看出什么时候预测y &#x3D; 1;什么时候预测y &#x3D; 0;\n\n\n\n决策边界将一个平面划分为两个区域，其中一片区域假设函数预测y &#x3D; 1；另一片区域假设函数预测y &#x3D; 0。只要我们确定好了参数，我们就将完全确定决策边界。例如下图所示：可以得出直线 X1 + X2 &#x3D; 3就是决策边界。\n\n\n\n例题：我们怎么才能使用Logistic回归来拟合这些数据呢？多项式回归及线性回归可以在特征中添加额外的高阶多项式，Logistic回归也可以使用。\n\n\n\n决策边界不是训练集的属性，而是假设本身及其参数的属性，只要给定了参数向量就可以确定决策边界\n\n04 代价函数如何拟合Logistic回归模型的参数。当代价函数为0时，可以得出与预测值想拟合。\n\n\n如果将代价函数带入到Logistic回归中可以得到左侧图像非凸函数，可是我们想要得到右侧这样得凸函数。\n\n\n\n因此我们需要重新找到个代价函数可以用在Logistic回归中，保证找到全局最小值。下面使y &#x3D; 1情况下。\n\n\n\n下面使y &#x3D; 0情况下\n\n\n05 简化代价函数与梯度下降\n简化后得代价函数\n\n\n\n式子是在统计学中得极大似然法得来得，他是统计学中为不同模型快速寻找参数得方法。同时他是凸的。\n\n\n\n如何最小化代价函数：使用梯度下降算法。虽然Logistic回归中梯度下降算法与线性回归中的梯度下降算法长的一样，但是由于假设的定义发生了变化，所以实际上是两种截然不同的。\n\n\n06 高级优化\n一些高级算法的优缺点\n\n\n\n自动求使代价函数最小的参数，使用代码将其实现\n\n\n\n写一个函数，他能返回代价函数值以及梯度值\n\n\n07 多元分类：一对多使用逻辑回归来解决多类别分类问题\n\n\n训练一个逻辑回归分类器，预测i类别y &#x3D; i的概率。在三个分类器中输入x，在其中选择h(x)最大的那个类别。也就是选择出三个里面可信度最高，效果最好的的哪个分类器。\n\n\n\n无论i是多少，我们都能得到一个最高的概率值，我们预测y就是那个值。\n\n\n"},{"title":"机器学习 day07正则化","url":"/2021/07/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day07%E6%AD%A3%E5%88%99%E5%8C%96/","content":"01 过拟合问题正则化可以减少过度拟合问题\n\n1.1  线性回归过拟合问题\n欠拟合 | 刚好合适 | 过拟合\n\n\n\n过度拟合问题将会在变量过多的时候出现，这时训练出的假设能够很好的拟合训练集（所以代价函数实际上可能非常接近于0。或者恰好等于0），但是可能会得到图三这样的曲线，去拟合训练集，以至于它无法泛化到新的样本中。\n泛化是指一个假设模型应用到新样本的能力\n\n1.2  逻辑回归过拟合问题\n欠拟合 | 刚好合适 | 过拟合\n\n\n\n如果我们有过多的特征变量而只有少量的训练集就会出现过拟合问题。\n有两种方法解决过拟合问题：\n尽量减少特征变量的数量（模型选择算法会自动选择哪些变量保留，哪些舍弃）\n正则化：减少量级或者参数的大小\n\n\n\n02 代价函数\n正则化将多阶函数变成二阶函数（将参数尽可能减小），这些参数越小，我们得到的图像也就越圆滑越简单。\n\n\n\n一般来说我们只对参数1以及1之后的进行正则化\n在对代价函数进行修改，添加正则化项的目的是为了缩小参数的值。\n正则化参数是为了控制两个不同目标之间的取舍\n正则化参数如果过大，那么参数都会接近于0，这样就相当于把假设函数的全部项都忽略了，最终变成了欠拟合。\n\n\n03 线性回归的正则化\n线性回归正则化的梯度下降法\n\n\n\n线性回归正则化的正规方程法\n\n\n\nm&lt;&#x3D;n，说明矩阵是不可逆的，但是当正则参数&gt;0，算出来的矩阵一定是可逆的。\n\n\n04 Logistic 回归的正则化\nLogistic 回归添加正则化项\n\n\n\nLogistic 回归的正则化的梯度下降法\n\n\n\n如何在更高级的算法中使用正则化：定义一个costFunction函数，以theta作为输入。在fminunc函数中括号里写上@cosFunction。\nfminunc的意思是函数在无约束条件下的最小值，fminunc函数会将costFunction函数最小化，\n\n\n"},{"title":"机器学习 day08神经网络学习","url":"/2021/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day08%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/","content":"01 非线性假设为什么已经有线性回归和逻辑回归算法了，还要学习神经网络？\n\n\n因为有特别多的特征，许多机器学习都需要学习复杂的非线性假设。如果使用逻辑回归算法，由于项数过多，可以能会导致过拟合问题，此外也存在运算量过大的问题。如果项数只包括二次项的的子集，这样将二次项的数量减少到100个，但是最有可能拟合出右下角椭圆而拟合不出左上角复杂的分界线。\n\n\n\n下图车子例子所示：如果是一张50 * 50 像素的图像， 则会有 50 * 50 &#x3D; 2500个像素单位（如果是彩色，每个像素又有0-255的RGB取值。即有 2500 * 3 &#x3D; 7500），特征数量则有约 n^2 &#x2F; 2 约 3000000 个特征数量。\n\n\n02 神经元与大脑神经网络能够很好的解决不同的机器学习问题\n\n\n神经网络的起源及发展\n\n\n\n神经元是一个计算单元，它从输入通道接受一定数量的信息，并做一些计算，然后将结果通过它的轴突传送到其他节点，或者大脑中其他神经元。\n\n\n03 模型展示\n神经网络模拟了大脑中的神经元或者神经网络。\n在神经网络里我们将使用一个很简单的模型来模拟神经元工作，我们将神经元模拟成一个逻辑单元。黄色代表类似于神经元细胞体的东西，经过“输入”-&gt;“计算”-&gt;“输出”三个步骤，因为X0（偏置单元或偏置神经元）总是1，会根据实际情况判断时候加上X0。\n在神经网络中激活函数是指非线性函数g(z)。单个神经元图如下：\n\n\n\n在神经网络中第一层叫做输入层，因为我们在这一层输入特征；第二层叫做隐藏层（任何一个非输入层和非输出层），隐藏层的值在训练中是看不到的；最后一层叫输出层，因为在这一层输出假设的最终计算结果；\n\n\n\nai(j)代表第j层第i个神经元或者单元的激活项，激活项是由一个具体神经元计算并输出的值。参数(j)就是权重矩阵，它控制从某一层到另外一层的映射。计算三个隐藏单位的值及输出如下：\n\n\n\n如何高效进行计算，并展示一个向量化的实现方法。\n\n前向传播方法\n\n\n\n下面这个神经网络所作的事情就像是逻辑回归，它不是以原本的X1、X2、X3作为特征，而是用a1、a2、a3作为新的特征\n\n\n04 例子与直觉理解神经网络计算复杂非线性函数的输入\n\n\nX1 AND X2运算\n\n\n\nX1 OR X2运算\n\n\n\nNOT X1\n\n\n\nX1 XNOR X2\n\n\n05 多元分类\n要是在神经网络中实现多类别分类，采用的方法本质是一对多法的拓展（其中Xi代表图像，Yi代表那些向量）\n\n\n"},{"title":"机器学习 day09神经网络参数的反向传播算法","url":"/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day09%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","content":"01 代价函数\n有m组训练样本，L代表神经网络结构的总层数，S_l代表第L层的单元数也就是神经元的数量（不包括第L层的偏差单元）。其中二元分类与多类别分类问题如下：\n\n\n\n应用于神经网络的代价函数：h(x)是一个k维向量，h(x)_i代表第i个输出；k的求和符号应用于y_k和h_K,是因为我们主要是将第k个输出单元的值和y_k的值的大小作比较；y_k的值就是这些向量中其应属于哪个类的量。\n\n\n02 反向传播算法反向传播算法是计算代价函数关于所有参数的导数或者偏导数的一种有效方法。\n\n\n使用前向传播方法来计算的顺序，计算一下在给定输入的时候，假设函数是否会真的输出结果。\n\n\n\n反向传播算法中，下图上方下标j上标（l)代表了第l层的第j个结点的误差，下图上方下标j上标（l)实际上就是假设的输出值和训练集y值之间的差。反向传播算法类似于把输出层的误差反向传播给了第三层，然后再传播给第二层，注意没有第一层（第一层可以直观的观察到，没有误差）。\n\n\n\n如何实现反向传播算法来计算这些参数的偏导数：\n\n首先将每一个i和j对应的三角形（三角形是上图上方下标j上标（l)的大写）置0\n接下来遍历整个训练集，将输入层的激活函数设定他为第i个训练样本的输入值\n接下来用正向传播来计算第二层的激活值，然后第三层，最后到最后一层\n使用输出值来计算这个输出值对应的误差项（假设输出-目标输出）\n再通过反向传播算法计算前几层的误差项，一直到第二层\n最后通过三角形来累计我们再前面写好的偏导数项\n\n\n跳出循环后，通过下面的式子计算D(j等于0和j不等于0的情况)，计算出来的D正好就是关于每个参数的偏导数，然后可以用梯度下降法或者一些其他的高级优化算法。\n\n\n\n03 理解反向传播\n理解前向传播\n\n\n\n代价函数应用在只有一个输出单元的情况\n\n\n\n理解反向传播：代价函数是一个关于标签y和神经网络中h(x)的输出值的函数，只要稍微将z(l)j改一下，就会影响神经网络的h(x)，最终改变代价函数的值。\n\n\n04 使用注意：展开参数把参数从矩阵展开向量，以便在高级最优化步骤中的使用需要\n\n\n高级最优化算法都假定theta和initialTheta初始值都是参数向量，也许是n或者n+1维，同时假定这个代价函数的第二个返回值(梯度值)也是n维或者n+1维向量。但是现在在神经网络，参数不再是向量而是矩阵，三个参数在Octave表达如下；梯度矩阵在Octave表达也如下：\n\n\n\n取出矩阵，并将其展开成向量传入theta中，并得到梯度返回值。\nthetaVec就是将这些矩阵全部展开成为一个很长的向量；DVec同理。reshape将相应元素组合起来成相应矩阵。\n\n\n\n上面步骤通过Octave实现如下：\n\n&lt;&lt;Theta1 = ones(10,11)Theta1 =   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1&lt;&lt;Theta2 = 2*ones(10,11)Theta2 =   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2&lt;&lt;Theta3 = 3*ones(1,11)Theta3 =   3   3   3   3   3   3   3   3   3   3   3&lt;&lt;thetaVec = [ Theta1(:);Theta2(:);Theta3(:)];&lt;&lt;size(thetaVec)ans =   231     1&lt;&lt;reshape(thetaVec(1:110),10,11)ans =   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1&lt;&lt;reshape(thetaVec(111:220),10,11)ans =   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2&lt;&lt;reshape(thetaVec(221:231),1,11)ans =   3   3   3   3   3   3   3   3   3   3   3\n\n\n将这一方法应用于我们的学习算法\n\n\n05 梯度检测因为反向传播使用时会出现一些bug，而梯度检测可以很好的解决这些问题，确保前向传播及反向传播都百分百正确。\n\n\n求出该点导数的近似值（参数是实数的情况）\n\n\n\n当参数维向量参数的时候\n\n\n\n在Octave中为了估算导数所要实现的\n\n\n\n总结下如何实现数值上的梯度检验：（注意反向传播算法比梯度检测效率高，检测完一定要关闭梯度检测）\n\n\n06 随机初始化\n在神经网络中将所有参数初始为0，没有任何意义，所有输入都是一样，也就意味这最后输出就输出一个特征，阻挡了神经网络学习任何有趣的东西，我们称之为高度冗余。\n\n\n\n因此就应该使用随机初始化方法。值得注意的是这里的EPSILON与梯度检测中的完全没有关系。\n\n\n\n为了训练神经网络首先将权重随机初始化为一个接近0范围在-EPSILON到EPSILON之间，然后进行反向传播，在进行梯度检测，最后梯度下降算法或其他高级优化算法来最小化代价函数（关于参数sita的函数）。\n\n07 组合到一起\n训练神经网络做的第一件事就是选择一种合适的网络架构（神经元之间的连接模式），注意输出时是输出一个向量y。\n\n\n\n训练神经网络所需要的步骤\n\n\n\n\n反向传播算法是为了算出梯度下降算法的下降方向\n\n"},{"title":"机器学习 day10应用机器学习的建议","url":"/2021/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day10%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/","content":"01 决定下一步做什么\n开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，应如何决定选择哪条路。不要随意选择。\n\n\n\n机器学习诊断法能够提前发现某些方法是无效的。\n\n\n02 评估假设\n将所有数据分为训练集和测试集，最经典的分割方法就是按照7:3的比例。\n\n\n\n线性回归算法和平方误差标准学习和测试学习算法，从训练集学习获得参数，在将参数带入测试集得到测试误差。\n\n\n\n训练和测试逻辑回归的步骤及用错误分类（0&#x2F;1分类错误）来定义测试误差。0&#x2F;1表示了你预测的分类是正确或错误的情况。\n\n\n03 模型选择和训练、验证、测试集\n模型选择问题（想要确定对于一个数据集最合适的多项式次数，怎样选用正确的特征来构造学习算法或者假如你需要选择学习算法中的正则化参数）\n\n模型选择问题：用不同的模型拟合数据集得到参数，接着对所有这些模型求出测试集误差，然后根据哪个模型有最小的测试误差来选择使用哪个模型。\n\n\n\n\n为了解决模型选择出现的问题，我们通常会采用如下的方法来评估一个假设。我们把数据分为三个部分，分别是训练集、验证集、测试集。分配比例分别是6:2:2。\n\n\n\n定义训练误差、交叉验证误差和测试误差\n\n\n\n用验证集选择模型而不是原来的测试集。省下来的测试集可以用它来衡量或者估算算法选择出的模型的泛化误差了。\n\n\n04 诊断偏差与方差如果一个算法表现得不理想，要么是偏差比较大，要么是方差比较大。换句话说要么欠拟合要么过拟合。\n\n\n训练误差随着我们增大多项式的次数而减小；随着我们增大多项式的次数，我们对训练集拟合的也就越好。对于验证误差来说，如果d为1，会有较大误差；如果d为中等次数大小，能够更好的拟合；当d为4时，也就可能过拟合。\n\n\n\n对于验证误差来说，左边这一端对应的就是高偏差问题；右边这一端对应的就是高方差问题。如果训练误差很小，并且验证误差远大于训练误差说明出现过拟合问题（高方差）。如果是高偏差，则训练误差和验证误差都很大。\n\n\n05  正则化和偏差、方差\n第一个图是高偏差，欠拟合；中间正合适；最后一个图是高方差，过拟合。\n\n\n\n我们对训练、验证、测试误差的定义都是平均的误差平方和，或者是不使用正则化项时，训练集、验证集和测试集的平均的误差平方和的一半。\n\n\n\n自动选择正则化参数的方法：首先选取一系列想要试用的步长，通常来说步长设为2倍速增长，直到一个比较大的值。这样就选取了12个对应的正则化参数。然后对这12个模型分别最小化代价函数，得到完全不同的参数向量。可以把这些模型用不同的正则化参数来进行拟合，然后我们可以用验证集来评价这些参数sita在验证集上的平均的误差平方和，最终选择误差最小的模型。\n\n\n\n当我们改变正则化参数时，我们的假设在训练集和验证集上的表现（对应本节第一个图）\n\n\n06 学习曲线学习曲线可以判断某一学习算法是否处于偏差或者方差问题，还是二者都有。\n\n\n当训练集个数很少的时候，能够十分完美的拟合数据，训练集误差基本为0，但是随着训练集越来越多，训练集误差也就会越来越大，逐渐趋于水平。而验证集误差，随着训练集的个数增加而减小，最终趋于水平。\n\n\n\n在高偏差的情况下，训练集误差和验证集误差最终将十分接近，再增加训练集数量将毫无意义。\n\n\n\n在高方差的情况下，总体来说随着训练集数量的增多，训练集误差将会增加，但是增加的很小。而验证集误差一直都比较高，虽然会有所下降，但是不多。所以增加训练集数量还是很有用的。\n\n\n07 决定接下来做什么\n接下来回到第一节的第一个图，1和2和6对应着高方差的情况，3和4和5对应高偏差的情况（个人理解：高方差就是在多项式的形式下出现的，高偏差就是在项数少的情况下出现的）。\n\n\n\n小型神经网络计算量少，大型神经网络比较容易出现过拟合问题（但是可以用正则化来进行解决），相对来说大型神经网络性能更好。\n还有就是选择隐含层层数的问题，可以将数据分为训练集、验证集还有测试集。用训练集分别训练一层、两层、三层的隐含层，最终用验证集来测试，选出合适的层数。\n\n\n"},{"title":"机器学习 day11机器学习系统设计","url":"/2021/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day11%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","content":"01 确定执行的优先级在实际工作过程中，我们应该优先处理哪些事情\n\n\n以邮件筛选为例，选择邮件的特征向量的方法。通常我们会挑选出在训练集中出现频率最多的n个单词，将其作为特征向量。\n\n\n\n如何在有限时间里让垃圾邮件分类器具有高精准度和低错误率。\n用更复杂的特征变量来描述邮件（可以在邮件标题中获取复杂的特征，来捕捉这封邮件的来源，以此判断是否为垃圾邮件）。\n关注邮件的正文，并构建更复杂的特征。\n来检测单词是否故意出现拼写的错误\n\n\n\n\n02 误差分析误差分析就是一种手动地去检查算法所出现的失误的过程，走向最有成效的道路。\n\n\n通过手动检查分类错误的邮件，来看哪一类分类错误的多，哪一个出现错的情况最多，就着重去构造这类特征，加以训练。\n\n\n\n交叉验证错误率：单一规则的数值评价指标。\n如果只是手动地去检查看看这些例子表现得好不好，会让你很难去决定到底应不应该做出某种决定；但是通过交叉验证错误率就可以直观的看误差率是变大还是变小了，他能告诉你你的想法是提高了还是降低。\n\n\n\n一旦有了一个初始的算法实现，我们就能使用一个强有力的工具，来帮助决定下一步应该做什么：\n看看他所造成的错误：通过误差分析来看看它出现了什么失误，然后以此决定之后的优化方法。\n如果已经有了一个简单粗暴算法实现，又有一个数值评价指标，这些能帮助来试验新的想法，能够快速观察是否能够提高算法的表现，决定应该包含什么，应该舍弃什么。\n\n\n\n03 不对称性分类的误差评估当有倾斜类问题时，使用准确率与召回率来评价学习算法要比用分类误差或者分类准确率好得多。\n\n\n偏斜类：一个类中的样本数与另一个类中的数据相比多很多（比如，没有肿瘤的比有肿瘤的要多得多）。所以说恒把y&#x3D;0算出来的误差将会很小，因为有肿瘤的人很少。\n\n\n\n所以我们想要一个不同的评估度量值：查准率和召回率。其中查准率是指对于所有我们的预测，患有癌症的病人，有多大比率的病人是真正患有癌症的。召回率是指假设如果测试集或者验证集中的病人确实得了癌症，有多大比率正确预测他们得了癌症。也就是如果所有病人都得了癌症，有多少人我们能够正确告诉他们你需要治疗。查准率和召回率越高越好。算法预测值与实际值分别是：1&#x2F;1（真阳性）、0&#x2F;0（真阴性）、1&#x2F;0（假阳性）、0&#x2F;1（假阴性）。\n\n\n04 精确度和召回率的权衡\n在逻辑回归中逻辑输出在0到1之间，其中0.5是个分界值，但是我们想在十分确定得情况下告诉病人真实信息，因此分界值为0.7，甚至0.9（是一个高查准率的模型，但是召回率会变低）。现在我们将分界值设置到较低（有30%几率得病），会得到高召回率，较低得查准率。\n\n\n\n有没有办法自动选取临界值？或者说有不同的算法，我们如何比较不同的查准率和召回率？或者临界值不同，我们怎样决定哪个更好？–如果使用平均值来计算是不可行的，因为如果假设y &#x3D; 1和y &#x3D; 0这两种极端的情况（要么很高召回率、很低查准率，要么很低召回率、很高查准率），他们俩不是好的模型。再此我们使用F值的公式，因为它同时结合召回率及查准率。\n\n\n\n自动选择临界值来决定你希望预测y&#x3D;1还是y&#x3D;0合理的方法：试一试不同的临界值，在检验集进行测试，看哪个临界值可以在检验集得到最高的F。这就是为分类器自动选择临界值的合理方法。\n\n05 机器学习数据在一定条件下，得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获取具有良好性能的学习算法。这种情况一般出现在这些条件对于你的问题都成立，并且可以得到大量数据。\n\n\n并不是拥有最好算法的人能成功，而是拥有最多数据的人能成功。\n\n\n\n如果让一个英语好的选词填空，它可以通过特征x让我们能够准确的预测y，相反的，我们让一个房地产专家预测一个房价，而只告诉它房子的面积，其他特征不告诉，他会很难预测。因此如果这个假设正确可以看出大量数据是很有意义的。\n\n\n\n得到一个低偏差（一个强大的具有很多参数的学习算法，可以很好的拟合复杂的函数）和低方差（如果训练集远大于参数的数量，就不大可能会过拟合）的学习算法（特征值足够并且训练集很庞大）\n\n\n"},{"title":"机器学习总结","url":"/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","content":"\n监督学习算法：线性回归、逻辑回归、神经网络、支持向量机（在这些问题中会有带标签的数据和样本）\n无监督学习：K-均值聚类算法、主成分分析法（进行降维）、异常检测算法（对算法进行评估）\n特定的应用和话题：推荐系统、大规模机器学习系统（包括并行和映射-化简算法）\n其他的应用：滑动窗口分类器（计算机视觉问题）\n从各个不同的方面给出了如何构建机器学习系统的建议：偏差和方差（尝试了是什么使得机器学习算法工作或者是不工作）、正则化（解决一些方差问题）、学习算法的评估方法：召回率和F1分数这样的评价指标和实践方面的评测方法：训练集-交叉验证集-测试集（当你开发一个机器学习系统时如何合理分配你的时间）、诊断方法：学习曲线和误差分析及上限分析（如何调试算法确保学习算法能够正常工作）。所有这些工具都能帮助你决定下一步该做什么以及怎么分配时间。\n\n\n"},{"title":"机器学习 day12支持向量机","url":"/2021/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day12%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","content":"01 优化目标支持向量机（SVM）在学习复杂的非线性方程时，能够提供一种更为清晰和更加强大的方式。\n\n\n从逻辑回归开始，稍作改动成为支持向量机\n观察下逻辑回归的假设函数和sigmoid激活函数\n\n\n\n\n\n观察逻辑回归的代价函数，当把整个假设函数的定义代入其中，得到的就是每个样本对总体函数的具体贡献。\n\n\n\n为了构建支持向量机，我们从这个代价函数开始进行少量修改，我们取z&#x3D;1，画出要使用的代价函数，右边都是平的，左边画出一条和最开始的幅度相似的直线，这就是y&#x3D;1时使用的代价函数。同样的做出y&#x3D;0时使用的代价函数。\n\n\n\n有了这些定义后，就可以开始构造支持向量机了，将修改后的代价函数定义带入到原始的逻辑回归代价函数中。支持向量机的代价函数将1&#x2F;m去掉，因为1&#x2F;m是一个常数，不影响得到参数的最优值。对于SVM来说我们将用一个不同的参数来控制第一项A与第二项B的相对权重，如果把C设置的很小，那么B就比A占有更大的权重。于是就得到了支持向量机的总体优化目标\n\n\n\n最后和逻辑回归不同的是，支持向量机并不会输出概率，相对的我们得到的是通过优化这个代价函数得到参数sita，支持向量机它是进行了一个直接的预测y&#x3D;0&#x2F;y&#x3D;1,学习得到参数sita后，这就是支持向量机的假设函数的形式。\n\n\n02 直观上对大间隔的理解\n下面是SVM代价函数，支持向量机不是恰好能正确分类就行，因此需要比0大或者小很多（也就是1或者-1）。\n\n\n\n如果C非常的大，那么当最小化最优目标的时候，将迫切的希望找到一个值使得第一项等于0。在两种情况下，通过选择参数sita使得第一项为0。\n\n\n\n支持向量机会选择尽量把正样本和负样本以最大的间距分开的假设模型。可以看出黑色的决策边界和训练样本的最小距离要更大一些。\n\n\n\n下面的大间距分类器是在常数C被设的非常大情况下得出的，平常情况下会得到黑色线，但是如果在一侧加入异常样本，那么就可能会是粉色的线。如果C不是很大，那么就算是加入异常点也会是黑色线。\n\n\n03 大间隔分类器的数学原理\n向量内积\n\n\n\n支持向量机的优化目标函数，当n&#x3D;2时我们只有两个特征量（也就是只有两个参数sita）。因此对于优化目标函数来说支持向量机做的是最小化参数向量sita的范数的平方。（为了简便都令sita0&#x3D;0）\n\n\n\n将sita转置x(i)替换后的结果写入我们的优化目标函数。令sita0&#x3D;0意味着决策边界必须通过原点(0,0)。下图是支持向量机选择不同的决策边界的情况。（向量sita一定是垂直于决策边界的），支持向量机通过让间距变大，使得p(i)变大，以至于输出一个较小的sita的范数。（为了简便都令sita0&#x3D;0，即使不为0，效果也不变。支持向量机仍然会找出正样本和负样本之间大间距分隔）\n\n\n04 核函数1改造支持向量机算法来构造复杂的非线性分类器。\n\n\n希望拟合一个非线性的判别边界来区分正负实例。一种方法是构造一个复杂多项式特征的集合，在这里我们用f1、f2、f3来表示这些我们将要计算的新的特征变量。\n\n\n\n构造新特征f1、f2、f3的方法：首先手动选取三个点（标记），接着将新特征定义为一种相似度的度量即度量训练样本x与标记的相似度（用下面公式表达）。相似度函数就是一个核函数（这里是高斯核函数）。\n\n\n\n对于这个核函数取两种情况：一种是x与标记点很近，一种是x与标记点很远。\n\n\n\n下面是核函数参数大小不同时的表现：\n\n\n\n选择不同的点，来预测y值是1还是0。可以看出离l_1和l_2近的点预测y值为1（带入下面的预测函数中如果&gt;&#x3D;0说明y&#x3D;1;&lt;0说明y&#x3D;0)。\n\n\n05 核函数2\n特征函数基本上是在描述每一个样本距离样本集中其他样本的距离，下面是这个过程的大纲。\n\n\n\n当已知参数sita时，怎样做出预测的过程。因为标记点的个数等于训练点的个数（m），所以参数向量sita为m+1维\n\n\n\n但是怎样得到参数sita？通过解决最小化的问题，你就得到了支持向量机的参数sita。\n\n\n\n在使用支持向量机时，怎么选择支持向量机中的参数C？–C相当于正则化参数的倒数，根据需求选择C。高斯核函数中的参数？–当高斯核函数中的参数相对较大时，图像将会倾向于变得相对平滑，可能会带来较高的偏差和较低的方差；当高斯核函数中的参数相对较小时，图像弧度将会相对大，可能会带来低偏差和较高的方差。（图像跟第四节倒数第二个相配套）\n\n\n06 使用SVM高度优化好的软件库：liblinear、libsvm\n\n\n我们虽然不用自己写SVM优化库，但是还是有几件事需要我们做：\n参数C的选择\n选择内核参数或者想要使用的相似函数\n\n\n对于内核函数其中第一个选择是不需要任何内核参数，没有内核参数的理念又叫线性核函数。为什么想要做这件事呢？如果有大量的特征n，而训练集m很小，也许就想拟合一条线性的判定边界，而不去拟合一条复杂的非线性函数。因此选择线性核函数可能很合适。\n对于内核函数其中第二个选择是可以构造个高斯内核函数。n很少，m很多，使用高斯内核函数可能很合适。\n\n\n\n如果选择要用高斯核函数，接下来要做的事：\n根据使用的支持向量机软件包可能需要实现一个核函数或者相似函数。因此如果使用Octave来实现支持向量机的话，那么就需要提供一个函数来计算核函数的特征值，它将自动的生成所有特征变量。因此对应一个i需要计算f_i。\n如果有大小很不一样的特征变量，要在使用高斯核函数之前，将这些特征变量的大小按比例归一化。下面以计算x与l之间的范数为例，如果第一特征房子面积特别大，第二特征房间个数在1到5之间，那么间距可能都是由特征一所决定，所以需要归一化。\n\n\n\n\n\n默塞尔定理是确保所有的SVM包能够用大类的优化方法并可以快速得到参数sita。其他核函数也都满足默塞尔定理，如下所示：多项式核函数（一个是自己添加的值可以是0、1、3…，还有一个是指数可以修改）。\n\n\n\n在多类分类中，输出在多个类别中恰当的判定边界。如果有k个类别用以将每个类别从其他的类别中区分开来。例如第一个参数sita_1,y&#x3D;1作为正类别，其他作为负类别得到的，以此类推。\n\n\n\n逻辑回归开始构造了SVM，然后更改下代价函数。如果n相对于m足够大，那么我通常使用逻辑回归或者线性核函数；如果n很小，m适中，通常使用高斯核函数的SVM比较好；当n很小m很大，建议手动地创建拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。这种情况不适用神经网络，因为运行会很慢，而且SVM不用去考虑局部最优的问题。\n\n\n"},{"title":"当你的浏览器中地址栏输入地址并回车的一瞬间到页面能够展示回来，经历了什么？","url":"/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E5%BD%93%E4%BD%A0%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E5%9C%B0%E5%9D%80%E6%A0%8F%E8%BE%93%E5%85%A5%E5%9C%B0%E5%9D%80%E5%B9%B6%E5%9B%9E%E8%BD%A6%E7%9A%84%E4%B8%80%E7%9E%AC%E9%97%B4%E5%88%B0%E9%A1%B5%E9%9D%A2%E8%83%BD%E5%A4%9F%E5%B1%95%E7%A4%BA%E5%9B%9E%E6%9D%A5%EF%BC%8C%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"（1）浏览器本身是一个客户端，当你输入URL的时候，首先浏览器会去请求DNS服务器，通过DNS获取相应的域名对应的IP（2）然后通过IP地址找到IP对应的服务器后，要求建立TCP连接（3）浏览器发送完HTTP Request（请求）包后，服务器接收到请求包之后才开始处理请求包（4）在服务器收到请求之后，服务器调用自身服务，返回HTTP Response（响应）包（5）客户端收到来自服务器的响应后开始渲染这个Response包里的主体（body），等收到全部的内容随后断开与该服务器之间的TCP连接。\n"},{"title":"请你谈谈网站是如何进行访问的","url":"/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E8%AF%B7%E4%BD%A0%E8%B0%88%E8%B0%88%E7%BD%91%E7%AB%99%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E7%9A%84/","content":"\n输入一个域名；回车 \n检查本机的 C:\\Windows\\System32\\drivers\\etc\\hosts配置文件下有没有这个域名映射；\n\n\n有：直接返回对应的ip地址，这个地址中，有我们需要访问的web程序，可以直接访问\n没有：去DNS服务器找，找到的话就返回，找不到就返回找不到；\n\n\n"}]