[{"title":"机器学习 day01初识机器学习","url":"/2021/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day01%E5%88%9D%E8%AF%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","content":"01 监督学习\n监督学习是指我们给算法一个数据集，其中包含了正确的答案。算法的目的就是给出更多的正确答案。\n回归是指我们设法预测连续值的属性，可以应用在预测房子价格等方面。\n分类是指我们设法预测离散值的输出(0或1)，可以应用在判断账户是否被入侵等方面。\n\n02 无监督学习\n无监督学习也会给一个数据集，但是数据集不包括正确答案(里面的数据要么都有相同的标签要么都没有标签)。无监督学习会将数据分为一个个不同的簇，这就是聚类算法。\n聚类算法可以应用在|搜集新闻并将相关新闻组合在一起|星系形成理论|市场销售等领域\n\n\n建议使用Octave免费开源的软件，许多学习算法都可以用几行代码将其实现。例如svd函数(奇异值分解)已经作为线性代数常规函数内置在Octave中了。\n","tags":["机器学习"]},{"title":"机器学习 day02单变量线性回归","url":"/2021/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day02%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","content":"01 模型描述为了更好的描述监督学习问题，需要给出训练集并以此构建一个模型。\n\n下面先学习几个符号：\n\nm:代表的是训练集有几个\nx:代表的是输入的特征\ny:代表的是输出，也就是预测的目标变量\nh:代表假设函数，引导从x得到y的函数\n\n02 代价函数（平方误差函数）\n可以通过代价函数来衡量假设函数的准确性。\n\n代价函数取值越小，假设函数就越准确。\n\n\n\n\n代价函数有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。\n\n在线性回归中，我们要解决的是最小化问题\n\n代价函数是解决回归问题最常用的手段\n\n\n\n03 梯度下降梯度下降法：可以将代价函数最小化\n\n一般将两个参数初始化为0，再不断的改变参数的值，使得代价函数取值达到最小。\n\n\na：代表学习率（永远为正），用来控制使用梯度下降时，迈出步子的大小。下面是a太小与太大的情况。\n\n\n\n导数的含义：针对于只有一个参数的，两种不同初始点的情况分析。\n\n\n\n事实上，当取值越接近最优解时，梯度下降的幅度也就越小，因为导数始终向0靠拢。\n\n\n04 线性回归的梯度下降\n将代价函数带入梯度下降算法，并求偏导数可得：\n\n\n\n将所求的偏导数带回梯度下降算法\n\n\n\n通过梯度下降算法，一步步的进行，最终可以得到与数据最拟合的直线\n\n\n","tags":["机器学习"]},{"title":"机器学习 day03线性代数基础","url":"/2021/07/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day03%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80/","content":"01 矩阵与向量\n小写字母表示向量：a b c\n大写字母表示矩阵：A B C\n\n02 加法和标量乘法\n只有维度相同的矩阵才能相加\n一个数乘一个矩阵与一个矩阵乘一个数的结果相同\n\n\n03 矩阵向量乘法\n矩阵与向量相乘\n\n\n\n例子：\n\n\n\n将一个方程转化为矩阵向量相乘的形式\n\n\n04 矩阵相乘\n矩阵相乘计算原理：只需将第二个矩阵的每一列提取出来，计算矩阵与向量相乘，最终将其拼接成为最终答案。\n\n\n\n将三个方程转化为两个矩阵相乘的形式\n\n\n05 矩阵乘法特性\n不能使用乘法交换律（单位矩阵除外），会改变结果的维度及数值\n\n\n\n矩阵与实数一样，都符合乘法结合律\n\n\n06 逆和转置\n只有方阵才有逆矩阵\n\n\n\n矩阵的转置：将A的第一行变成A的转置的第一列，将A的第二行变成A的转置的第二列。。。。。。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day04多变量线性回归","url":"/2021/07/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day04%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","content":"01 多元线性回归–多特征向量情况下的假设形式\n一些符号表示：\n\n\n\n简化下面等式的表达方法：向量内积转化\n\n\n02 多元梯度下降算法\n多元线性回归方程+代价函数+梯度下降函数\n\n\n\n单元及多元线性回归的梯度下降法对比\n\n\n03 多元梯度下降法–特征缩放\n特征缩放的目的只是为了运行更快。使特征值比较接近，使图像变得比较圆。以至于梯度下降的速度更快，收敛所需要的迭代次数更少，收敛更快。缩放前后对比图如下：\n\n\n\n特征值的取值别太大也别太小，与下面这个范围足够接近最好。\n\n\n\n均值归一化的工作：X &#x3D;（当前值-平均值）&#x2F;【（最大值-最小值）只要是这个范围左右就可以】\n\n\n04 多元梯度下降法–学习率\n梯度算法正常工作图如下：代价函数随迭代次数的变化，最终收敛。\n\n\n\n如果所得图像不是一直减小的，那么需要减小学习率，当然学习率也不能过小，否则梯度下降将会十分缓慢，迭代次数无限增加。\n\n\n\n得到一个不错的学习率：按照三的倍数来取值，尝试一系列的学习率，找到个太小的值，再找到另一个太大的值，然后取太大的值，或者比太大的值略小的比较合理的值\n\n\n05 特征和多项式回归\n特征可以根据自己的需求选择合适的特征，例如将两个不同的特征相乘得到一个新的特征\n如果只用多次函数，适当使用特征缩放将起到很好的效果：\n\n\n\n可以有多种合理的选择，比如也可以是平方根。\n\n\n06 正规方程–区别于迭代方法的直接解法\n正规方程：对代价函数求偏导数，并将其置0，就可以得到使代价函数最小的值。\n\n\n\n方程的形式及例子：\n\n\n\n使用正规方程就不用对特征进行缩放了。\n\n选择合适的算法（梯度下降还是正规方程）一般特征&lt;10000时选用正规方程直接求解，他们二者的优缺点：\n\n\n\n07 正规方程在矩阵不可逆情况下的解决方法\n在Octave中pinv（伪逆）与inv（逆）是求逆矩阵的，就算矩阵没有逆，pinv也会求出它的逆。\n\n首先看是否有多余的特征（两个特征线性相关），选择进行删除，直到没有多余的为止；再观察是否特征过多，选择没有影响的特征进行删除。\n\n\n\n","tags":["机器学习"]},{"title":"机器学习 day05Octave教程","url":"/2021/07/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day05Octave%E6%95%99%E7%A8%8B/","content":"01 基本操作\n注释表达：%\n&gt;&gt; 1~= 2 %注释ans = 1\n\n\n\n不等号表达：~&#x3D;\n&gt;&gt; 1~= 2ans = 1\n\n\n\n隐藏提示命令：PS1(‘&gt;&gt; ‘);\noctave:6&gt; PS1(&#x27;&gt;&gt; &#x27;);&gt;&gt;\n\n\n\n想分配一个变量，但是不想在屏幕上显示结果：只需在结尾加分号（;）\n&gt;&gt; a = pi;&gt;&gt; a = pia = 3.1416\n\n\n\n显示命令：disp( );\n&gt;&gt; disp(a);3.1416\n\n用disp( );显示字符串\n&gt;&gt; disp(sprintf(&#x27;2 decimals:%0.2f&#x27;,a)) %0.2f:小数点后两位2 decimals:3.14\n\n显示更多的小数点后几位：format long\n&gt;&gt; format long&gt;&gt; aa = 3.141592653589793\n\n\n\n显示少量的小数点后几位：format short\n&gt;&gt; format short&gt;&gt; aa = 3.1416\n\n\n\n建立一个矩阵：\n\n\n&gt;&gt; A = [1 2;3 4;5 6]A =   1   2   3   4   5   6&gt;&gt; A = [1 2;&gt; 3 4;&gt; 5 6]A =   1   2   3   4   5   6\n\n\n\n\n建立向量：\n\n&gt;&gt; v = [1 2 3] %行向量v =   1   2   3&gt;&gt; v = [1; 2; 3] %列向量v =   1   2   3&gt;&gt; v = 1:0.1:2 %从1开始每次增加0.1到2的行向量v =    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000    1.8000    1.9000    2.0000&gt;&gt; V = 1:6 %从1到6的整数V =   1   2   3   4   5   6\n\n\n生成单位矩阵\n\n&gt;&gt; ones(2,3) %2×3的ans =   1   1   1   1   1   1\n\n\n生成零矩阵\n\n&lt;&lt; zeros(2,3)ans =   0   0   0   0   0   0\n\n\n随机生成0到1的值\n\n&lt;&lt; rand(3,3)ans =   0.914686   0.131127   0.563647   0.296402   0.590713   0.134373   0.243013   0.477658   0.071331\n\n\n生成服从高斯分布的随机数：均值为0，标准差或者方差为1\n\n&lt;&lt;  w = randn(1,3)w =  -0.1177   2.0111   0.7261\n\n\n绘制直方图：hist( ):均值为-6，方差为10，标准差为根号10\n\n\n\n生成单位矩阵：eye( )\n\n&lt;&lt; eye(4)ans =Diagonal Matrix   1   0   0   0   0   1   0   0   0   0   1   0   0   0   0   1\n\n\n显示帮助：help eye;help help;按q退出\n\n显示矩阵大小:size( )\n\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt; size(A)ans =   3   2&lt;&lt; size(A,1) %显示行数ans = 3&lt;&lt; size(A,2) %显示列数ans = 2\n\n\n返回最大维度大小：length( )|一般对向量使\n\n&lt;&lt; length(A)ans = 3 %3×2，最大维度为3&lt;&lt; length([1;2;3;4;5])ans = 5\n\n02 移动数据\n显示当前在内存中存储的所有变量：who\nwhos显示更加完善的信息\n\n&lt;&lt; whoVariables visible from the current scope:A    ans&lt;&lt; whosVariables visible from the current scope:variables in scope: top scope   Attr Name        Size                     Bytes  Class   ==== ====        ====                     =====  =====        A           3x2                         48  double        ans         1x10                        10  charTotal is 16 elements using 58 bytes\n\n\n载入文件格式：load+文件；load(‘文件’)\n删除某个变量：clear+变量\n存储数据到硬盘中：save+存储文件名+数据\n在找矩阵某一个元素时，：表示这一行或者这一列的元素\n\n&lt;&lt; A(2,1)ans = 3&lt;&lt; A(2,:)ans =   3   4&lt;&lt; A(:,2)ans =   2   4   6\n\n\n索引操作：同时取得1、3行所有元素\n\n&lt;&lt; A([1 3],:)ans =   1   2   5   6\n\n\n将第二列用其他元素代替\n\n&lt;&lt; A(:,2) = [10; 11; 12]A =    1   10    3   11    5   12    \n\n\n在A的右侧附加一列\n\n&lt;&lt; A = [A,[100; 200; 300]]A =     1    10   100     3    11   200     5    12   300\n\n\n把A所有元素放在单独的一列\n\n&lt;&lt; A(:)ans =     1     3     5    10    11    12   100   200   300\n\n\n将两个矩阵结合在一起\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt;  B = [11 12; 13 14; 15 16]B =   11   12   13   14   15   16&lt;&lt; C = [A B] %左右结合C =    1    2   11   12    3    4   13   14    5    6   15   16&lt;&lt; C = [A;B] %上下结合C =    1    2    3    4    5    6   11   12   13   14   15   16\n\n03 计算数据\n两个矩阵相乘\n\n&lt;&lt; A = [1 2; 3 4; 5 6]A =   1   2   3   4   5   6&lt;&lt;  B = [11 12; 13 14; 15 16]B =   11   12   13   14   15   16&lt;&lt; C = [1 1;2 2]C =   1   1   2   2&lt;&lt; A*Cans =    5    5   11   11   17   17\n\n\n两个矩阵对应数相乘\n\n&lt;&lt; A.*Bans =   11   24   39   56   75   96\n\n\nA的每个元素进行乘方\n\n&lt;&lt; A.^2ans =    1    4    9   16   25   36\n\n\n求V对应元素的倒数\n\n&lt;&lt; V = [1; 2; 3]V =   1   2   3&lt;&lt; 1 ./Vans =   1.0000   0.5000   0.3333\n\n\n以e为底，v中元素为指数的幂运算\n\n&lt;&lt; exp(V)ans =    2.7183    7.3891   20.0855\n\n\n求绝对值：abs( )\n求相反数直接加-；例如-V\n将V中每个元素+1\n\n&lt;&lt; V + ones(length(V),1) %与V +1等价ans =   2   3   4\n\n\n求A的转置\n\n&lt;&lt; A&#x27;ans =   1   3   5   2   4   6\n\n\n求X最大的数及其索引\n\n&lt;&lt; X = [1 15 2 0.5]X =    1.0000   15.0000    2.0000    0.5000&lt;&lt; val = max(X)val = 15&lt;&lt; [val,ind] = max(X)val = 15ind = 2%如果A是矩阵，那么将求出每一列最大值&lt;&lt; max(A)ans =   5   6\n\n\nX中每个元素与3比较，根据结果返回真和假\n\n&lt;&lt; X&lt;3ans =  1  0  1  1\n\n\n找到比3小的数并返回其索引\n\n&lt;&lt; find(X&lt;3)ans =   1   3   4\n\n\n生成一个幻方矩阵，每行、每列、每个对角线加起来都等于一个数\n\n&lt;&lt; B = magic(3)B =   8   1   6   3   5   7   4   9   2\n\n\n找出B中&gt;&#x3D;7的元素，并返回下标\n\n&lt;&lt; [r,c] = find(B &gt;=7)r =   1   3   2c =   1   2   3\n\n\nX中所有元素相加\n\n&lt;&lt; sum(X)ans = 18.500\n\n\nX中所有元素相乘\n\n&lt;&lt; prod(X)ans = 15\n\n\n对X中元素向下取整\n\n&lt;&lt; floor(X)ans =    1   15    2    0\n\n\n对X中元素向上取整\n\n&lt;&lt; ceil(X)ans =    1   15    2    1\n\n\n取两个随机矩阵中较大的数组合成一个较大值的矩阵\n\n&lt;&lt;  max(rand(3),rand(3))ans =   0.6957   0.9634   0.7179   0.8404   0.9784   0.8319   0.5236   0.8063   0.6697\n\n\n取每一列&#x2F;行最大值\n\n&lt;&lt; max(B,[],1)ans =   8   9   7&lt;&lt; max(B,[],2)ans =   8   7   9\n\n\n求每一行、每一列、对角线相加\n\n&lt;&lt; C = magic(9)C =   47   58   69   80    1   12   23   34   45   57   68   79    9   11   22   33   44   46   67   78    8   10   21   32   43   54   56   77    7   18   20   31   42   53   55   66    6   17   19   30   41   52   63   65   76   16   27   29   40   51   62   64   75    5   26   28   39   50   61   72   74    4   15   36   38   49   60   71   73    3   14   25   37   48   59   70   81    2   13   24   35&lt;&lt; sum(C,2)ans =   369   369   369   369   369   369   369   369   369&lt;&lt; sum(C,1)ans =   369   369   369   369   369   369   369   369   369&lt;&lt; sum(sum(C.*eye(9))) %正对角线相加ans = 369&lt;&lt; sum(sum(C.*flipud(eye(9)))) %副对角线相加，flipud表示使矩阵垂直翻转ans = 369\n\n04 数据绘制\n绘制一个正弦函数图像：plot( );\n\n&lt;&lt; t = [0:0.01:0.98];&lt;&lt; y1 = sin(2*pi*4*t);&lt;&lt; plot(t,y1);\n\n\n\n绘制一个余弦函数图像：plot( );\n\n&lt;&lt; y2 = cos(2*pi*4*t);&lt;&lt; plot(t,y2);\n\n\n\n在旧的图像上面绘制新的图像：hold on;\n\n&lt;&lt; t = [0:0.01:0.98];&lt;&lt; y1 = sin(2*pi*4*t);&lt;&lt; y2 = cos(2*pi*4*t);&lt;&lt; plot(t,y1);&lt;&lt; hold on;&lt;&lt; plot(t,y2,&#x27;r&#x27;);\n\n\n\n加上横与纵轴的标签，标记两条函数，输入标题，并保存到桌面\n\n&lt;&lt; xlabel(&#x27;time&#x27;)&lt;&lt; ylabel(&#x27;value&#x27;)&lt;&lt; legend(&#x27;sin&#x27;, &#x27;cos&#x27;)&lt;&lt; title(&#x27;my plot&#x27;)&lt;&lt; cd &#x27;C:\\Users\\1\\Desktop&#x27;; print -dpng &#x27;plot.png&#x27;\n\n\n\n\n为不同图像标号：就可以同时有两个图像\n\n&lt;&lt; figure(1); plot(t,y1);&lt;&lt; figure(2); plot(t,y2);\n\n\n\n将一个界面分为两个格子，其中正弦占第一个，余弦占第二个\n\nsubplot(1,2,1);&lt;&lt; plot(t,y1);&lt;&lt; subplot(1,2,2);&lt;&lt; plot(t,y2);\n\n\n\n设置横竖轴的范围\n\n&lt;&lt; plot(t,y1);&lt;&lt; axis([0.5 1 -1 1])\n\n\n\n可视化矩阵\n\n&lt;&lt; A = magic(5)A =   17   24    1    8   15   23    5    7   14   16    4    6   13   20   22   10   12   19   21    3   11   18   25    2    9&lt;&lt; imagesc(A)\n\n\n\n生成颜色图像、灰度分布图并在右边加入一个颜色分布\n\n&lt;&lt; imagesc(A), colorbar, colormap gray;\n\n\n\n添加环境路径，找文件时，即使不在文件环境，也可以使用其中的函数\n\naddpath(&#x27;C:\\Users\\1\\Desktop&#x27;)\n\n05 控制语句\noctave可以返回多个返回值\n\n函数定义\n\n\nfunction J = costFunctionJ(X,y, theta)  m = size(X,1);  predictions = X*theta;  sqrError = (predictions-y).^2;  J = 1/(2*m) * sum(sqrError);\n\n\n函数使用\n\n&lt;&lt; addpath(&#x27;C:\\Users\\1\\Desktop&#x27;)&lt;&lt; X = [1 1; 1 2; 1 3]X =   1   1   1   2   1   3&lt;&lt; y = [1; 2; 3]y =   1   2   3&lt;&lt; theta = [0;1];&lt;&lt; J = costFunctionJ(X,y, theta)J = 0&lt;&lt; theta = [0;0];&lt;&lt; J = costFunctionJ(X,y, theta)J = 2.3333&lt;&lt; (1^2+2^2+3^2)/(2*3)ans = 2.3333\n\n06 矢量\n非向量化与向量化的代码对比\n\n\n\n用C++语言及C++线性库所写代码\n\n\n\n向量化\n\n\n","tags":["机器学习"]},{"title":"机器学习 day06Logistic回归","url":"/2021/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day06Logistic%E5%9B%9E%E5%BD%92/","content":"01 分类\n针对于离散值来进行分类：y &#x3D; {0，1}\n0表示负类，没有什么东西\n1表示正类，有什么东西\n不建议将线性回归函数应用于分类情况中\n使用线性回归在分类问题，如果一个值远离其他值，将会使线性回归算法不够准确。\n\n\n\nLogistic回归算法的预测值一直介于0和1之间，并不会像线性回归算法大于1或者小于0\n\n02 假设陈述\n假设陈述：当有一个分类问题的时候，我们要使用哪个方程来表示我们的假设。\nLogistic函数的形式如下：对线性回归方程稍作修改。\n\n\n\n输出某个数字，我们会把这个数字当作对一个输入x，y&#x3D;1的概率估计\n\n\n03 决策界限\n决策界限可以帮助我们理解Logistic回归的假设函数在计算什么。\n可以从图看出什么时候预测y &#x3D; 1;什么时候预测y &#x3D; 0;\n\n\n\n决策边界将一个平面划分为两个区域，其中一片区域假设函数预测y &#x3D; 1；另一片区域假设函数预测y &#x3D; 0。只要我们确定好了参数，我们就将完全确定决策边界。例如下图所示：可以得出直线 X1 + X2 &#x3D; 3就是决策边界。\n\n\n\n例题：我们怎么才能使用Logistic回归来拟合这些数据呢？多项式回归及线性回归可以在特征中添加额外的高阶多项式，Logistic回归也可以使用。\n\n\n\n决策边界不是训练集的属性，而是假设本身及其参数的属性，只要给定了参数向量就可以确定决策边界\n\n04 代价函数如何拟合Logistic回归模型的参数。当代价函数为0时，可以得出与预测值想拟合。\n\n\n如果将代价函数带入到Logistic回归中可以得到左侧图像非凸函数，可是我们想要得到右侧这样得凸函数。\n\n\n\n因此我们需要重新找到个代价函数可以用在Logistic回归中，保证找到全局最小值。下面使y &#x3D; 1情况下。\n\n\n\n下面使y &#x3D; 0情况下\n\n\n05 简化代价函数与梯度下降\n简化后得代价函数\n\n\n\n式子是在统计学中得极大似然法得来得，他是统计学中为不同模型快速寻找参数得方法。同时他是凸的。\n\n\n\n如何最小化代价函数：使用梯度下降算法。虽然Logistic回归中梯度下降算法与线性回归中的梯度下降算法长的一样，但是由于假设的定义发生了变化，所以实际上是两种截然不同的。\n\n\n06 高级优化\n一些高级算法的优缺点\n\n\n\n自动求使代价函数最小的参数，使用代码将其实现\n\n\n\n写一个函数，他能返回代价函数值以及梯度值\n\n\n07 多元分类：一对多使用逻辑回归来解决多类别分类问题\n\n\n训练一个逻辑回归分类器，预测i类别y &#x3D; i的概率。在三个分类器中输入x，在其中选择h(x)最大的那个类别。也就是选择出三个里面可信度最高，效果最好的的哪个分类器。\n\n\n\n无论i是多少，我们都能得到一个最高的概率值，我们预测y就是那个值。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day07正则化","url":"/2021/07/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day07%E6%AD%A3%E5%88%99%E5%8C%96/","content":"01 过拟合问题正则化可以减少过度拟合问题\n\n1.1  线性回归过拟合问题\n欠拟合 | 刚好合适 | 过拟合\n\n\n\n过度拟合问题将会在变量过多的时候出现，这时训练出的假设能够很好的拟合训练集（所以代价函数实际上可能非常接近于0。或者恰好等于0），但是可能会得到图三这样的曲线，去拟合训练集，以至于它无法泛化到新的样本中。\n泛化是指一个假设模型应用到新样本的能力\n\n1.2  逻辑回归过拟合问题\n欠拟合 | 刚好合适 | 过拟合\n\n\n\n如果我们有过多的特征变量而只有少量的训练集就会出现过拟合问题。\n有两种方法解决过拟合问题：\n尽量减少特征变量的数量（模型选择算法会自动选择哪些变量保留，哪些舍弃）\n正则化：减少量级或者参数的大小\n\n\n\n02 代价函数\n正则化将多阶函数变成二阶函数（将参数尽可能减小），这些参数越小，我们得到的图像也就越圆滑越简单。\n\n\n\n一般来说我们只对参数1以及1之后的进行正则化\n在对代价函数进行修改，添加正则化项的目的是为了缩小参数的值。\n正则化参数是为了控制两个不同目标之间的取舍\n正则化参数如果过大，那么参数都会接近于0，这样就相当于把假设函数的全部项都忽略了，最终变成了欠拟合。\n\n\n03 线性回归的正则化\n线性回归正则化的梯度下降法\n\n\n\n线性回归正则化的正规方程法\n\n\n\nm&lt;&#x3D;n，说明矩阵是不可逆的，但是当正则参数&gt;0，算出来的矩阵一定是可逆的。\n\n\n04 Logistic 回归的正则化\nLogistic 回归添加正则化项\n\n\n\nLogistic 回归的正则化的梯度下降法\n\n\n\n如何在更高级的算法中使用正则化：定义一个costFunction函数，以theta作为输入。在fminunc函数中括号里写上@cosFunction。\nfminunc的意思是函数在无约束条件下的最小值，fminunc函数会将costFunction函数最小化，\n\n\n","tags":["机器学习"]},{"title":"机器学习 day08神经网络学习","url":"/2021/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day08%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/","content":"01 非线性假设为什么已经有线性回归和逻辑回归算法了，还要学习神经网络？\n\n\n因为有特别多的特征，许多机器学习都需要学习复杂的非线性假设。如果使用逻辑回归算法，由于项数过多，可以能会导致过拟合问题，此外也存在运算量过大的问题。如果项数只包括二次项的的子集，这样将二次项的数量减少到100个，但是最有可能拟合出右下角椭圆而拟合不出左上角复杂的分界线。\n\n\n\n下图车子例子所示：如果是一张50 * 50 像素的图像， 则会有 50 * 50 &#x3D; 2500个像素单位（如果是彩色，每个像素又有0-255的RGB取值。即有 2500 * 3 &#x3D; 7500），特征数量则有约 n^2 &#x2F; 2 约 3000000 个特征数量。\n\n\n02 神经元与大脑神经网络能够很好的解决不同的机器学习问题\n\n\n神经网络的起源及发展\n\n\n\n神经元是一个计算单元，它从输入通道接受一定数量的信息，并做一些计算，然后将结果通过它的轴突传送到其他节点，或者大脑中其他神经元。\n\n\n03 模型展示\n神经网络模拟了大脑中的神经元或者神经网络。\n在神经网络里我们将使用一个很简单的模型来模拟神经元工作，我们将神经元模拟成一个逻辑单元。黄色代表类似于神经元细胞体的东西，经过“输入”-&gt;“计算”-&gt;“输出”三个步骤，因为X0（偏置单元或偏置神经元）总是1，会根据实际情况判断时候加上X0。\n在神经网络中激活函数是指非线性函数g(z)。单个神经元图如下：\n\n\n\n在神经网络中第一层叫做输入层，因为我们在这一层输入特征；第二层叫做隐藏层（任何一个非输入层和非输出层），隐藏层的值在训练中是看不到的；最后一层叫输出层，因为在这一层输出假设的最终计算结果；\n\n\n\nai(j)代表第j层第i个神经元或者单元的激活项，激活项是由一个具体神经元计算并输出的值。参数(j)就是权重矩阵，它控制从某一层到另外一层的映射。计算三个隐藏单位的值及输出如下：\n\n\n\n如何高效进行计算，并展示一个向量化的实现方法。\n\n前向传播方法\n\n\n\n下面这个神经网络所作的事情就像是逻辑回归，它不是以原本的X1、X2、X3作为特征，而是用a1、a2、a3作为新的特征\n\n\n04 例子与直觉理解神经网络计算复杂非线性函数的输入\n\n\nX1 AND X2运算\n\n\n\nX1 OR X2运算\n\n\n\nNOT X1\n\n\n\nX1 XNOR X2\n\n\n05 多元分类\n要是在神经网络中实现多类别分类，采用的方法本质是一对多法的拓展（其中Xi代表图像，Yi代表那些向量）\n\n\n","tags":["机器学习"]},{"title":"机器学习 day09神经网络参数的反向传播算法","url":"/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day09%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","content":"01 代价函数\n有m组训练样本，L代表神经网络结构的总层数，S_l代表第L层的单元数也就是神经元的数量（不包括第L层的偏差单元）。其中二元分类与多类别分类问题如下：\n\n\n\n应用于神经网络的代价函数：h(x)是一个k维向量，h(x)_i代表第i个输出；k的求和符号应用于y_k和h_K,是因为我们主要是将第k个输出单元的值和y_k的值的大小作比较；y_k的值就是这些向量中其应属于哪个类的量。\n\n\n02 反向传播算法反向传播算法是计算代价函数关于所有参数的导数或者偏导数的一种有效方法。\n\n\n使用前向传播方法来计算的顺序，计算一下在给定输入的时候，假设函数是否会真的输出结果。\n\n\n\n反向传播算法中，下图上方下标j上标（l)代表了第l层的第j个结点的误差，下图上方下标j上标（l)实际上就是假设的输出值和训练集y值之间的差。反向传播算法类似于把输出层的误差反向传播给了第三层，然后再传播给第二层，注意没有第一层（第一层可以直观的观察到，没有误差）。\n\n\n\n如何实现反向传播算法来计算这些参数的偏导数：\n\n首先将每一个i和j对应的三角形（三角形是上图上方下标j上标（l)的大写）置0\n接下来遍历整个训练集，将输入层的激活函数设定他为第i个训练样本的输入值\n接下来用正向传播来计算第二层的激活值，然后第三层，最后到最后一层\n使用输出值来计算这个输出值对应的误差项（假设输出-目标输出）\n再通过反向传播算法计算前几层的误差项，一直到第二层\n最后通过三角形来累计我们再前面写好的偏导数项\n\n\n跳出循环后，通过下面的式子计算D(j等于0和j不等于0的情况)，计算出来的D正好就是关于每个参数的偏导数，然后可以用梯度下降法或者一些其他的高级优化算法。\n\n\n\n03 理解反向传播\n理解前向传播\n\n\n\n代价函数应用在只有一个输出单元的情况\n\n\n\n理解反向传播：代价函数是一个关于标签y和神经网络中h(x)的输出值的函数，只要稍微将z(l)j改一下，就会影响神经网络的h(x)，最终改变代价函数的值。\n\n\n04 使用注意：展开参数把参数从矩阵展开向量，以便在高级最优化步骤中的使用需要\n\n\n高级最优化算法都假定theta和initialTheta初始值都是参数向量，也许是n或者n+1维，同时假定这个代价函数的第二个返回值(梯度值)也是n维或者n+1维向量。但是现在在神经网络，参数不再是向量而是矩阵，三个参数在Octave表达如下；梯度矩阵在Octave表达也如下：\n\n\n\n取出矩阵，并将其展开成向量传入theta中，并得到梯度返回值。\nthetaVec就是将这些矩阵全部展开成为一个很长的向量；DVec同理。reshape将相应元素组合起来成相应矩阵。\n\n\n\n上面步骤通过Octave实现如下：\n\n&lt;&lt;Theta1 = ones(10,11)Theta1 =   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1&lt;&lt;Theta2 = 2*ones(10,11)Theta2 =   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2&lt;&lt;Theta3 = 3*ones(1,11)Theta3 =   3   3   3   3   3   3   3   3   3   3   3&lt;&lt;thetaVec = [ Theta1(:);Theta2(:);Theta3(:)];&lt;&lt;size(thetaVec)ans =   231     1&lt;&lt;reshape(thetaVec(1:110),10,11)ans =   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1&lt;&lt;reshape(thetaVec(111:220),10,11)ans =   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2&lt;&lt;reshape(thetaVec(221:231),1,11)ans =   3   3   3   3   3   3   3   3   3   3   3\n\n\n将这一方法应用于我们的学习算法\n\n\n05 梯度检测因为反向传播使用时会出现一些bug，而梯度检测可以很好的解决这些问题，确保前向传播及反向传播都百分百正确。\n\n\n求出该点导数的近似值（参数是实数的情况）\n\n\n\n当参数维向量参数的时候\n\n\n\n在Octave中为了估算导数所要实现的\n\n\n\n总结下如何实现数值上的梯度检验：（注意反向传播算法比梯度检测效率高，检测完一定要关闭梯度检测）\n\n\n06 随机初始化\n在神经网络中将所有参数初始为0，没有任何意义，所有输入都是一样，也就意味这最后输出就输出一个特征，阻挡了神经网络学习任何有趣的东西，我们称之为高度冗余。\n\n\n\n因此就应该使用随机初始化方法。值得注意的是这里的EPSILON与梯度检测中的完全没有关系。\n\n\n\n为了训练神经网络首先将权重随机初始化为一个接近0范围在-EPSILON到EPSILON之间，然后进行反向传播，在进行梯度检测，最后梯度下降算法或其他高级优化算法来最小化代价函数（关于参数sita的函数）。\n\n07 组合到一起\n训练神经网络做的第一件事就是选择一种合适的网络架构（神经元之间的连接模式），注意输出时是输出一个向量y。\n\n\n\n训练神经网络所需要的步骤\n\n\n\n\n反向传播算法是为了算出梯度下降算法的下降方向\n\n","tags":["机器学习"]},{"title":"机器学习 day10应用机器学习的建议","url":"/2021/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day10%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/","content":"01 决定下一步做什么\n开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，应如何决定选择哪条路。不要随意选择。\n\n\n\n机器学习诊断法能够提前发现某些方法是无效的。\n\n\n02 评估假设\n将所有数据分为训练集和测试集，最经典的分割方法就是按照7:3的比例。\n\n\n\n线性回归算法和平方误差标准学习和测试学习算法，从训练集学习获得参数，在将参数带入测试集得到测试误差。\n\n\n\n训练和测试逻辑回归的步骤及用错误分类（0&#x2F;1分类错误）来定义测试误差。0&#x2F;1表示了你预测的分类是正确或错误的情况。\n\n\n03 模型选择和训练、验证、测试集\n模型选择问题（想要确定对于一个数据集最合适的多项式次数，怎样选用正确的特征来构造学习算法或者假如你需要选择学习算法中的正则化参数）\n\n模型选择问题：用不同的模型拟合数据集得到参数，接着对所有这些模型求出测试集误差，然后根据哪个模型有最小的测试误差来选择使用哪个模型。\n\n\n\n\n为了解决模型选择出现的问题，我们通常会采用如下的方法来评估一个假设。我们把数据分为三个部分，分别是训练集、验证集、测试集。分配比例分别是6:2:2。\n\n\n\n定义训练误差、交叉验证误差和测试误差\n\n\n\n用验证集选择模型而不是原来的测试集。省下来的测试集可以用它来衡量或者估算算法选择出的模型的泛化误差了。\n\n\n04 诊断偏差与方差如果一个算法表现得不理想，要么是偏差比较大，要么是方差比较大。换句话说要么欠拟合要么过拟合。\n\n\n训练误差随着我们增大多项式的次数而减小；随着我们增大多项式的次数，我们对训练集拟合的也就越好。对于验证误差来说，如果d为1，会有较大误差；如果d为中等次数大小，能够更好的拟合；当d为4时，也就可能过拟合。\n\n\n\n对于验证误差来说，左边这一端对应的就是高偏差问题；右边这一端对应的就是高方差问题。如果训练误差很小，并且验证误差远大于训练误差说明出现过拟合问题（高方差）。如果是高偏差，则训练误差和验证误差都很大。\n\n\n05  正则化和偏差、方差\n第一个图是高偏差，欠拟合；中间正合适；最后一个图是高方差，过拟合。\n\n\n\n我们对训练、验证、测试误差的定义都是平均的误差平方和，或者是不使用正则化项时，训练集、验证集和测试集的平均的误差平方和的一半。\n\n\n\n自动选择正则化参数的方法：首先选取一系列想要试用的步长，通常来说步长设为2倍速增长，直到一个比较大的值。这样就选取了12个对应的正则化参数。然后对这12个模型分别最小化代价函数，得到完全不同的参数向量。可以把这些模型用不同的正则化参数来进行拟合，然后我们可以用验证集来评价这些参数sita在验证集上的平均的误差平方和，最终选择误差最小的模型。\n\n\n\n当我们改变正则化参数时，我们的假设在训练集和验证集上的表现（对应本节第一个图）\n\n\n06 学习曲线学习曲线可以判断某一学习算法是否处于偏差或者方差问题，还是二者都有。\n\n\n当训练集个数很少的时候，能够十分完美的拟合数据，训练集误差基本为0，但是随着训练集越来越多，训练集误差也就会越来越大，逐渐趋于水平。而验证集误差，随着训练集的个数增加而减小，最终趋于水平。\n\n\n\n在高偏差的情况下，训练集误差和验证集误差最终将十分接近，再增加训练集数量将毫无意义。\n\n\n\n在高方差的情况下，总体来说随着训练集数量的增多，训练集误差将会增加，但是增加的很小。而验证集误差一直都比较高，虽然会有所下降，但是不多。所以增加训练集数量还是很有用的。\n\n\n07 决定接下来做什么\n接下来回到第一节的第一个图，1和2和6对应着高方差的情况，3和4和5对应高偏差的情况（个人理解：高方差就是在多项式的形式下出现的，高偏差就是在项数少的情况下出现的）。\n\n\n\n小型神经网络计算量少，大型神经网络比较容易出现过拟合问题（但是可以用正则化来进行解决），相对来说大型神经网络性能更好。\n还有就是选择隐含层层数的问题，可以将数据分为训练集、验证集还有测试集。用训练集分别训练一层、两层、三层的隐含层，最终用验证集来测试，选出合适的层数。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day11机器学习系统设计","url":"/2021/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day11%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","content":"01 确定执行的优先级在实际工作过程中，我们应该优先处理哪些事情\n\n\n以邮件筛选为例，选择邮件的特征向量的方法。通常我们会挑选出在训练集中出现频率最多的n个单词，将其作为特征向量。\n\n\n\n如何在有限时间里让垃圾邮件分类器具有高精准度和低错误率。\n用更复杂的特征变量来描述邮件（可以在邮件标题中获取复杂的特征，来捕捉这封邮件的来源，以此判断是否为垃圾邮件）。\n关注邮件的正文，并构建更复杂的特征。\n来检测单词是否故意出现拼写的错误\n\n\n\n\n02 误差分析误差分析就是一种手动地去检查算法所出现的失误的过程，走向最有成效的道路。\n\n\n通过手动检查分类错误的邮件，来看哪一类分类错误的多，哪一个出现错的情况最多，就着重去构造这类特征，加以训练。\n\n\n\n交叉验证错误率：单一规则的数值评价指标。\n如果只是手动地去检查看看这些例子表现得好不好，会让你很难去决定到底应不应该做出某种决定；但是通过交叉验证错误率就可以直观的看误差率是变大还是变小了，他能告诉你你的想法是提高了还是降低。\n\n\n\n一旦有了一个初始的算法实现，我们就能使用一个强有力的工具，来帮助决定下一步应该做什么：\n看看他所造成的错误：通过误差分析来看看它出现了什么失误，然后以此决定之后的优化方法。\n如果已经有了一个简单粗暴算法实现，又有一个数值评价指标，这些能帮助来试验新的想法，能够快速观察是否能够提高算法的表现，决定应该包含什么，应该舍弃什么。\n\n\n\n03 不对称性分类的误差评估当有倾斜类问题时，使用准确率与召回率来评价学习算法要比用分类误差或者分类准确率好得多。\n\n\n偏斜类：一个类中的样本数与另一个类中的数据相比多很多（比如，没有肿瘤的比有肿瘤的要多得多）。所以说恒把y&#x3D;0算出来的误差将会很小，因为有肿瘤的人很少。\n\n\n\n所以我们想要一个不同的评估度量值：查准率和召回率。其中查准率是指对于所有我们的预测，患有癌症的病人，有多大比率的病人是真正患有癌症的。召回率是指假设如果测试集或者验证集中的病人确实得了癌症，有多大比率正确预测他们得了癌症。也就是如果所有病人都得了癌症，有多少人我们能够正确告诉他们你需要治疗。查准率和召回率越高越好。算法预测值与实际值分别是：1&#x2F;1（真阳性）、0&#x2F;0（真阴性）、1&#x2F;0（假阳性）、0&#x2F;1（假阴性）。\n\n\n04 精确度和召回率的权衡\n在逻辑回归中逻辑输出在0到1之间，其中0.5是个分界值，但是我们想在十分确定得情况下告诉病人真实信息，因此分界值为0.7，甚至0.9（是一个高查准率的模型，但是召回率会变低）。现在我们将分界值设置到较低（有30%几率得病），会得到高召回率，较低得查准率。\n\n\n\n有没有办法自动选取临界值？或者说有不同的算法，我们如何比较不同的查准率和召回率？或者临界值不同，我们怎样决定哪个更好？–如果使用平均值来计算是不可行的，因为如果假设y &#x3D; 1和y &#x3D; 0这两种极端的情况（要么很高召回率、很低查准率，要么很低召回率、很高查准率），他们俩不是好的模型。再此我们使用F值的公式，因为它同时结合召回率及查准率。\n\n\n\n自动选择临界值来决定你希望预测y&#x3D;1还是y&#x3D;0合理的方法：试一试不同的临界值，在检验集进行测试，看哪个临界值可以在检验集得到最高的F。这就是为分类器自动选择临界值的合理方法。\n\n05 机器学习数据在一定条件下，得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获取具有良好性能的学习算法。这种情况一般出现在这些条件对于你的问题都成立，并且可以得到大量数据。\n\n\n并不是拥有最好算法的人能成功，而是拥有最多数据的人能成功。\n\n\n\n如果让一个英语好的选词填空，它可以通过特征x让我们能够准确的预测y，相反的，我们让一个房地产专家预测一个房价，而只告诉它房子的面积，其他特征不告诉，他会很难预测。因此如果这个假设正确可以看出大量数据是很有意义的。\n\n\n\n得到一个低偏差（一个强大的具有很多参数的学习算法，可以很好的拟合复杂的函数）和低方差（如果训练集远大于参数的数量，就不大可能会过拟合）的学习算法（特征值足够并且训练集很庞大）\n\n\n","tags":["机器学习"]},{"title":"机器学习 day12支持向量机","url":"/2021/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day12%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","content":"01 优化目标支持向量机（SVM）在学习复杂的非线性方程时，能够提供一种更为清晰和更加强大的方式。\n\n\n从逻辑回归开始，稍作改动成为支持向量机\n观察下逻辑回归的假设函数和sigmoid激活函数\n\n\n\n\n\n观察逻辑回归的代价函数，当把整个假设函数的定义代入其中，得到的就是每个样本对总体函数的具体贡献。\n\n\n\n为了构建支持向量机，我们从这个代价函数开始进行少量修改，我们取z&#x3D;1，画出要使用的代价函数，右边都是平的，左边画出一条和最开始的幅度相似的直线，这就是y&#x3D;1时使用的代价函数。同样的做出y&#x3D;0时使用的代价函数。\n\n\n\n有了这些定义后，就可以开始构造支持向量机了，将修改后的代价函数定义带入到原始的逻辑回归代价函数中。支持向量机的代价函数将1&#x2F;m去掉，因为1&#x2F;m是一个常数，不影响得到参数的最优值。对于SVM来说我们将用一个不同的参数来控制第一项A与第二项B的相对权重，如果把C设置的很小，那么B就比A占有更大的权重。于是就得到了支持向量机的总体优化目标\n\n\n\n最后和逻辑回归不同的是，支持向量机并不会输出概率，相对的我们得到的是通过优化这个代价函数得到参数sita，支持向量机它是进行了一个直接的预测y&#x3D;0&#x2F;y&#x3D;1,学习得到参数sita后，这就是支持向量机的假设函数的形式。\n\n\n02 直观上对大间隔的理解\n下面是SVM代价函数，支持向量机不是恰好能正确分类就行，因此需要比0大或者小很多（也就是1或者-1）。\n\n\n\n如果C非常的大，那么当最小化最优目标的时候，将迫切的希望找到一个值使得第一项等于0。在两种情况下，通过选择参数sita使得第一项为0。\n\n\n\n支持向量机会选择尽量把正样本和负样本以最大的间距分开的假设模型。可以看出黑色的决策边界和训练样本的最小距离要更大一些。\n\n\n\n下面的大间距分类器是在常数C被设的非常大情况下得出的，平常情况下会得到黑色线，但是如果在一侧加入异常样本，那么就可能会是粉色的线。如果C不是很大，那么就算是加入异常点也会是黑色线。\n\n\n03 大间隔分类器的数学原理\n向量内积\n\n\n\n支持向量机的优化目标函数，当n&#x3D;2时我们只有两个特征量（也就是只有两个参数sita）。因此对于优化目标函数来说支持向量机做的是最小化参数向量sita的范数的平方。（为了简便都令sita0&#x3D;0）\n\n\n\n将sita转置x(i)替换后的结果写入我们的优化目标函数。令sita0&#x3D;0意味着决策边界必须通过原点(0,0)。下图是支持向量机选择不同的决策边界的情况。（向量sita一定是垂直于决策边界的），支持向量机通过让间距变大，使得p(i)变大，以至于输出一个较小的sita的范数。（为了简便都令sita0&#x3D;0，即使不为0，效果也不变。支持向量机仍然会找出正样本和负样本之间大间距分隔）\n\n\n04 核函数1改造支持向量机算法来构造复杂的非线性分类器。\n\n\n希望拟合一个非线性的判别边界来区分正负实例。一种方法是构造一个复杂多项式特征的集合，在这里我们用f1、f2、f3来表示这些我们将要计算的新的特征变量。\n\n\n\n构造新特征f1、f2、f3的方法：首先手动选取三个点（标记），接着将新特征定义为一种相似度的度量即度量训练样本x与标记的相似度（用下面公式表达）。相似度函数就是一个核函数（这里是高斯核函数）。\n\n\n\n对于这个核函数取两种情况：一种是x与标记点很近，一种是x与标记点很远。\n\n\n\n下面是核函数参数大小不同时的表现：\n\n\n\n选择不同的点，来预测y值是1还是0。可以看出离l_1和l_2近的点预测y值为1（带入下面的预测函数中如果&gt;&#x3D;0说明y&#x3D;1;&lt;0说明y&#x3D;0)。\n\n\n05 核函数2\n特征函数基本上是在描述每一个样本距离样本集中其他样本的距离，下面是这个过程的大纲。\n\n\n\n当已知参数sita时，怎样做出预测的过程。因为标记点的个数等于训练点的个数（m），所以参数向量sita为m+1维\n\n\n\n但是怎样得到参数sita？通过解决最小化的问题，你就得到了支持向量机的参数sita。\n\n\n\n在使用支持向量机时，怎么选择支持向量机中的参数C？–C相当于正则化参数的倒数，根据需求选择C。高斯核函数中的参数？–当高斯核函数中的参数相对较大时，图像将会倾向于变得相对平滑，可能会带来较高的偏差和较低的方差；当高斯核函数中的参数相对较小时，图像弧度将会相对大，可能会带来低偏差和较高的方差。（图像跟第四节倒数第二个相配套）\n\n\n06 使用SVM高度优化好的软件库：liblinear、libsvm\n\n\n我们虽然不用自己写SVM优化库，但是还是有几件事需要我们做：\n参数C的选择\n选择内核参数或者想要使用的相似函数\n\n\n对于内核函数其中第一个选择是不需要任何内核参数，没有内核参数的理念又叫线性核函数。为什么想要做这件事呢？如果有大量的特征n，而训练集m很小，也许就想拟合一条线性的判定边界，而不去拟合一条复杂的非线性函数。因此选择线性核函数可能很合适。\n对于内核函数其中第二个选择是可以构造个高斯内核函数。n很少，m很多，使用高斯内核函数可能很合适。\n\n\n\n如果选择要用高斯核函数，接下来要做的事：\n根据使用的支持向量机软件包可能需要实现一个核函数或者相似函数。因此如果使用Octave来实现支持向量机的话，那么就需要提供一个函数来计算核函数的特征值，它将自动的生成所有特征变量。因此对应一个i需要计算f_i。\n如果有大小很不一样的特征变量，要在使用高斯核函数之前，将这些特征变量的大小按比例归一化。下面以计算x与l之间的范数为例，如果第一特征房子面积特别大，第二特征房间个数在1到5之间，那么间距可能都是由特征一所决定，所以需要归一化。\n\n\n\n\n\n默塞尔定理是确保所有的SVM包能够用大类的优化方法并可以快速得到参数sita。其他核函数也都满足默塞尔定理，如下所示：多项式核函数（一个是自己添加的值可以是0、1、3…，还有一个是指数可以修改）。\n\n\n\n在多类分类中，输出在多个类别中恰当的判定边界。如果有k个类别用以将每个类别从其他的类别中区分开来。例如第一个参数sita_1,y&#x3D;1作为正类别，其他作为负类别得到的，以此类推。\n\n\n\n逻辑回归开始构造了SVM，然后更改下代价函数。如果n相对于m足够大，那么我通常使用逻辑回归或者线性核函数；如果n很小，m适中，通常使用高斯核函数的SVM比较好；当n很小m很大，建议手动地创建拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。这种情况不适用神经网络，因为运行会很慢，而且SVM不用去考虑局部最优的问题。\n\n\n","tags":["机器学习"]},{"title":"机器学习总结","url":"/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","content":"\n监督学习算法：线性回归、逻辑回归、神经网络、支持向量机（在这些问题中会有带标签的数据和样本）\n无监督学习：K-均值聚类算法、主成分分析法（进行降维）、异常检测算法（对算法进行评估）\n特定的应用和话题：推荐系统、大规模机器学习系统（包括并行和映射-化简算法）\n其他的应用：滑动窗口分类器（计算机视觉问题）\n从各个不同的方面给出了如何构建机器学习系统的建议：偏差和方差（尝试了是什么使得机器学习算法工作或者是不工作）、正则化（解决一些方差问题）、学习算法的评估方法：召回率和F1分数这样的评价指标和实践方面的评测方法：训练集-交叉验证集-测试集（当你开发一个机器学习系统时如何合理分配你的时间）、诊断方法：学习曲线和误差分析及上限分析（如何调试算法确保学习算法能够正常工作）。所有这些工具都能帮助你决定下一步该做什么以及怎么分配时间。\n\n\n","tags":["机器学习"]},{"title":"当你的浏览器中地址栏输入地址并回车的一瞬间到页面能够展示回来，经历了什么？","url":"/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E5%BD%93%E4%BD%A0%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E5%9C%B0%E5%9D%80%E6%A0%8F%E8%BE%93%E5%85%A5%E5%9C%B0%E5%9D%80%E5%B9%B6%E5%9B%9E%E8%BD%A6%E7%9A%84%E4%B8%80%E7%9E%AC%E9%97%B4%E5%88%B0%E9%A1%B5%E9%9D%A2%E8%83%BD%E5%A4%9F%E5%B1%95%E7%A4%BA%E5%9B%9E%E6%9D%A5%EF%BC%8C%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"（1）浏览器本身是一个客户端，当你输入URL的时候，首先浏览器会去请求DNS服务器，通过DNS获取相应的域名对应的IP（2）然后通过IP地址找到IP对应的服务器后，要求建立TCP连接（3）浏览器发送完HTTP Request（请求）包后，服务器接收到请求包之后才开始处理请求包（4）在服务器收到请求之后，服务器调用自身服务，返回HTTP Response（响应）包（5）客户端收到来自服务器的响应后开始渲染这个Response包里的主体（body），等收到全部的内容随后断开与该服务器之间的TCP连接。\n","tags":["面试题"]},{"title":"请你谈谈网站是如何进行访问的","url":"/2022/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E8%AF%B7%E4%BD%A0%E8%B0%88%E8%B0%88%E7%BD%91%E7%AB%99%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E7%9A%84/","content":"\n输入一个域名；回车 \n检查本机的 C:\\Windows\\System32\\drivers\\etc\\hosts配置文件下有没有这个域名映射；\n\n\n有：直接返回对应的ip地址，这个地址中，有我们需要访问的web程序，可以直接访问\n没有：去DNS服务器找，找到的话就返回，找不到就返回找不到；\n\n\n","tags":["面试题"]},{"title":"1编程作业：线性回归","url":"/2021/08/06/1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","content":"原任务是在Octave&#x2F;MATLAB实现，本次编程作业全部以python完成。\n\n01 简单的练习总结下题目：输出一个5*5的单位矩阵\n\n\n在此我们用np.eye(N,M&#x3D;None, k&#x3D;0, dtype&#x3D;&lt;type ‘float’&gt;)，首先N代表的是输出方阵的维度，第二个参数不用设置默认M&#x3D;N，主要看第三个参数，默认是对角线为1，其余全为0；如果k为正数，则对角线往上第k个全为1，其余全为0；如果k为负数，则对角线往下第k个全为1，其余全为0。\n\nimport numpy as npA = np.eye(5)print(A)&quot;&quot;&quot;[[1. 0. 0. 0. 0.] [0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.]]&quot;&quot;&quot;\n\n02 单变量线性回归根据这座城市的人口数量及该城市小吃店的利润，来预测开小吃店的利润。\n\n2.1 绘制数据\n读入数据：在此我们引入pandas库，该库可以帮助我们从诸如 csv 类型的文件导入数据，并且可以用它快速的对数据进行转换和过滤的操作。\n\nimport pandas as pdpath = &quot;machine-learning-ex1\\machine-learning-ex1\\ex1\\ex1data1.txt&quot;data = pd.read_csv(path,header=None,names=[&#x27;Population&#x27;,&#x27;Profit&#x27;])#header决定要不要原始的表头，name给出自定义的表头。print(data.head())#从头查询数据&quot;&quot;&quot;   population   profit0      6.1101  17.59201      5.5277   9.13022      8.5186  13.66203      7.0032  11.85404      5.8598   6.8233&quot;&quot;&quot;\n\n\n数据可视化：在此我们引入matplotlib.pyplot库，使用plot函数画图。\n\nimport matplotlib.pyplot as pltdata.plot(kind=&#x27;scatter&#x27;, x=&#x27;Population&#x27;, y=&#x27;Profit&#x27;, figsize=(12,8))#生成图形，kind‘指定所画图的类型，figsize 指定图片大小。plt.show()#显示图形\n\n\n2.2 梯度下降这部分需要使用梯度下降将线性回归参数 θ 拟合到数据集上。\n2.21 公式\n代价函数\n\n\n\n假设函数\n\n\n\n参数更新\n\n\n\n随着梯度下降不断地更新参数，参数也就越接近使代价函数最小的最优值\n\n2.22 实现\n我们要为我们之前读取的数据添加一列x，用来更新θ_0。\n\ndata.insert(0, &#x27;Ones&#x27;, 1) #相当于在第0列，添加一个表头名为Ones，并且该列均为1print(data.head())&quot;&quot;&quot;   Ones  Population   Profit0     1      6.1101  17.59201     1      5.5277   9.13022     1      8.5186  13.66203     1      7.0032  11.85404     1      5.8598   6.8233&quot;&quot;&quot;\n\n\n分割X和y。使用pandas的iloc来进行选择训练集X和目标y\n\n# 分割X和ylists = data.shape[1]#输出列数X = data.iloc[:,:-1]#X是第一列到最后一列，但不包括最后一列，因为 python的范围/切片不包括终点y = data.iloc[:,lists-1:lists]#最后一列#y = data.iloc[:,-1]#也是最后一列print(X.head())&quot;&quot;&quot;   Ones  Population0     1      6.11011     1      5.52772     1      8.51863     1      7.00324     1      5.8598&quot;&quot;&quot;print(y.head())&quot;&quot;&quot;    Profit0  17.59201   9.13022  13.66203  11.85404   6.8233&quot;&quot;&quot;\n\n\n我们还要将θ初始化为0，并将θ、X、y全部转化为矩阵\n\nX = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix(np.array([0,0]))print(X.shape)#(97, 2)print(y.shape)#(97, 1)print(theta.shape)#(1, 2)\n\n2.23 计算J(θ)\n计算代价函数来检测代价函数的收敛性。根据上面的公式我们写出代价函数。\n\ndef computeCost(X, y, theta):    inner = np.power((X * theta.T)-y,2)#数组元素求n次方    return np.sum(inner) / (2 * len(X))print(computeCost(X, y, theta)) #32.072733877455676\n\n2.24 梯度下降代价函数J(θ)的参数是由向量θ表示，假设你已经实现了梯度下降，如果计算正确，J(θ)的值不应该增加，而应该减小然后在算法结束时收敛到一个稳定值。\ndef gradientDescent(X, y, theta, alpha, iters):    temp = np.matrix(np.zeros(theta.shape))#创建0矩阵[[0. 0.]]    parameters = int(theta.ravel().shape[1]) #ravel()将多维数组转换为一维数组,.shape[1]是看列数为多少-2    cost = np.zeros(iters)#初始化代价函数数组    for i in range(iters):        error = (X * theta.T) - y        for j in range(parameters):            term = np.multiply(error, X[:, j])            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))#更新参数        theta = temp        cost[i] = computeCost(X, y, theta)    return theta, costalpha = 0.01iters = 1500g, cost = gradientDescent(X, y, theta, alpha, iters)print(g)#[[-3.63029144  1.16636235]]predict1 = [1,3.5]*g.Tprint(predict1)#[[0.45197679]]predict2 = [1,7]*g.Tprint(predict2)#[[4.53424501]]\n\n2.3 调试\npython可视化：原始数据以及拟合的直线\n\n# 在指定的间隔内返回均匀间隔的数字：从data.Population的最小值到最大的范围内，等间距的返回100个样本x = np.linspace(data.Population.min(), data.Population.max(), 100)f = g[0, 0] + (g[0, 1] * x)#参数为最优值的直线fig, ax = plt.subplots(figsize=(12,8))#创建一个12*8的图即多维窗口ax.plot(x, f, &#x27;r&#x27;, label=&#x27;Prediction&#x27;) #定义x, y, 颜色，图例上显示的东西ax.scatter(data.Population, data.Profit, label=&#x27;Traning Data&#x27;)ax.legend(loc=2)#指定图例的位置ax.set_xlabel(&#x27;Population&#x27;)ax.set_ylabel(&#x27;Profit&#x27;)ax.set_title(&#x27;Predicted Profit vs. Population Size&#x27;)plt.show()\n\n\n03 多变量线性回归根据ex1data2.txt里的数据建立模型，预测房屋的价格，其中第一列是房屋大小，第二列是卧室数量，第三列是房屋售价\n\n\n第一步依旧是读入数据：\n\npath = &#x27;machine-learning-ex1\\machine-learning-ex1\\ex1\\ex1data2.txt&#x27;data2 = pd.read_csv(path,header = None,names=[&#x27;Size&#x27;, &#x27;Bedrooms&#x27;, &#x27;Price&#x27;])print(data2.head())&#x27;&#x27;&#x27; Size  Bedrooms   Price0  2104         3  3999001  1600         3  3299002  2400         3  3690003  1416         2  2320004  3000         4  539900&#x27;&#x27;&#x27;\n\n3.1 特征归一化特征缩放的目的只是为了运行更快。使特征值比较接近，使图像变得比较圆。以至于梯度下降的速度更快，收敛所需要的迭代次数更少，收敛更快。\n\n\nmean()函数功能：求取均值，std()函数是用来求标准差的（std &#x3D; sqrt(mean(abs(x - x.mean())**2))）。\n\ndata2 = (data2 - data2.mean()) / data2.std()print(data2.head())&#x27;&#x27;&#x27;      Size  Bedrooms     Price0  0.130010 -0.223675  0.4757471 -0.504190 -0.223675 -0.0840742  0.502476 -0.223675  0.2286263 -0.735723 -1.537767 -0.8670254  1.257476  1.090417  1.595389&#x27;&#x27;&#x27;\n\n3.2 梯度下降data2.insert(0, &#x27;Ones&#x27;, 1)cols = data2.shape[1]X2 = data2.iloc[:,0:cols-1]y2 = data2.iloc[:,cols-1:cols]X2 = np.matrix(X2.values)y2 = np.matrix(y2.values)theta2 = np.matrix(np.array([0,0,0]))g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)print(g2)&#x27;&#x27;&#x27;[[-1.10898288e-16  8.84042349e-01 -5.24551809e-02]]&#x27;&#x27;&#x27;\n\n3.3 正规方程训练集特征矩阵为 X（包含了x_0&#x3D;1）训练集结果为向量 y，则利用正规方程解出向量:其中np.linalg.inv()：矩阵求逆。\n\ndef normalEqn(X, y):    theta = ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)    # theta = np.linalg.inv(X.T@X)@X.T@y    return thetatheta2=normalEqn(X2, y2)print(theta2)&#x27;&#x27;&#x27;[[-7.11223170e-17] [ 8.84765988e-01] [-5.31788197e-02]] &#x27;&#x27;&#x27;\n\n04 代码总结import matplotlib.pyplot as pltimport numpy as npimport pandas as pdpath = &quot;machine-learning-ex1\\machine-learning-ex1\\ex1\\ex1data1.txt&quot;data = pd.read_csv(path,header=None,names=[&#x27;Population&#x27;,&#x27;Profit&#x27;])#header决定要不要原始的表头，name给出自定义的表头。#data.plot(kind=&#x27;scatter&#x27;, x=&#x27;Population&#x27;, y=&#x27;Profit&#x27;, figsize=(12,8))#生成图形，kind‘指定所画图的类型，figsize 指定图片大小。# plt.show()#显示图形#==============================================================================data.insert(0, &#x27;Ones&#x27;, 1) #相当于在第0列，添加一个表头名为Ones，并且该列均为1# 分割X和ylists = data.shape[1]#输出列数X = data.iloc[:,:-1]#X是第一列到最后一列，但不包括最后一列，因为 python的范围/切片不包括终点y = data.iloc[:,lists-1:lists]#最后一列#y = data.iloc[:,-1]#也是最后一列X = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix(np.array([0,0]))def computeCost(X, y, theta):    inner = np.power((X * theta.T)-y,2)#数组元素求n次方    return np.sum(inner) / (2 * len(X))# print(computeCost(X, y, theta)) #32.072733877455676#梯度下降算法如下：def gradientDescent(X, y, theta, alpha, iters):    temp = np.matrix(np.zeros(theta.shape))#创建0矩阵[[0. 0.]]    parameters = int(theta.ravel().shape[1]) #ravel()将多维数组转换为一维数组,.shape[1]是看列数为多少    cost = np.zeros(iters)    for i in range(iters):        error = (X * theta.T) - y        for j in range(parameters):            term = np.multiply(error, X[:, j])            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))        theta = temp        cost[i] = computeCost(X, y, theta)    return theta, costalpha = 0.01iters = 1500g, cost = gradientDescent(X, y, theta, alpha, iters)#print(g)#[[-3.63029144  1.16636235]]predict1 = [1,3.5]*g.T#print(predict1)#[[0.45197679]]predict2 = [1,7]*g.T#print(predict2)#[[4.53424501]]# 在指定的间隔内返回均匀间隔的数字：从data.Population的最小值到最大的范围内，等间距的返回100个样本x = np.linspace(data.Population.min(), data.Population.max(), 100)f = g[0, 0] + (g[0, 1] * x)#参数为最优值的直线fig, ax = plt.subplots(figsize=(12,8))#创建一个12*8的图即多维窗口ax.plot(x, f, &#x27;r&#x27;, label=&#x27;Prediction&#x27;) #定义x, y, 颜色，图例上显示的东西ax.scatter(data.Population, data.Profit, label=&#x27;Traning Data&#x27;)ax.legend(loc=2)#指定图例的位置ax.set_xlabel(&#x27;Population&#x27;)ax.set_ylabel(&#x27;Profit&#x27;)ax.set_title(&#x27;Predicted Profit vs. Population Size&#x27;)#plt.show()#===========================================================================path = &#x27;machine-learning-ex1\\machine-learning-ex1\\ex1\\ex1data2.txt&#x27;data2 = pd.read_csv(path,header = None,names=[&#x27;Size&#x27;, &#x27;Bedrooms&#x27;, &#x27;Price&#x27;])data2 = (data2 - data2.mean()) / data2.std()data2.insert(0, &#x27;Ones&#x27;, 1)cols = data2.shape[1]X2 = data2.iloc[:,0:cols-1]y2 = data2.iloc[:,cols-1:cols]X2 = np.matrix(X2.values)y2 = np.matrix(y2.values)theta2 = np.matrix(np.array([0,0,0]))g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)print(g2)def normalEqn(X, y):    theta = ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)    # theta = np.linalg.inv(X.T@X)@X.T@y    return thetatheta2=normalEqn(X2, y2)print(theta2)\n","tags":["机器学习"]},{"title":"机器学习 day13无监督学习","url":"/2021/07/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day13%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","content":"01 无监督学习\n无监督学习：训练集没有标签，也就是图上的点没有任何标签信息。我们要将这系列无标签的数据，输入到算法中，然后我们要让算法找到一些隐含在数据中的结构，这个图中数据集中的点两组分开的簇，这种能够找到这些簇的算法叫做聚类算法。\n\n\n02 K-Means算法K均值算法是现在最为广泛运用的聚类算法\n\n\n通过K均值算法将下图分为两个簇的具体操作：\n随机生成两点（聚类中心），选取两点的原因是想将数据聚成两类。\n\n\nK均值算法是个迭代算法，可以做两件事：簇分配和移动聚类中心。\nK均值算法每次内循环的第一步是要进行簇分配，观察图中的绿点，是接近哪个聚类中心，距离哪个近就分配给哪个。\n\n\n\n根据离红色或者蓝色聚类中心的远近，将每个点染成红色或者蓝色。\n\n\n\nK均值算法每次内循环的第二步是移动聚类中心，将两个聚类中心移动到同色点的均值处。\n\n\n\n接着我们进入下一个簇分配步骤：再次检查所有的无标签样本，然后根据这些点离红色或者蓝色聚类中心的远近，将其染成红色或者蓝色。\n\n\n\n再一次移动聚类中心，将两个聚类中心移动到同色点的均值处。\n\n\n\n以此类推，不断重复上面操作，得到最终结果。\n\n\n\nX^(i)是一个n维实数向量，这也就是训练样本是n维向量而不是n+1维向量的原因。\n\n\n\n以更加规范的格式写出K均值算法。\n\n\n\n假设u_k是某个簇的均值，那么如果存在一个没有点的聚类中心，会怎么样？–直接移除那个聚类中心，这样会得到k-1个簇而不是k个簇；如果你确实必须要k个簇，那么就随机初始化这个聚类中心。\nK均值算法最常还应用在分离不佳的簇的问题（右图），如果想把衣服根据体重身高分为小中大三个号，这时就可以使用k均值算法。\n\n\n03 优化目标\n当运行k均值算法时，我们会对两组变量进行跟踪。\n\nc^(i)代表的是当前样本x^(i)所属的那个簇的索引。\n用u_k表示第k个聚类中心的位置（K表示聚类中心的数量，k表示聚类中心的下标）。\n\n\nk均值算法的优化目标即最小化代价函数，代价函数有时也叫失真代价函数或者K均值算法的失真：\n\n\n\n\n簇分配实际上就是，不改变聚类中心的位置，而是选出c^(1)到c^(m)来最小化这个代价函数；而移动聚类中心实际上就是选择u值来最小化J。然后保持迭代。\n\n\n04 随机初始化如何初始化K均值聚类算法？引导讨论如何使算法避开局部最优。\n\n\n初始化聚类中心的比较好的方法，通常随机挑选K个训练样本，然后设定u_1到u_k，由于随机初始化状态不同，所以K均值最后可能会得到不同的结果。（最好使用这种方法）\n\n\n\n随机化得到结果比较好（右上图），也可能得到不好的局部最优值（右下图）。局部最优值指的是畸变函数J的局部最优，相对来说左下角图的这些局部最优所对应的情况，其实是K均值算法落在了局部最优，因而不能很好的最小化J。如果想要K均值算法找到最有可能的聚类（右上图），我们可以尝试多次随机初始化，并且运行多次。\n\n\n\n具体步骤如下：典型运行K均值算法的次数在50到1000次，需要在最后选取代价最小的一个。如果你运行K均值算法时，所用的聚类数相当小（2到10之间），那么多次随机初始化通常能保证找到较好的局部最优解，保证能找到更好的聚类，很好的最小化J；如果K非常大的话，多次初始化也不会有太大的改善。更有可能第一次初始化就会有很好的结果。\n\n\n05 选取聚类数量K均值如何去选择聚类数量？–最常用还是根据观察可视化的图，或通过观察聚类算法的输出去手动选择。\n\n\n当谈到选择聚类数量的方法时，可能谈到的一种方法叫“肘部法则”。我们所要做的是改变K，我们先用一个类来聚类，然后计算代价函数，接着用两个、三个…..（其中可能会多次初始化），可能会得到一个图像（随着K增多J下降），选择K&#x3D;3可能是正确的，因为K&#x3D;3之前下降很快，而之后就下降很慢。但是实际上好多时候都是右图，比较平滑没有拐点，从而不能准确选择K。不能期望它能解决任何问题。\n\n\n\n如果后续目的如市场分割能给一个评估标准，那么决定K更好的方法是看哪个聚类数量能更好地应用于后续目的。从商业角度看，例如如果K&#x3D;5是否我的衣服能够很好的满足顾客需求，还是K&#x3D;3时更加符合利益。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day14降维","url":"/2021/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day14%E9%99%8D%E7%BB%B4/","content":"01 目标 I：数据压缩\n想要使用降维的的原因：\n数据压缩，数据压缩不仅能使数据占用少量的内存或硬盘空间，还能对算法进行加速。\n\n\n如果特征高度相关，那么真的可能需要降低维数。\n将二维降到一维：两个特征变成一个特征，把每个样本都保持为一个数字，这样就能把内存需求（数据空间需求）减半。\n\n\n\n从三维降到二维：\n\n\n02 目标 II：可视化\n想要使用降维的的原因：\n\n可视化数据：将数据从50维或者更高维降到三维或者两维，这样就可以将其在图中画出来更好的理解他。\n\n\n50维数据：\n\n\n\n\n从50维降到2维，通常我们要了解z特征大致意味着什么。\n\n\n\n并画出2维图像\n\n\n03 主成分分析问题规划1对于降维问题最常用的一种算法叫主成分分析方法(PCA)的算法，本节课会试着用公式准确的表述PCA的用途。\n\n\nPCA所做的就是试图寻找一个投影平面对数据进行投影，使得能最小化这个距离。（PCA所作的是找到一个低维平面也就是图中的线，然后将数据投影到上面，使得蓝色的小线段（投影误差）长度平方最小）。对比可得选择橘色的而不选择粉色的。\n\n\n\n在应用PCA之前最常规的做法是先进行均值归一化和特征规范化使得特征量x_1和x_2其均值为0，并且其数值在可比较的范围之内。\n\nPCA与线性回归的区别：左图为线性回归，右图为PCA。线性回归的点是垂直横轴的（最小化蓝线之和的平方），而PCA的点是垂直那条线的（最小化蓝线的长度）。线性回归是通过输入x来预测y，PCA的特征值x之间都是平级关系。\n\n\n\n04 主成分分析问题规划2\n在使用PCA之前首先要做的的就是对数据进行预处理。\n给定一个训练集一定要做的是执行均值标准化：我们首先计算每个特征的均值，我们用减去它的均值取代每个特征x，这将使每个特征的均值正好为0\n根据数据也有可能需要进行特征缩放，缩放每个特征到一个相对的价值范围。\n\n\n\n\n\nPCA要做的是：计算这些向量例如u^(1)（即投影平面）；计算这些数字z\n\n\n\n用公式求解的过程：我们想把n维数据降到k维度，我们首先要做的是计算协方差（通常用大写的Sigma表示，是表示矩阵而不是求和），然后运用Octave中的svd()(奇异值分解)，也可以用eig()，不过svd()在数值上更加稳定。输出的将是三个U,S,V矩阵，我们真正需要的是U矩阵，可以发现U矩阵的列正是我们想要的u^(1)、u^()….。把n维数据降到k维度，就提取前k个向量。\n\n\n\n接下来我们要做的是获取我们的原始实数域数据集x找到一个低维的表示z。然后运用公式求解z\n\n\n\n协方差矩阵sigma的一个向量化实现：sigma &#x3D; (1&#x2F;m)X^TX,类似于K均值这里的X_0不能为1.\n\n\n05 主成分数量选择n维特征减少为k维特征，k为PCA算法的一个参数，也被称为主成分的数字，人们如何考虑选取k？\n\n\nPCA试图去减小投影误差平方的平均值，也就是试图减小x和x在低维面的投影的距离的差的平方。\n数据的总方差为样本x的平方和的平均值，也就是我们训练集中每个样本的平均长度（我们样本距离全零向量（原点）的距离）。\n选择k一个常用的经验方法是选择较小的值，使得这两者之间的比值小于0.01：让平均投影误差平方&#x2F;数据的总方差（数据的波动程度）。我们希望这个比例小于0.01或者小于1%，用PCA语言来说就是99%的方差性会被保留。当然0.01也可以换成其他值。为了保留99%的方差可以减少数据维数，但仍保留大部分的方差。\n\n\n\n下面的算法不断尝试从1开始修改k的值，然后进行PCA运算，最终使99%的方差性会被保留。这是一种方式来选择最小的k从而使99%的方差被保留。另一种方法是：通过SVD()计算出S矩阵（只有对角才有值，剩下的值为0），然后通过公式直接将k从1开始带进去，取前k个对角线的值相加&#x2F;整个对角线的值相加，看是否大于0.99.如果大于就选择这个k。对比可以看出第二种算法效率高，因为只需调用一次SVD()，而第一种需要多次调用PCA算法，相对来说效率低。\n\n\n06 压缩重现如何从低维的z^(i)变回原来的高维x^(i)?\n\n\n原始数据的重构：从压缩之后的低维的z^(i)变回原来未压缩的高维x^(i)：应用如下公式：\n\n\n07 应用 PCA 的建议\n使用PCA算法对监督学习算法进行加速的步骤：\n检查已经被标记的训练集，并抽取输入x，我们就得到了一个无标签的训练集x^(1)….x^(m)。\n用PCA得到原始数据的低维表示z，得到新的训练集。\n我们可以把这个低维的训练集输入到一个学习算法中，可以是神经网络、逻辑回归算法进行预测。\n注意PCA运算只能在训练集拟合这些参数，而不能在验证集或者测试集。定义x到z的映射后才能应用验证集和测试集中\n\n\n\n\n\n对PCA错误的使用就是尝试使用PCA来减少数据维度去防止过拟合，正确方法就是使用正则化。PCA运算是不使用标签的，只是针对输入x^(i)变为低维z^(i)，而不关心y，如果有99%的方差得以保留可以防止过拟合，但是也可能舍弃一些重要的信息，使得方差保留过低。\n\n\n\n在写下一个包含PCA的项目计划之前，应该问一问这个问题：如果我们直接去做而不使用PCA会怎样？一般直接建议：首先使用最原始的数据x^(i)，只有这么做达不到目的的情况下才考虑使用PCA和z^(i)，不要一开始就花大量的时间去应用PCA，计算k等等。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day15异常检测","url":"/2021/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day15%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/","content":"01 问题动机异常检测主要用在非监督学习，但从某些角度看，跟有监督学习问题是非常相似的。\n\n\n异常检测问题就是我们希望知道新的测试数据是否有某种异常，换句话说新的测试数据是否需要进一步测试。如下面这个图（飞机引擎的例子），其中x_1、x_2是飞机引擎的特征。通过测试发现如果x_test在对应点里面，那么他就不存在异常，如果在对应的外面，那么就存在异常。\n\n\n\n我们通常认定这m个数据都是异常或者都不是异常的，我们需要一个算法告诉我们一个新的样本数据是否异常。因此我们要做的就是给定无标签训练集，我们对数据建模即p(x)，也就是对x的分布概率建模。如果测试集的概率p低于阈值那么就将其标为异常，否则正常。越中心的点概率越高。\n\n\n\n异常检测最常见的应用是欺诈检测，例如许多购物网站常用来识别异常用户。可以针对不同的用户活动计算特征变量x^(i)，于是建立一个模型来表示用户表现出各种行为的可能性。x_1也许是用户登陆的频率，x_2也许是用户访问某个页面的次数……，然后根据这些数据建立一个模型p(x),就可以用这个模型发现网站上行为可疑的人。\n另一个例子就是上面的飞机引擎例子。\n第三个是数据中心的计算机监控，这种技术被各大数据中心来监测大量的计算机可能发生的异常。\n\n\n02 高斯分布（正态分布）\n假设x是一个实数的随机变量，如果x的概率分布服从高斯分布，那么记作X~N(均值，方差（标准差的平方）)。高斯分布的概率密度绘制出来是一个钟形，均值控制中心位置，标准差控制宽度。\n\n\n\n下面是参数不同画出来的图像，不管怎么样阴影部分的积分一定是1。\n\n\n\n参数估计问题：给定数据集希望能找到能够估算出均值和方差的值。下面是公式：\n\n\n03 算法\n我们处理异常检测的方法是：我们我们要用数据集建立起模型概率p(x)，我们要试图解决出哪些特征量出现的概率较高，哪些出现的概率较低。分布项p(x)的估计问题有时也称为密度估计问题，我们列出p(x)的计算公式（x_1到x_n连乘）如下：\n\n\n\n异常检测算法：\n选择特征量（能够描述所收集数据的一般特性的特征），能够帮助我们指出那些反常的样本。\n给出m个未作标记的样本的训练集。\n进行参数拟合u_1到u_n，(sita_1)^2到(sita_n)^2,运用公式来求解这些参数。\n给一个测试样本，根据p(x)的公式来测量是否异常。\n\n\n\n\n\n如何对这些数据拟合出参数值，通过第一个图的数据求出第二个图（概率图），然后将第二个图两个数据进行相乘画出第三个图（三维图）。如果一个想要检测一个样本是否异常，可以直接看三维图，异常的都在图像下围显示（概率低），而正常都在图像上围（概率高），其中上下围以一个值（伊克塞了吗）作为分界线。\n\n\n04 开发和评估异常检测系统\n实数评估的重要思想：当你为某一个应用开发学习算法时，你需要进行一系列的选择，比如选择使用什么特征。如果有一个方法通过返回一个实数来评估算法，那么就将变得容易多。如果有一个特征要考虑该不该将其纳入，就可以将其带入算法中返回一个实数来告诉你纳入前后对算法的影响。\n能够评估异常检测算法的标准方法：用一些带标签的数据来指明哪些是异常样本，哪些是正常样本。\n我们首先假设一些训练集看作是无标签的，他们是很大的正常样本的集合，即使其中掺杂了少许的异常样本也没关系。接下来在定义一些验证集和测试集（包含了已知是异常的样本即y&#x3D;1）用来评估这个异常检测算法。\n\n\n\n举一个例子：飞机成产了一万个正常飞机，2到50个异常飞机。把这些数据分配给训练集、验证集和测试集的经典方法是：分配正常样本是6：2：2。异常样本只分给验证集和测试集一人一半。按照道理来说验证集和测试集应该是完全不同的两种数据。\n\n\n\n得到训练集、验证集和测试集后推导和评估算法：\n使用训练集拟合模型p(x)，也就是将这m个无标签（实际上都是正常的）样本都用高斯函数来拟合。\n在验证集和测试集给出x的值，算法将预测出y的值，y&#x3D;1对应异常样本，y&#x3D;0代表正常样本。\n\n\n对于这个问题如果数据是倾斜数据（绝大多数数据都是正常的），我们就应该采用原来学过的真阳性、假阳性、真阴性、假阴性来计算召回率和准确率来预测算法的好坏。\n如果有一个验证集一个选择参数（伊克塞了吗）的方法就是尝试去使用许多不同的值，然后选择一个能够最大化F_1-score这样的实数，或者有其他好表现的。\n\n\n05 异常检测 VS 监督学习\n什么情况下很可能使用异常检测算法以及什么时候使用监督学习算法？\n\n第一种–&gt;对于异常检测算法：它有很少的正样本和很多的负样本，当我们在处理估计p(x)的值来拟合所有的高斯参数的过程中，我们只需要负样本就够了；相反对于监督学习算法：在合理的范围内会有大量的正样本和负样本\n第二种–&gt;对于异常检测算法：经常会有许多不同类型的异常，因此如果是这种情况你会有很少量的正样本，对于一个算法就很难从少量的正样本去学习异常，尤其是未来可能出现的异常可能会和已有的异常截然不同。比如你在正样本中已经了解到5个或者更多发生故障的情况，但是可能到了明天你就需要检测一个全新的集合（新的异常），这种情况就可以对负样本用高斯分布模型来建模；相反对于监督学习算法：有一个足够多的正样本或者一个能识别正样本的算法（未来可能出现的正样本与你当前训练集中的正样本类似），他能观察大量的正样本和大量负样本来学习相应的特征，可以正确区分正负样本。（例如垃圾分类）\n\n\n\n下面是一些异常检测和监督学习的例子，对于欺诈行为&#x2F;飞机引擎&#x2F;计算机检测，如果有大量的欺诈行为&#x2F;引擎异常&#x2F;计算机运行异常的情况，就可以转化为监督学习，但是一般都是异常检测；对于垃圾分类，天气预报、癌症分类等问题有大量的同量级正负样本这些情况就使用监督学习。\n\n\n\n\n总结一句话就是异常检测以正常（负样本）来建模，监督学习以不正常（正样本）来训练。\n\n06 选择要使用的功能你使用什么特征来实现异常检测算法将会运用产生很大的影响\n\n\n在异常检测中我们要做的其中一件事就是使用高斯分布来对特征建模，所以我们经常做的一件事就是画出数据或者用直方图来表示数据，以确保这些数据在进入异常检测算法前看上去比较接近高斯分布。当然即使数据不是高斯分布也是可以运行的，就是运行好坏的问题。例如下图所示，图一是比较理想的，如果得到图二，我希望对数据进行一些如下不同的转换使得它看上去更接近高斯分布。\n\n\nhist(x.^0.05,50)\n\n\n你如何得到异常检测算法的特征：通常用的方法就是通过一个误差分析步骤（跟我们之前讨论监督学习算法时误差分析的步骤是类似的）。我们先完整地训练出一个算法，然后在一组验证集上运行算法，然后找出那些预测出错的样本，并看看我们能否找到一些其他的特征来帮助学习算法，让那些在验证集中判断出错的样本表现得更好。下面通过一个例子来解释这个过程。一些大众的问题就是正负样本的概率都很高，以一个特征为例，x&#x3D;2.5时的一个测试样本是异常的，但是混在了正常样本里，这时就需要找到一个新的特征来使算法可以很好的将x&#x3D;2.5的测试样本进行分离。（绿色的代表异常）。\n\n\n\n下面是一些对异常检测算法选择特征变量时的一些思考：对于那些可能异常的样本即不选择那些特别特别大，也不选择特别特别小的特征。以监控数据中心的计算机的例子，如果一个计算机出现了故障，下面给出了一些可选的特征包括占用内存、磁盘每秒访问的次数、CPU负载和网络流量，现在假如说我怀疑某个出错的情况，在我的数据集中我的CPU负载和网络流量应该是线性关系，比如说检测服务器，一个服务器在运行多台电脑，这时x_3和x_4都很高；如果在运行任务时出现了死循环（x_3升高，x_4正常），这时要检测就可以建立一个新的特征x_5或者x_6。通过建立新的特征就可以捕捉到这些特殊的特征组合所出现的异常值。\n\n\n07 多变量高斯分布这节课将会学习目前为止学习的异常检测算法的一种可能的延伸，这个延伸会用到多元高斯分布。他有一些优势也有一些劣势，他能捕捉到之前的算法检测不出来的异常。\n\n\n异常检测算法给x_1和x_2建模的方法如下，给出一个样本(绿色的)，可以从大体上看出x_1和x_2是线性的，而绿色的可以看出x_1很少，而x_2很多是不符合线性的。该图认为同一个圈的概率是一样的，但是实际上从图可以看出其实是不一样的。\n\n\n\n为了解决这个问题，我们要开发一个改良版的异常检测算法，要用到多元高斯分布或者叫多元正态分布。有一个特征向量x，我们不要对x_1和x_2分别建模，而是要建立一个整体的p(x)模型。它的参数是n维向量(u)和n×n的协方差矩阵。\ndet(Sigma)--&gt;用来计算行列式\n\n\n\n我们来看下这里的p(x)是什么样的。其中Sigma衡量的是方差或者说是x_1和x_2的变化量。缩小方差相当于缩小(Sigma)^2，下面是同时改变Sigma的两个值。\n\n\n\n\n接下来我们只改变特征向量x_1的方差，展示如下：\n\n\n\n同样的们只改变特征向量x_2的方差，展示如下：\n\n\n\n多元高斯分布一件很棒的事是你可以用它给数据的相关性建立模型（可以用它来给x_1和x_2高度相关的情况建立模型），如果改变非对角线上的元素你会得到一种不同的高斯分布，随着非对角线上的值不断增加可以看出x接近y（大部分数据落到了x_1&#x3D;x_2），变得更窄。当把其设为正值时如下：\n\n\n\n当把其设为负值时（x_1&#x3D;-x_2）如下：\n\n\n\n当然还可以改变平均参数u，即中心点所在位置。\n\n\n08 使用多变量高斯分布的异常检测\n给定参数集来进行参数拟合或者说是参数估计问题（其中Sigma和PCA中写的是一样的）。\n\n\n\n把这些结合起来开发一个异常检测算法：\n用我们的数据集来拟合模型\n有一个测试样本，用多元高斯分布的公式来计算p(x)\n比较p(x)与伊克塞了吗的大小来得出结果。\n\n\n\n\n\n原始模型对比多元高斯模型，多元高斯模型的轮廓都是轴对齐的。其实原始的模型就是多元高斯模型的特殊形式，，当多元高斯模型Sigma的非对角线都为0的情况下，将原始的方差放入到Sigma中，这时两个模型就会完全相同，也就是说这个用了多元高斯分布的新模型，比起之前的旧模型，这个新模型对应的分布方程的轮廓就是轴对齐的，所以不能用不同的特征之间的关系进行建模。\n\n\n\n那么什么时候用原始模型（比较常用），什么时候用多元高斯模型呢？比如之前计算机检测的例子，原始模型可能就需要建立一个新的特征，而多元高斯模型就能自动捕捉这种不同特征值之间的关系。\n原始模型的一个巨大的优势就是计算成本比较低（能适应数量巨大的特征），而多元高斯模型需要计算Sigma的逆矩阵也就意味着计算成本非常高。原始模型还有个优势是即使你有一个较小的有一定相关性的训练集也能顺利运行，而多元高斯分布模型需要样本的数量大于特征的数量（因为需要求逆矩阵）。我们使用多元高斯分布模型当且仅当m&gt;&gt;n（如果不满足这一点那么多元高斯分布模型就会有很多参数，因为协方差矩阵是一个n*n的，所以协方差矩阵有n^2个参数，同样因为是一个对称矩阵所以更接近n^2&#x2F;2），所以需要确保有一个很大的m值，确保有足够的数据来拟合变量。\n如果你需要捕捉一些具有相关性的特征，通常是手动设计新的特征，但是当m很大或者是n不是很大时这时多元高斯模型更好。\n如果在实际操作中发现Sigma是奇异的不可逆的那么有两种情况：\n没有满足m大于n的条件\n存在冗余特征（比如x_1&#x3D;x_2、x_3&#x3D;x_4+x_5）\n\n\n\n\n\n总结下使用多元高斯分布来做异常检测，可以自动地捕捉正负样本各特征之间地关系。如果发现某些特征的组合值是异常的他就会标为异常样本。\n\n","tags":["机器学习"]},{"title":"机器学习 day16推荐系统","url":"/2021/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day16%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","content":"01 问题规划特征对于机器学习来说是十分重要的，机器学习领域有一个伟大的想法，对于某些问题有一些算法可以自动的学习一系列合适的特征。\n\n\n我们为什么要谈论推荐系统？\n它是机器学习中的一个重要的应用，比如说亚马逊会根据你之前所买的书来给你推荐一些书。\n有一些环境能让你开发某个算法来学习使用哪些特征，而推荐系统就是那些环境中的一个例子，通过推荐系统我们能领悟一些特征学习的思想。\n\n\n推荐系统问题的组成：以电影评分为例，先说下这些符号的意思。n_u表示用户的数量、n_m表示电影的数量、r(i,j)代表用户j是否对电影i进行评价（&#x3D;1代表评价了），当用户对电影进行评分后会得到一个值y(i,j)，它代表用户j对电影i所给出的评分（0~5星表示）。\n因此推荐系统的问题是给出r(i,j)和y(i,j)数据然后去查找那些没有评分的电影，并试图预测这些电影的评价星级。\n\n\n\n总结：如果我们想开发一个推荐系统，那我们应该想出一个学习算法，一个能自动为我们填补那些缺失值的算法，这样就可以知道用户还有哪些电影没看过并推荐新电影给用户。\n\n02 基于内容的推荐算法\n我们用x_1（一部电影为爱情片的程度）和x_2（一部电影为动作片的程度）来表示电影的特征。为了做出预测我们把每个用户的评价预测值看做成一个线性回归问题。预测值就是用户j所学参数的转置与电影特征的内积。\n\n\n\n我们要m^(j)表示评价了电影j的用户数量，为了学习用户参数向量（sita）^(j)我们应该怎么做呢？这是一个基本的线性回归问题，我们可以直接选择一个参数向量，那么这里的预测值会尽可能接近我们在训练集中观察的值。为了学习参数向量我们来最小化参数向量（我们对所有用户j评价的所有电影的(预测值-实际观测值)的平方求和）再除以m。这就像是线性回归我们选择参数向量来最小化这种方差项，并且也可以加入正则化项。如果将这个公式最小化，你可以得到一个很好的对参数向量的估计值，用来对用户j的电影评价做预测。\n\n\n\n当构建推荐系统时，我们不仅是要学习单个用户的参数，我们要学习的是所有用户的参数向量，这时就要用到下面这个式子：\n\n\n\n总结一下：图片上面是我们的优化目标函数，换一种写法J依旧是优化目标函数也是我们试图最小化的项。如果去推到梯度下降更新的话，将会得到下面公式（分别是k&#x3D;0和k!&#x3D;0的情况，因为上面的公式k是不等于0的），用梯度下降算法进行参数更新，最小化J来学习所有的参数。\n\n\n\n基于内容的推荐算法，因为我们假设变量是已有的即我们有描述电影内容的特征量（电影中爱情成分和动作成分有多少），同时用这些特征量来进行预测。\n\n03 协同过滤\n协同过滤有一个很有意思的特性叫特征学习，这种算法能自行学习所要使用的特征。\n我们假设已经知道了每个用户的参数，那么通过每个用户的评分就可以算出一个电影包含爱情和动作的程度。（右下角)\n\n\n\n我们将这一学习问题标准化到任意特征x^(i)，求出x^(i)让预测值尽可能的接近真实值。\n\n\n\n基于内容的推荐算法是给特征求参数，第二个是给参数求特征。实际上我们可以随机地猜取一些参数，然后就能得到特征，再根据特征求参数不断地迭代得到更好地参数和特征，最终将会收敛到一组合理的特征和参数。这个过程就是协同过滤算法。\n\n\n\n对于推荐系统问题，仅建立在每位用户都对数个电影进行了评价，并且每部电影都经过数位用户评价的情况下，这样才能重复这个迭代过程，才能求出参数和特征。\n总结下：协同过滤算法是指当你执行算法时，要观察大量的用户及用户的实际行为来协同得到每个人对电影更佳的评分值。协同的另一层的意思就是说每一位用户都在帮助算法更好的进行特征学习。如果每一个用户都对一部分电影做出了评价，那么每个用户都在帮助算法学习出更适合的特征，然后这些被学习出来的特征又可以更好的预测其他用户的评分。\n\n04 协同过滤算法\n存在一种算法不需要不停的计算特征和参数，而是能够将特征和参数同时计算出来。下面就是这种算法，需要将前两个优化目标函数结合为一个。第一个平方误差项（所有用户j的总和及所有被该用户评分过的电影总和）和第二个平方误差项（与上面那个是相反的运算：对每部电影i，将所有对它评分过的用户j求和）相加得到新的平方误差项（对所有有评分的用户和电影进行求和），剩下两项拖下来。\n新的优化目标函数J有一个特性，你将x作为常数并关于参数（sita）最优化其实就是在计算第一个式子，反过来你将参数（sita）作为常数并关于x求J的最小值的话，其实就是在计算第二个式子。\n当我们以这样的方法学习特征量时我们不再遵循原来的惯例（x_0&#x3D;1），理由是我们现在是在学习所有的特征，没有必要将一个特征值硬编码为1，因为如果算法真的需要一个特征值为1，那么它可以选择靠自己去获取1这个数值，比如将x_1设为1.\n\n\n\n协同过滤算法步骤：\n将特征和参数初始为小的随机值（这有点像神经网络）\n用梯度下降或者其他的高级优化算法将代价函数最小化（这里没有分出k&#x3D;0的情况是因为不存在x_0&#x3D;1这一项了）\n如果给你参数和特征，就可以预测评分了\n\n\n\n\n\n这个算法可以学习几乎所有电影的特征和所有用户参数，能对不同用户会如何对他们尚未评分的电影做出评价，给出相当准确的预测。\n\n05 矢量化：低轶矩阵分解将介绍协同过滤算法向量化的实现及使用算法可以实现的一些功能（比如说给定一个商品，可以找到与之相关的一些产品）。\n\n\n将数据的预测值写成矩阵形式：\n\n\n\n用另一种方法低轶矩阵分解（矩阵X乘以sita的转置）写出这个算法的所有预测评分：\n\n\n\n利用学习到的属性来找到相关的电影，通常算法会学到一些有意义的特征。我们可以衡量两个电影的相似度（两个电影的特征距离很接近）来找到相关影片。\n\n\n06 实施细节：均值规范化\n下面增加了一个用户，这个用户之前没有给任何电影评过分，将其带入协同过滤算法中。因为用户对电影没有评分，所以算法的第一项对用户参数的选择是没有影响的，只有最后一项可以影响参数的选择，又因为最后一项是正则化，所以说得出来的参数是二维的零向量。二维零向量与哪个二维向量内积都为0，这样的结果不是我们想要的。\n\n\n\n均值归一化的思想可以帮助我们解决这个问题，下面介绍它是如何工作的：把每个电影评分都归一化使得均分为0\n\n将所有的评分都放到Y矩阵中\n计算每个电影所得评分的均值，并将其存放在一个叫u的向量中\n将Y中所有元素-均值\n对新的评分数据集使用协同过滤算法来学习用户参数及电影特征\n最后来计算电影评分：用户参数的转置与电影特征的内积+u（因为训练集中减去了均值，因此在这里需要加上）\n\n\n之前用户参数为二维零向量的问题依然存在，但是第五个用户的预测评分变了，变成了电影评分的均值。选择一个大众比较喜欢的推荐给第五个用户，当然如果一个电影没有评分（就不应该推荐，因为关心没有评价电影的人比关系没有被评价过电影更有意义），也可以对Y的每一列进行均值归一化。\n\n\n\n\n总结一下：均值归一化的实现作为协同过滤算法的预处理步骤，根据不同的数据集，有时可以让你的算法表现得更好一些。\n\n","tags":["机器学习"]},{"title":"机器学习 day17大规模机器学习","url":"/2021/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day17%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","content":"01 学习大数据集大规模机器学习就是处理大数据集的算法\n\n\n一种获取高性能的机器学习系统的途径是采用低偏差的学习算法并用大数据进行训练。\n大数据集学习的特有问题是计算问题。\n在训练一亿个样本之前，我们可以随机选择一亿个样本中的一千个样本，然后用这一千个样本来训练我们的算法。如果用一千个样本训练效果也是一样的话，在训练大量的模型前预先检查往往也是一个明智的选择。如果效果相同的话，可以根据这一千个样本绘制学习曲线，如果画出左图高方差的曲线那么增加训练集还是有作用的，如果是右边高偏差的算法，样本也不会比训练1000个更好了，如果是右图那么自然而然地会添加额外地特征项或在神经网络中添加额外地隐藏单元，这样最终可能得到左边的图。\n\n\n02 随机梯度下降当我们训练集很大时，梯度下降算法的计算量会变得非常大，因此对普通的梯度下降法进行改良为随机梯度下降法，使得算法能够应用于大规模的数据训练。\n\n\n先来回顾下线性回归：第一个式子是假设函数，第二个式子是代价函数，第三个式子是不断地更新参数sita来使得代价函数趋于最小，右图是构建的三维函数可以看出是弓形。\n\n\n\n在参数初始值，运行梯度下降算法，将会不断迭代，最终得到参数的全局最小值。如果m的值非常的大，那么单单是计算微分项就需要很大的计算量，更何况还要一次次的迭代。这个梯度下降算法又叫批量梯度下降，其中批量是指每次都需要同时考虑全部的训练样本。\n\n\n\n随机梯度下降算法每次迭代不需要每次都同时考虑全部的训练样本，仅仅只考虑一个训练样本。为了更好的描述随机梯度下降算法，我们将代价函数定义为右侧的第一个式子，这个代价函数实际上是衡量我的假设函数在某个样本(x(i),y(i))上的表现。而左图第一个总体代价函数可以写成右图第二个式子（假设函数在m个训练样本中每一个样本(x(i),y(i))上的代价函数的平均值），随机梯度下降的步骤如下：\n将所有的m个训练样本重新随机排列。(这保证了遍历时对训练集样本的访问是以随机顺序排列的，这一步能让随机梯度下降在收敛时能够更快一点)\n对所有的训练样本进行遍历，然后做如下参数更新.（就是不断地对参数进行调整来依次适应第一个参数、第二个参数……..）\n\n\n随机梯度下降和批量梯度下降相比不需要对全部的m个样本求和来得到梯度项，而只需要对单个样本就可求出梯度项（黄色框画出来的就是梯度项）。在随机梯度下降过程中我们已经开始一点点把参数向全局最小值的方向进行修改了。\n\n\n\n随机梯度下降每一次迭代都会更快，因此我们不需要对所有训练样本进行求和，每一次迭代只要保证能拟合某一个训练样本即可。虽然它的参数走的线不是一条曲线，但是它是随机而迂回的路径朝着全局最小值的方向移动的。随机梯度下降和批量梯度下降相比收敛形式是不同的，他就是连续不断的在某个区域中朝着全局最小值的方向徘徊，而不是直接打到全局最小值（在实际应用中，只要能接近全局最小值就能得到一个很好的效果）。\n在随机梯度下降法中其实还有一个外层循环（1到10次，通常一次就可，还是取决于数据集m的大小）来决定内层循环的次数\n\n\n\n总结下：批量梯度下降算法是一次就需要遍历全部的训练集，然后在进行迭代（迭代一次就相当于遍历两次全部训练集）才有可能得到全局最小值，而随机梯度下降算法有可能只需要遍历一次就可以的到全局最小值。所以说将随机梯度下降法的思想应用到学习算法中来适应更大的数据集从而提升算法的性能。\n\n03 Mini-Batch 梯度下降Mini-Batch 梯度下降有时甚至比随机梯度下降还要快一些。\n\n\n总结下我们迄今为止学的梯度下降算法：\n批量梯度下降算法每次迭代都要用到所有的m个样本\n随机梯度下降算法每次迭代只需要使用一个样本\n而Mini-Batch 梯度下降算法每次迭代会使用b个样本（b是一个称为Mini-Batch大小的参数，通常选择b&#x3D;10，b的范围是2~100），它是介于批量梯度下降算法和随机梯度下降算法之间的算法。\n\n\n\n\n\nMini-Batch 梯度下降完整算法如下：以步长为10增长为例，仅用前十个样本就能运行算法来更新参数，这也是为什么Mini-Batch 梯度下降比批量梯度下降算法要快的原因。为什么Mini-Batch 梯度下降不像随机梯度下降算法一样每次只使用一个样本呢？–&gt;因为在向量化过程中Mini-Batch 梯度下降比随机梯度下降更快（仅当有好的向量化方法）。\nMini-Batch 梯度下降算法的缺点之一是当有一个额外的参数b时，你需要确定Mini-Batch大小，这需要额外的时间，当然如果有好的向量化算法Mini-Batch 梯度下降比随机梯度下降更快。\n\n\n\n如果有一个合适的向量化的方法，蓝框的求和公式将在10个样本中实现部分并行计算，换句话来说通过合适的向量化方法计算余下的样本，可以部分使用好的数值代数库，然后对b个样本并行进行梯度计算，随机梯度下降算法每次只针对一个样本肯定没有太多的并行计算。\n\n04 随机梯度下降收敛当你运行随机梯度下降算法时如何确保调试过程已经完成，并且已收敛到合适的位置呢？怎样调整随机梯度下降算法中的学习速率a的值呢？\n\n\n批量梯度下降算法确保梯度下降已经收敛的一个标准方法就是绘制优化代价函数（关于迭代次数的函数），我们要确保代价函数J每一次迭代都是下降的。当m教小时使用这个还算可以，但是当训练集m特别大时，你肯定不希望得定期的暂停这个算法来计算蓝色框中的式子，因为每次计算都会遍历整个数据集。\n对于随机梯度下降算法为了检查算法是否收敛可以进行下面的工作：当随机梯度下降法对训练集进行扫描时，在我们使用某个样本(x(i),y(i))来更新参数sita之前，让我们来计算出这个训练样本假设的表现有多好即cost()函数。最后为了检查是否收敛，每1000次迭代就画出前一步骤所计算出来的cost()函数前一千个样本的的平均值，这样通过观察图就可以看是否下降收敛，可以看出算法在你前一千个样本中表现得有多好。\n\n\n\n下面第一个图中红色线代表更小的学习速率a，可以看出更加得平缓但是能得到更好的效果。（因为随机梯度下降不是直接达到全局最小值，而是在这个区域不断震荡，学习速率越小震荡幅度也就越小。）第二个图中得红色线代表样本从一千增加到五千，可以看出得到的曲线比较平滑，但是得到的关于算法表现有多好的反馈就有一点延迟，因为图中每一个数据点是从五千个样本中得到，而不是从之前的1000个样本得到。第三个图由于样本太少所以表现得好像没有下降收敛一样，红线代表样本增加到5000，可以看出比最开始显得下降了。而粉色线也代表样本增加到了五千但是没有啥变化，这就说明算法基本就没有学习，这时需要调整学习速率或者特征或者其他东西。第四个图是上升的趋势，这时算法发散了，这时就需要更小的学习速率。\n\n\n\n总结：如果出现噪声太大或者图像老是上下振动，就可以试着增加求均值的样本的数量，如果图像上升那么就用一个更小的学习速率。\n如果你想要将随机梯度下降到更好的收敛到全局最小值（也是接近哈），那么就需要让学习速率的值随时间变化而逐渐减小。经典的做法就是让a&#x3D;某个常数1&#x2F;（迭代次数+某个常数2），但是很少用，原因有二：一是收敛得到的值已经很满意了，二是还需要确定两个常数，无形中又增加了算法的复杂性还有计算量。\n\n\n05 在线学习在线学习机制可以让我们模型化一些问题，就是我们有连续一波数据，想要用算法从中学习的这类问题。\n\n\n以提供运输服务为例，y&#x3D;1是购买运输服务，y&#x3D;0是不购买，我们想要用（出发地、目的地、我们提供的价格等）特征来学习他们选择我们来运输包裹的概率。如果我们可以估计出每种价格下用户选择使用服务的概率，我们就可以选择一个价格即可能使用户选择我们，我们还会有回报。算法如下：当用户访问我们时，我们会得到与其对应的（x，y）对，在线学习算法就会利用刚得到的（x，y）(因为我们不使用固定的训练集，所以这里不是（x^(i),y^(i)）)来更新参数theta。我们使用完这个样本就会将其丢弃不在使用，这也就是为什么一次只处理一个样本。\n\n\n\n在比如说搜索的例子（又叫点击率预测学习问题 -CTR），当你搜索手机时，他会从100个手机里给你推送10个，提供了特征x(与你搜索的手机关联度)，y&#x3D;1是点击进入。运行此类网站的一个方法就是不停的给用户展示你对他们可能会喜欢的十个手机的预测，每一次用户访问你将会得到十个样本即十个对应的（x，y）对，然后运行在线学习算法来更新参数，对这十个样本利用10步梯度下降法来更新参数。\n\n\n\n在线学习的一个优点就是如果有一个变化的用户群又或者是你在预测的事情在缓慢的变化，在线学习算法可以慢慢地调试你所学习到的假设，将其调节到最新的用户行为。\n总结：在线学习算法与随机梯度下降算法的唯一的区别就是我们不会使用一个固定的数据集，而是获取一个用户样本从那个样本中学习，然后丢弃那个样本继续处理下一个。如果某一个应用有一个连续的数据流，这样的算法是非常值得考虑的。\n\n06 减少映射与数据并行MapReduce思想可以将学习算法应用于随机梯度下降不能解决的规模更大的问题。\n\n\n根据MapReduce思想我们把训练集分为不同的子集，有几台电脑或者机器并行处理训练集数据就分为几个子集。每台机器使用相对应的子集来处理梯度下降算法中的求和部分，这样每个机器都进行工作，效率就提高了好几倍。最后我们将这些temp变量发送给一个中心服务器，中心服务器会整合这些结果，尤其是将更新我的参数theta。可以说这个公式完全等同于批量梯度下降法，只是不需要在一台机器上处理这400个数据。\n\n\n\n就例子而言，四台电脑各自承担四分之一的计算量，你可以加速到四倍速，如果没有网络延迟和忽略传输数据所有时间，那么就是四倍的速度，当然现实中的网络延迟及汇总所需要额外的时间，实际速度比四倍小。\n\n\n\n使用MapReduce思想还需要考虑学习算法是否可以表示成对训练集的一种求和。如果可以就能将学习算法的适用范围扩充到非常非常大的数据集。下面一个例子：高级优化算法需要计算优化目标的代价函数的计算过程和这些偏导项的计算过程，电脑把他们发送给中心服务器，然后将各部分和（temp^(i)_j）加起来，获得总的代价函数获得总的偏导项，接着将这两个值发送给高级优化算法。\n\n\n\n现在也可以在单机上使用MapReduce，因为电脑有多个处理核心CPU，CPU又有多个核心。这样的好处是可以不用担心网络延迟问题。如果你有一个多核机器，同时你有某些线性代数库（可以自动在一台电脑不同核心上进行并行代数运算），并且你的学习算法有非常好的向量化表示，你就可以直接以向量化的形式应用标准学习算法，不用担心并行（线性代数库就可以处理好）。\n\n\n","tags":["机器学习"]},{"title":"机器学习 day18应用举例：照片OCR（光学字符识别）","url":"/2021/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20day18%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%EF%BC%9A%E7%85%A7%E7%89%87OCR%EF%BC%88%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%EF%BC%89/","content":"01 问题描述与 OCR pipeline照片OCR问题注重的是如何让计算机读出照片中的文字信息（照片OCR对于读取文档来说很简单，但是对于数码照片来说还是很难的）\n\n\n照片OCR当你输入文字时，计算机就能自动的帮你找到相册里带有这些文字的照片，它有如下几个步骤：\n给定某个照片，将其图像扫描一遍。\n找出照片中的文字信息\n找出照片中的文字信息之后，将重点关注这些文字区域，并对区域中的文字进行识别，当它正确读出这些文字之后，他会将这些文字内容显示并记录下来。\n\n\n\n\n\n总体来说步骤分为三类：文字检测、字符分割以及字符分类（其实还有个拼写校正）。\n\n\n\n像这样的系统，我们把它称为机器学习流水线：首先有一个照片，然后把它传给文字检测系统，识别出文字区域后，将文字区域中的单个字符分割出来，最后对这些单个字母进行识别。\n\n\n02 滑动窗口关于流水线中每个独立的组件的工作原理，主要讲一个叫滑动窗口分类器。\n\n\n照片OCR问题难在文字区域对应的矩形具有不同的长宽比例，而对于行人检测来说，人们的长宽比基本相同。\n\n\n\n建立一个行人检测系统，我们可以这么做：假如决定要把比例标准定为82 * 36（当然其他比例也可以），然后从数据集中收集一些正样本和负样本（都是82 * 36的照片），在你的网络中训练或者使用其他学习算法向其中输入一个82 * 36的图块来对y进行分类，来划分每个图块是否包括一个行人\n\n\n\n下图相当于一个训练集，首先在图片中选取一个矩形块（比如是左上角绿框），然后将图块传送给我们的分类器，来检查图块中是否有行人，然后每次以一定的步长（也称滑动参数）来移动图块，传送给分类器不断地判断图块中是否有行人。然后再以更大窗口移动来判断是否有行人，这样算法便能检测出图中各个地方是否出现行人。这就是一个监督学习分类器，然后使用一个滑动窗口分类器来找出图中所有行人。\n\n\n\n接下来看一下在图片OCR中如何找出图中的文字区域：\n\n首先也是拿出一系列的包括正样本和负样本的训练集。\n\n\n\n训练完后将其应用在一个新的测试集中的图片，在此使用固定比例的滑动窗口，会得到左下角的图片（白色表示文本检查系统发现了文本，而深浅不同的灰色对应的是分类器认为该处有文字的概率），然后将分类器的输出应用到一种叫放大算子（就是扩大白色区域，如何扩大呢？就是通过检查像素附近是否存在白色像素，然后把这一范围内都变成白色）的东西上得到右下角的图，然后需要在右下角图中白色周围绘制边框，文本周围的框宽度应该远大于高度，通过这个特性就可以筛选出正常的文字区域。\n\n\n\n接下来进入识别文本阶段也就是字符分割，在此我们再次使用监督学习算法用一些正样本（可以进行分割）和负样本（不可以进行分割），对分类器进行训练完就可以将其运行在文字检测系统输出的这些文本中，最终可以将图像全部分成单独的字符。\n\n\n\n\n03 获取大量数据和人工数据一个最可靠得到高性能机器学习系统的方法是使用一个低偏差机器学习算法并且使用庞大的训练集去训练他。其中人工数据合成可以为合适的机器学习问题轻松得到大规模的训练集。\n\n\n人工数据合成主要有两种形式：\n\n自己创造数据（从0开始创造新数据）\n已经有小的标签训练集，然后以某种方法扩充训练集（引入失真）\n\n\n假如我们收集到了一个大的标签数据集如左图所示，我们的目标是输入一个图像块然后识别出图像块中央的字符。为了简化操作我们将图片处理成灰度图像而不是彩色图像。如果想要更多的训练样本一个方法就是用不同的字体生成字符，然后将其粘贴到任意不同的背景中，然后可以应用一点模糊算子或者仿射变换（仿射的意思是进行等分、缩放和一些旋转操作），完成这些操作就会得到一个人工合成训练集如右图所示。这个就是自己创造数据。\n\n\n\n\n使用现有的样本生成训练集。可以对图片进行人工拉伸或者人工扭曲（必须要合理的，具有代表性），这样就可以将一个样本变成16个新样本了。通过这种方法就能将一个小的标签训练集扩充为一个更大的训练集。当然不同的机器学习应用，可能其他类型的失真将更合理。以语音识别为例，目的是从对话中获取内容，可以通过人工添加失真引入不同背景音乐，得到大量的训练集。生成的新样本一定要具有代表性是有可能在测试集中见到的样本。\n\n\n\n\n在生成大量人工训练集之前最好画一个学习曲线保证你有一个低偏差高方差的分类器。如果你的偏差太高，可以尝试持续增加分类器的特征数量或者增加神经网络隐藏单元的数量，然后在花精力在生成大量人工训练集，这样就避免了花费大量的时间来收集数据却发现没有多大作用。用学习曲线做一个合理的检验看更多的数据是否真的有用。\n\n获得目前训练集十倍的量需要花费多少工作量（方法）？\n\n人工合成数据\n自己收集数量或者添加标签\n众包数据标记：从花钱网上找人来帮你标记训练集\n\n\n\n\n04 天花板分析：下一步工作的 pipeline上线分析：当你自己或者跟你的团队在设计某个机器学习系统工作流时，这种方法能够提供一个很有价值的信号，知道你工作流中哪一部分最值得花费时间去研究。\n\n\n为了决定如何开发系统一个有效的方法就是对学习系统使用一个数值评价量度，假如我们用字符准确度作为这个量度，给定一个测试样本图像，那么这个数值就表示我们对测试图像中的文本识别正确的比例。如图所示：不管你用什么度量值来度量，整个系统的总的准确率为72%。\n下面是上线分析的主要思想：还是以图片OCR流水线为例，首先我们先关注文本检测，对于每一个测试样本都给一个正确的文本检测结果，换句话说就是要100%正确检测出图片中的文本信息。只需要找到对应图像，然后人为的识别出测试集图像中出现文本的区域，让这个模块人为的输出正确的结果，将结果传送给字符分割模块，然后运行后面两个模块，使用之前一样的评价量度指标算出整个系统总的准确率89%，同样的在上一个处理的基础上，字符分割模块也用全部正确的结果去输出，得到了90%的准确率，字符分类也一样（全都是正确的当然是100%）。\n使用上限分析就可以看出每个模块进行改善后各自的成长空间是多少，可以看出完美的文本检测得到的增益最大。\n\n\n\n人脸识别也是一样：\n\n\n","tags":["机器学习"]}]